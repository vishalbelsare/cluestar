{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.15.1.json",
  "config": {
    "axis": {
      "grid": false
    },
    "view": {
      "continuousHeight": 300,
      "continuousWidth": 300,
      "strokeWidth": 0
    }
  },
  "data": {
    "name": "data-a3882071c71fa920dc716dd7159b8040"
  },
  "datasets": {
    "data-a3882071c71fa920dc716dd7159b8040": [
      {
        "r": 0,
        "text": "By leveraging this diversity, the collected dataset and the collection system aim to achieve higher recognition accuracy.",
        "trunc_text": "By leveraging this diversity, the collected dataset and the collection system aim to achieve higher recognition accuracy",
        "x1": 4.034946441650391,
        "x2": 4.963094711303711,
        "y1": 7.552857875823975,
        "y2": 2.761812925338745
      },
      {
        "r": 0,
        "text": "In this paper, we study linear regression applied to data structured on a manifold.",
        "trunc_text": "In this paper, we study linear regression applied to data structured on a manifold.",
        "x1": -1.246946096420288,
        "x2": 0.2294621765613556,
        "y1": 7.305799961090088,
        "y2": 1.0174270868301392
      },
      {
        "r": 0,
        "text": "We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold's extrinsic geometry on the regression.",
        "trunc_text": "We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impa",
        "x1": -1.242258071899414,
        "x2": 0.2500046491622925,
        "y1": 7.310015678405762,
        "y2": 1.0316237211227417
      },
      {
        "r": 0,
        "text": "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on early releases of ChatGPT that elicit undesired behavior.",
        "trunc_text": "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the ",
        "x1": 7.172708034515381,
        "x2": 6.204278469085693,
        "y1": 5.116878986358643,
        "y2": 6.19088077545166
      },
      {
        "r": 0,
        "text": "Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.",
        "trunc_text": "Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire I",
        "x1": 4.733817100524902,
        "x2": 3.6767351627349854,
        "y1": 4.222575664520264,
        "y2": 5.859053611755371
      },
      {
        "r": 0,
        "text": "However, we identify issues with the dataset quality and evaluation metric.",
        "trunc_text": "However, we identify issues with the dataset quality and evaluation metric.",
        "x1": 5.07607889175415,
        "x2": 5.536123752593994,
        "y1": 5.954461097717285,
        "y2": 3.728003978729248
      },
      {
        "r": 0,
        "text": "We will release our annotation scheme, the corpus, and codes to the research community to alleviate the scarcity of labeled data in this domain.",
        "trunc_text": "We will release our annotation scheme, the corpus, and codes to the research community to alleviate the scarcity of labe",
        "x1": 2.5988171100616455,
        "x2": 1.575907826423645,
        "y1": 3.4576103687286377,
        "y2": 5.553575038909912
      },
      {
        "r": 0,
        "text": "Extensive experiments are conducted to demonstrate the effectiveness of our proposed method.",
        "trunc_text": "Extensive experiments are conducted to demonstrate the effectiveness of our proposed method.",
        "x1": 5.481165885925293,
        "x2": 6.550716400146484,
        "y1": 8.893805503845215,
        "y2": 4.871631622314453
      },
      {
        "r": 0,
        "text": "Compared to a variety of baselines, our method achieves superior results.",
        "trunc_text": "Compared to a variety of baselines, our method achieves superior results.",
        "x1": 3.7313694953918457,
        "x2": 4.833104133605957,
        "y1": 6.491553783416748,
        "y2": 3.9217617511749268
      },
      {
        "r": 0,
        "text": "Previous segmentation methods for noisy label problems only utilize a single image while the potential of leveraging the correlation between images has been overlooked.",
        "trunc_text": "Previous segmentation methods for noisy label problems only utilize a single image while the potential of leveraging the",
        "x1": 0.8836878538131714,
        "x2": 1.0271077156066895,
        "y1": 5.432308197021484,
        "y2": 2.9907219409942627
      },
      {
        "r": 0,
        "text": "Experiments with both synthetic and real-world label noise demonstrate that our method outperforms recent state-of-the-art robust segmentation approaches.",
        "trunc_text": "Experiments with both synthetic and real-world label noise demonstrate that our method outperforms recent state-of-the-a",
        "x1": 0.871695876121521,
        "x2": 1.0003036260604858,
        "y1": 5.319084644317627,
        "y2": 3.0343470573425293
      },
      {
        "r": 0,
        "text": "We detail corpus statistics and demonstrate high inter-annotator agreement.",
        "trunc_text": "We detail corpus statistics and demonstrate high inter-annotator agreement.",
        "x1": 2.787797212600708,
        "x2": 1.6671631336212158,
        "y1": 3.4031734466552734,
        "y2": 5.579768657684326
      },
      {
        "r": 0,
        "text": "Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting.",
        "trunc_text": "Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, o",
        "x1": 0.8618607521057129,
        "x2": 2.167836904525757,
        "y1": 7.3359375,
        "y2": 1.612460732460022
      },
      {
        "r": 0,
        "text": "However, even manually labeled datasets contain errors, not to mention automatically labeled ones.",
        "trunc_text": "However, even manually labeled datasets contain errors, not to mention automatically labeled ones.",
        "x1": 1.4443820714950562,
        "x2": 0.9040427803993225,
        "y1": 4.184645175933838,
        "y2": 4.239068984985352
      },
      {
        "r": 0,
        "text": "Label error is a ubiquitous problem in annotated data.",
        "trunc_text": "Label error is a ubiquitous problem in annotated data.",
        "x1": 1.4341075420379639,
        "x2": 0.866433322429657,
        "y1": 4.059389114379883,
        "y2": 4.228272438049316
      },
      {
        "r": 0,
        "text": "After demonstrating that our methodology empirically outperforms other algorithms for label error detection, we apply our approach to discover many label errors in the CelebA image tagging dataset.",
        "trunc_text": "After demonstrating that our methodology empirically outperforms other algorithms for label error detection, we apply ou",
        "x1": 1.205680012702942,
        "x2": 0.7455461025238037,
        "y1": 4.299688339233398,
        "y2": 3.960155963897705
      },
      {
        "r": 0,
        "text": "These properties highlight a tradeoff between classification error probability and error-correction capabilities of label encodings.",
        "trunc_text": "These properties highlight a tradeoff between classification error probability and error-correction capabilities of labe",
        "x1": 1.3791770935058594,
        "x2": 0.9222063422203064,
        "y1": 4.291571140289307,
        "y2": 4.096385478973389
      },
      {
        "r": 0,
        "text": "In this work, we for the first time introduce a benchmark for label error detection methods on object detection datasets as well as a label error detection method and a number of baselines.",
        "trunc_text": "In this work, we for the first time introduce a benchmark for label error detection methods on object detection datasets",
        "x1": 1.121065378189087,
        "x2": 0.7299268245697021,
        "y1": 4.32115364074707,
        "y2": 3.949748992919922
      },
      {
        "r": 0,
        "text": "Label encodings found by RLEL result in lower or comparable errors to manually designed label encodings.",
        "trunc_text": "Label encodings found by RLEL result in lower or comparable errors to manually designed label encodings.",
        "x1": 1.3644009828567505,
        "x2": 0.8129414319992065,
        "y1": 4.11915922164917,
        "y2": 4.139528274536133
      },
      {
        "r": 0,
        "text": "We also propose an improved self-labeling loss; it is robust to pseudo-labeling errors and enforces stronger fairness.",
        "trunc_text": "We also propose an improved self-labeling loss; it is robust to pseudo-labeling errors and enforces stronger fairness.",
        "x1": 1.3861148357391357,
        "x2": 1.1249475479125977,
        "y1": 4.542959690093994,
        "y2": 3.893186092376709
      },
      {
        "r": 0,
        "text": "Inferencing unlabeled data from labeled data is an error-prone process.",
        "trunc_text": "Inferencing unlabeled data from labeled data is an error-prone process.",
        "x1": 1.318877935409546,
        "x2": 0.8654496073722839,
        "y1": 4.21882438659668,
        "y2": 4.075872421264648
      },
      {
        "r": 0,
        "text": "However, creating such large keypoint labels is time-consuming and costly, and is often error-prone due to inconsistent labeling.",
        "trunc_text": "However, creating such large keypoint labels is time-consuming and costly, and is often error-prone due to inconsistent ",
        "x1": 1.5939712524414062,
        "x2": 1.1103527545928955,
        "y1": 4.341132640838623,
        "y2": 4.007691860198975
      },
      {
        "r": 0,
        "text": "The losses are computed with respect to the predictions and the noisy labels including simulated label errors, aiming at detecting the latter.",
        "trunc_text": "The losses are computed with respect to the predictions and the noisy labels including simulated label errors, aiming at",
        "x1": 1.336324691772461,
        "x2": 1.106284499168396,
        "y1": 4.797586441040039,
        "y2": 3.5578205585479736
      },
      {
        "r": 0,
        "text": "PseudoAugments outperforms pseudo labeling by mitigating pseudo labeling errors and generating diverse fused training scenes.",
        "trunc_text": "PseudoAugments outperforms pseudo labeling by mitigating pseudo labeling errors and generating diverse fused training sc",
        "x1": 1.4457424879074097,
        "x2": 1.3353687524795532,
        "y1": 4.574244022369385,
        "y2": 3.7277650833129883
      },
      {
        "r": 0,
        "text": "Our model is also able to maintain high classification accuracy with very few labels, with only 7.79% error when only using 145 labels.",
        "trunc_text": "Our model is also able to maintain high classification accuracy with very few labels, with only 7.79% error when only us",
        "x1": 1.4747846126556396,
        "x2": 0.9464614987373352,
        "y1": 4.376220703125,
        "y2": 4.081840991973877
      },
      {
        "r": 0,
        "text": "Detecting errors in KGs is challenging since the patterns of errors are unknown and diverse, while ground-truth labels are rare or even unavailable.",
        "trunc_text": "Detecting errors in KGs is challenging since the patterns of errors are unknown and diverse, while ground-truth labels a",
        "x1": 1.6935662031173706,
        "x2": 1.0313774347305298,
        "y1": 4.3458991050720215,
        "y2": 3.9999656677246094
      },
      {
        "r": 0,
        "text": "We analyze the factors affecting this approximation error and design a pseudo-label clustering generation method to reduce the approximation error.",
        "trunc_text": "We analyze the factors affecting this approximation error and design a pseudo-label clustering generation method to redu",
        "x1": 1.391115665435791,
        "x2": 1.1946849822998047,
        "y1": 4.493894100189209,
        "y2": 3.7431108951568604
      },
      {
        "r": 0,
        "text": "To ameliorate the impact of label errors, we equipped our method with a novel negative label sampling strategy to strengthen the model robustness.",
        "trunc_text": "To ameliorate the impact of label errors, we equipped our method with a novel negative label sampling strategy to streng",
        "x1": 1.3078147172927856,
        "x2": 0.9500700831413269,
        "y1": 4.656033992767334,
        "y2": 3.724745273590088
      },
      {
        "r": 0,
        "text": "We propose an extension of the Confident Learning framework to this setting, as well as a label quality score that ranks examples with label errors much higher than those which are correctly labeled.",
        "trunc_text": "We propose an extension of the Confident Learning framework to this setting, as well as a label quality score that ranks",
        "x1": 1.2787740230560303,
        "x2": 1.053065538406372,
        "y1": 4.620603084564209,
        "y2": 3.869525671005249
      },
      {
        "r": 0,
        "text": "The later case can generate dense flow labels but the interpolated events are prone to errors.",
        "trunc_text": "The later case can generate dense flow labels but the interpolated events are prone to errors.",
        "x1": 1.3994572162628174,
        "x2": 0.8929601311683655,
        "y1": 4.341342926025391,
        "y2": 4.066623210906982
      },
      {
        "r": 0,
        "text": "Improper fingerprint localization and finger labeling errors lead to poor matching performance.",
        "trunc_text": "Improper fingerprint localization and finger labeling errors lead to poor matching performance.",
        "x1": 1.3164669275283813,
        "x2": 0.9675739407539368,
        "y1": 4.398782730102539,
        "y2": 4.09595251083374
      },
      {
        "r": 0,
        "text": "Our experiments show that our method is robust to linguistic labels with poor orthography and alignment errors.",
        "trunc_text": "Our experiments show that our method is robust to linguistic labels with poor orthography and alignment errors.",
        "x1": 1.5759477615356445,
        "x2": 1.1640229225158691,
        "y1": 4.231647968292236,
        "y2": 4.110983848571777
      },
      {
        "r": 0,
        "text": "We derive an upper bound for the generalization error that is linear in the clients' label noise level.",
        "trunc_text": "We derive an upper bound for the generalization error that is linear in the clients' label noise level.",
        "x1": 1.548275351524353,
        "x2": 1.190232753753662,
        "y1": 4.865266799926758,
        "y2": 3.558494806289673
      },
      {
        "r": 0,
        "text": "For example, for the IMDB text data with known labeling errors, a 14% boost is shown.",
        "trunc_text": "For example, for the IMDB text data with known labeling errors, a 14% boost is shown.",
        "x1": 1.267617106437683,
        "x2": 0.8276552557945251,
        "y1": 4.2883477210998535,
        "y2": 4.091851711273193
      },
      {
        "r": 0,
        "text": "Large amounts of label error substantially degrades the quality of deep learning models.",
        "trunc_text": "Large amounts of label error substantially degrades the quality of deep learning models.",
        "x1": 1.52749764919281,
        "x2": 1.1854181289672852,
        "y1": 4.782151222229004,
        "y2": 3.721283435821533
      },
      {
        "r": 0,
        "text": "We simulate four different types of randomly introduced label errors on train and test sets of well-labeled object detection datasets.",
        "trunc_text": "We simulate four different types of randomly introduced label errors on train and test sets of well-labeled object detec",
        "x1": 1.018369197845459,
        "x2": 0.749204158782959,
        "y1": 4.467499256134033,
        "y2": 3.80405592918396
      },
      {
        "r": 0,
        "text": "We prove that semi-supervised labels improve the downstream error bound whereas noisy labels have limited effects under such a paradigm.",
        "trunc_text": "We prove that semi-supervised labels improve the downstream error bound whereas noisy labels have limited effects under ",
        "x1": 1.3080496788024902,
        "x2": 1.0495151281356812,
        "y1": 4.845048427581787,
        "y2": 3.46193528175354
      },
      {
        "r": 0,
        "text": "This paper provides an exact characterization of the expected generalization error (gen-error) for semi-supervised learning (SSL) with pseudo-labeling via the Gibbs algorithm.",
        "trunc_text": "This paper provides an exact characterization of the expected generalization error (gen-error) for semi-supervised learn",
        "x1": 1.4517825841903687,
        "x2": 1.3532733917236328,
        "y1": 4.831872463226318,
        "y2": 3.4608683586120605
      },
      {
        "r": 0,
        "text": "However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowd workers.",
        "trunc_text": "However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowd workers.",
        "x1": 1.4761176109313965,
        "x2": 0.9457898736000061,
        "y1": 4.375237464904785,
        "y2": 4.161697864532471
      },
      {
        "r": 0,
        "text": "Most existing methods utilize the off-the-shelf pose or parsing networks as pseudo labels, which are prone to error.",
        "trunc_text": "Most existing methods utilize the off-the-shelf pose or parsing networks as pseudo labels, which are prone to error.",
        "x1": 1.4589784145355225,
        "x2": 1.373622179031372,
        "y1": 4.443871021270752,
        "y2": 3.7003064155578613
      },
      {
        "r": 0,
        "text": "The result is an SSL classification framework explicitly designed to overcome inevitable pseudo-label errors.",
        "trunc_text": "The result is an SSL classification framework explicitly designed to overcome inevitable pseudo-label errors.",
        "x1": 5.778923034667969,
        "x2": 1.5538125038146973,
        "y1": 5.501486301422119,
        "y2": 3.002758264541626
      },
      {
        "r": 0,
        "text": "Here we consider the task of finding sentences that contain label errors in token classification datasets.",
        "trunc_text": "Here we consider the task of finding sentences that contain label errors in token classification datasets.",
        "x1": 1.4002470970153809,
        "x2": 0.7821102142333984,
        "y1": 4.076772689819336,
        "y2": 4.109925270080566
      },
      {
        "r": 0,
        "text": "Scaling sequence length has become a critical demand in the era of large language models.",
        "trunc_text": "Scaling sequence length has become a critical demand in the era of large language models.",
        "x1": 4.754937648773193,
        "x2": 3.6819546222686768,
        "y1": 4.106607913970947,
        "y2": 5.920755863189697
      },
      {
        "r": 0,
        "text": "However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted.",
        "trunc_text": "However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum seq",
        "x1": 4.733103275299072,
        "x2": 3.71213698387146,
        "y1": 4.291069030761719,
        "y2": 5.7034525871276855
      },
      {
        "r": 0,
        "text": "In this work, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences.",
        "trunc_text": "In this work, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, ",
        "x1": 4.607015609741211,
        "x2": 3.835491895675659,
        "y1": 4.484203338623047,
        "y2": 5.63031530380249
      },
      {
        "r": 0,
        "text": "Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows.",
        "trunc_text": "Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows.",
        "x1": 2.7628965377807617,
        "x2": 2.8147895336151123,
        "y1": 5.804245471954346,
        "y2": 3.9242942333221436
      },
      {
        "r": 0,
        "text": "Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks.",
        "trunc_text": "Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general langua",
        "x1": 4.708477973937988,
        "x2": 3.7676644325256348,
        "y1": 4.190625190734863,
        "y2": 5.79199743270874
      },
      {
        "r": 0,
        "text": "Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across various domains.",
        "trunc_text": "Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across vario",
        "x1": 5.749414443969727,
        "x2": 4.133735656738281,
        "y1": 4.058283805847168,
        "y2": 7.266139984130859
      },
      {
        "r": 0,
        "text": "However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are crucial skills for intelligent embodied agents.",
        "trunc_text": "However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are",
        "x1": 6.000122547149658,
        "x2": 4.210313320159912,
        "y1": 4.1274919509887695,
        "y2": 7.52384090423584
      },
      {
        "r": 0,
        "text": "In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embodied environments.",
        "trunc_text": "In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embod",
        "x1": 6.014997482299805,
        "x2": 4.257992744445801,
        "y1": 4.149316787719727,
        "y2": 7.565698623657227
      },
      {
        "r": 0,
        "text": "Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomplish long-horizon tasks efficiently.",
        "trunc_text": "Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomp",
        "x1": 5.95615291595459,
        "x2": 4.196545600891113,
        "y1": 4.1272172927856445,
        "y2": 7.504188537597656
      },
      {
        "r": 0,
        "text": "We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using our framework without requiring fine-tuning or few-shot prompting.",
        "trunc_text": "We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective",
        "x1": 5.973058700561523,
        "x2": 4.56121301651001,
        "y1": 4.352818489074707,
        "y2": 6.6183695793151855
      },
      {
        "r": 0,
        "text": "We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans.",
        "trunc_text": "We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effec",
        "x1": 5.663139343261719,
        "x2": 4.125993251800537,
        "y1": 3.8020999431610107,
        "y2": 6.965487003326416
      },
      {
        "r": 0,
        "text": "For QE in particular, high-quality labeled data is often lacking due to the high-cost and effort associated with labeling such data.",
        "trunc_text": "For QE in particular, high-quality labeled data is often lacking due to the high-cost and effort associated with labelin",
        "x1": 1.57516348361969,
        "x2": 0.9893369674682617,
        "y1": 4.192626476287842,
        "y2": 4.235525608062744
      },
      {
        "r": 0,
        "text": "With many possible classes to consider, data annotators are likely to make errors when labeling such data in practice.",
        "trunc_text": "With many possible classes to consider, data annotators are likely to make errors when labeling such data in practice.",
        "x1": 1.7266956567764282,
        "x2": 0.9589722752571106,
        "y1": 3.9712560176849365,
        "y2": 4.500904083251953
      },
      {
        "r": 0,
        "text": "However, it usually suffers from a lack of high-quality datasets due to high annotation cost, inter-observer variability, human annotator error, and errors in computer-generated labels.",
        "trunc_text": "However, it usually suffers from a lack of high-quality datasets due to high annotation cost, inter-observer variability",
        "x1": 1.6947280168533325,
        "x2": 1.0172165632247925,
        "y1": 4.059757709503174,
        "y2": 4.397420883178711
      },
      {
        "r": 0,
        "text": "For such bone structure analyses, deep learning technologies are promising but require high-quality labeled data for the learning, while the data labeling is costly.",
        "trunc_text": "For such bone structure analyses, deep learning technologies are promising but require high-quality labeled data for the",
        "x1": 1.5461232662200928,
        "x2": 2.814699649810791,
        "y1": 6.119345664978027,
        "y2": 2.230628490447998
      },
      {
        "r": 0,
        "text": "However, agreement between annotators is often low, leading to inconsistent labels that hinder the reliability of models.",
        "trunc_text": "However, agreement between annotators is often low, leading to inconsistent labels that hinder the reliability of models",
        "x1": 2.1110243797302246,
        "x2": 1.178550362586975,
        "y1": 3.811903476715088,
        "y2": 4.832125186920166
      },
      {
        "r": 0,
        "text": "Our experiments show that this approach consistently improves inter-annotator agreement and annotation accuracy.",
        "trunc_text": "Our experiments show that this approach consistently improves inter-annotator agreement and annotation accuracy.",
        "x1": 2.3424935340881348,
        "x2": 1.3764959573745728,
        "y1": 3.6270527839660645,
        "y2": 5.162405490875244
      },
      {
        "r": 0,
        "text": "We advocate for the use of IAA in predicting the labeling quality of individual annotators, leading to cost and time efficiency in data production.",
        "trunc_text": "We advocate for the use of IAA in predicting the labeling quality of individual annotators, leading to cost and time eff",
        "x1": 2.012836456298828,
        "x2": 1.197728157043457,
        "y1": 3.8123884201049805,
        "y2": 4.72989559173584
      },
      {
        "r": 0,
        "text": "This paper presents a novel approach of leveraging Inter-Annotator Agreement (IAA), traditionally used for assessing labeling consistency, to optimize Data Management Operations (DMOps).",
        "trunc_text": "This paper presents a novel approach of leveraging Inter-Annotator Agreement (IAA), traditionally used for assessing lab",
        "x1": 2.1801815032958984,
        "x2": 1.2437717914581299,
        "y1": 3.805540084838867,
        "y2": 4.79796028137207
      },
      {
        "r": 0,
        "text": "Our study illustrates that different labeling methodologies directly impact the annotations' quality, as well as the capabilities of a deep learning classifier trained with the data respectively.",
        "trunc_text": "Our study illustrates that different labeling methodologies directly impact the annotations' quality, as well as the cap",
        "x1": 1.6341837644577026,
        "x2": 1.230018138885498,
        "y1": 4.429536819458008,
        "y2": 4.2268595695495605
      },
      {
        "r": 0,
        "text": "However, such annotations may fail in practice because of the change in annotation requirements, application scenarios, and modeling goals, where label validation and relabeling by domain experts are required.",
        "trunc_text": "However, such annotations may fail in practice because of the change in annotation requirements, application scenarios, ",
        "x1": 1.9872887134552002,
        "x2": 1.023659348487854,
        "y1": 3.7754478454589844,
        "y2": 4.747179985046387
      },
      {
        "r": 0,
        "text": "However, selecting training samples based on the degree of agreement between annotators introduces a bias in the training data and does not improve the results.",
        "trunc_text": "However, selecting training samples based on the degree of agreement between annotators introduces a bias in the trainin",
        "x1": 2.2350211143493652,
        "x2": 1.4013608694076538,
        "y1": 4.15673828125,
        "y2": 4.7791523933410645
      },
      {
        "r": 0,
        "text": "However, these annotations are inherently subjective and some of the instances are hard to classify, resulting in noisy annotations due to error or lack of agreement.",
        "trunc_text": "However, these annotations are inherently subjective and some of the instances are hard to classify, resulting in noisy ",
        "x1": 2.1266894340515137,
        "x2": 1.2029616832733154,
        "y1": 3.8680155277252197,
        "y2": 4.853307247161865
      },
      {
        "r": 0,
        "text": "We propose and evaluate an additional application of our method leading to the detection of annotation errors.",
        "trunc_text": "We propose and evaluate an additional application of our method leading to the detection of annotation errors.",
        "x1": 2.232969284057617,
        "x2": 1.257431149482727,
        "y1": 3.603837251663208,
        "y2": 5.074026584625244
      },
      {
        "r": 0,
        "text": "However, arbitrating the final annotation is not always effective because new biases might be produced during the process, especially when there are significant variations among annotations.",
        "trunc_text": "However, arbitrating the final annotation is not always effective because new biases might be produced during the proces",
        "x1": 2.2105977535247803,
        "x2": 1.2400826215744019,
        "y1": 3.7086493968963623,
        "y2": 5.031446933746338
      },
      {
        "r": 0,
        "text": "A two-step human annotation and inter-annotator agreement study guarantee the high quality of the PcMSP corpus.",
        "trunc_text": "A two-step human annotation and inter-annotator agreement study guarantee the high quality of the PcMSP corpus.",
        "x1": 2.812849998474121,
        "x2": 1.6038594245910645,
        "y1": 3.386338710784912,
        "y2": 5.484565734863281
      },
      {
        "r": 0,
        "text": "We observe a striking correlation between the model's and humans' annotation: Categories with consistent human annotations (>$0.9$ inter-rater reliability, IRR) also display higher human-model agreement (>$0.7$), while categories with less consistent human annotations ($0.7$-$0.8$ IRR) correspondingly demonstrate lower human-model agreement ($0.3$-$0.5$).",
        "trunc_text": "We observe a striking correlation between the model's and humans' annotation: Categories with consistent human annotatio",
        "x1": 2.470881462097168,
        "x2": 1.4273258447647095,
        "y1": 3.679405450820923,
        "y2": 5.164988994598389
      },
      {
        "r": 0,
        "text": "We propose two metrics to audit the noise of annotations.",
        "trunc_text": "We propose two metrics to audit the noise of annotations.",
        "x1": 2.233647584915161,
        "x2": 1.2459969520568848,
        "y1": 3.696462392807007,
        "y2": 5.024632930755615
      },
      {
        "r": 0,
        "text": "Whereas such annotation is costly and hard to scale, significantly holding back the development of the research.",
        "trunc_text": "Whereas such annotation is costly and hard to scale, significantly holding back the development of the research.",
        "x1": 2.319789171218872,
        "x2": 1.3493741750717163,
        "y1": 3.655531883239746,
        "y2": 5.056482791900635
      },
      {
        "r": 0,
        "text": "A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label.",
        "trunc_text": "A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining ",
        "x1": 2.1357388496398926,
        "x2": 1.481919527053833,
        "y1": 4.008366584777832,
        "y2": 4.755666255950928
      },
      {
        "r": 0,
        "text": "We hypothesize two failure modes of safety training: competing objectives and mismatched generalization.",
        "trunc_text": "We hypothesize two failure modes of safety training: competing objectives and mismatched generalization.",
        "x1": 3.7051572799682617,
        "x2": 3.908815622329712,
        "y1": 5.561306476593018,
        "y2": 4.304465293884277
      },
      {
        "r": 0,
        "text": "Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist.",
        "trunc_text": "Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs",
        "x1": 3.786872386932373,
        "x2": 3.918846368789673,
        "y1": 5.694733142852783,
        "y2": 4.2604827880859375
      },
      {
        "r": 0,
        "text": "We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models.",
        "trunc_text": "We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models.",
        "x1": 7.596564769744873,
        "x2": 6.6419572830200195,
        "y1": 5.216818809509277,
        "y2": 6.244349002838135
      },
      {
        "r": 0,
        "text": "Specifically, we analyze the impact of the manifold's curvatures (or higher order nonlinearity in the parameterization when the curvatures are locally zero) on the uniqueness of the regression solution.",
        "trunc_text": "Specifically, we analyze the impact of the manifold's curvatures (or higher order nonlinearity in the parameterization w",
        "x1": -1.2461448907852173,
        "x2": 0.20377980172634125,
        "y1": 7.310060977935791,
        "y2": 1.0066713094711304
      },
      {
        "r": 0,
        "text": "Our findings suggest that the corresponding linear regression does not have a unique solution when the embedded submanifold is flat in some dimensions.",
        "trunc_text": "Our findings suggest that the corresponding linear regression does not have a unique solution when the embedded submanif",
        "x1": -1.2678560018539429,
        "x2": 0.2247546911239624,
        "y1": 7.327144622802734,
        "y2": 1.0229395627975464
      },
      {
        "r": 0,
        "text": "Our findings thus reveal the role of data manifold geometry in ensuring the stability of regression models for out-of-distribution inferences.",
        "trunc_text": "Our findings thus reveal the role of data manifold geometry in ensuring the stability of regression models for out-of-di",
        "x1": -1.2256381511688232,
        "x2": 0.2512451112270355,
        "y1": 7.301884651184082,
        "y2": 1.0565721988677979
      },
      {
        "r": 0,
        "text": "To disentangle these effects, we propose an evaluation framework based on \"counterfactual\" task variants that deviate from the default assumptions underlying standard tasks.",
        "trunc_text": "To disentangle these effects, we propose an evaluation framework based on \"counterfactual\" task variants that deviate fr",
        "x1": 4.055990695953369,
        "x2": 3.7974326610565186,
        "y1": 5.54556941986084,
        "y2": 4.507071018218994
      },
      {
        "r": 0,
        "text": "Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions.",
        "trunc_text": "Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that",
        "x1": 4.119287014007568,
        "x2": 3.843214988708496,
        "y1": 5.431909561157227,
        "y2": 4.492413520812988
      },
      {
        "r": 0,
        "text": "We also propose an accurate pseudo label generation method through prototype learning.",
        "trunc_text": "We also propose an accurate pseudo label generation method through prototype learning.",
        "x1": 1.4329646825790405,
        "x2": 1.2646070718765259,
        "y1": 4.49319314956665,
        "y2": 3.7253894805908203
      },
      {
        "r": 0,
        "text": "Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities, representing distributions over classes in a classification setting, subject to a hyper-parameter encoding annotator reliability.",
        "trunc_text": "Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities, representing distr",
        "x1": 1.8738198280334473,
        "x2": 1.0409973859786987,
        "y1": 4.005356311798096,
        "y2": 4.666024208068848
      },
      {
        "r": 0,
        "text": "Based on this model, we propose a metric for measuring annotation uncertainty and provide uncertainty-adjusted metrics for performance evaluation.",
        "trunc_text": "Based on this model, we propose a metric for measuring annotation uncertainty and provide uncertainty-adjusted metrics f",
        "x1": 2.145151376724243,
        "x2": 1.2678393125534058,
        "y1": 3.7132136821746826,
        "y2": 4.994792938232422
      },
      {
        "r": 0,
        "text": "Identifying the samples with corrupted labels and preventing the model from learning them is a promising approach to address this challenge.",
        "trunc_text": "Identifying the samples with corrupted labels and preventing the model from learning them is a promising approach to add",
        "x1": 1.197170376777649,
        "x2": 0.8235670924186707,
        "y1": 4.626763343811035,
        "y2": 3.7118265628814697
      },
      {
        "r": 0,
        "text": "Furthermore, we detect real label errors a) on commonly used test datasets in object detection and b) on a proprietary dataset.",
        "trunc_text": "Furthermore, we detect real label errors a) on commonly used test datasets in object detection and b) on a proprietary d",
        "x1": 1.1507501602172852,
        "x2": 0.7981535792350769,
        "y1": 4.4136762619018555,
        "y2": 3.884089469909668
      },
      {
        "r": 0,
        "text": "Large-scale datasets in the real world inevitably involve label noise.",
        "trunc_text": "Large-scale datasets in the real world inevitably involve label noise.",
        "x1": 1.2102266550064087,
        "x2": 0.9940783381462097,
        "y1": 4.778994083404541,
        "y2": 3.6735386848449707
      },
      {
        "r": 0,
        "text": "This is partially due to the fact that obtaining a balanced, diverse, and perfectly labeled dataset is typically expensive, time-consuming, and error-prone.",
        "trunc_text": "This is partially due to the fact that obtaining a balanced, diverse, and perfectly labeled dataset is typically expensi",
        "x1": 1.5915151834487915,
        "x2": 0.9659712910652161,
        "y1": 4.223435878753662,
        "y2": 4.223451137542725
      },
      {
        "r": 0,
        "text": "We develop an efficient algorithm for detecting label errors and outlier data points based on the relational graph structure of the dataset.",
        "trunc_text": "We develop an efficient algorithm for detecting label errors and outlier data points based on the relational graph struc",
        "x1": 1.133567452430725,
        "x2": 0.8414997458457947,
        "y1": 4.4024338722229,
        "y2": 3.860548734664917
      },
      {
        "r": 0,
        "text": "By focusing on finding incorrect labels in the original training datasets, we can eliminate erroneous examples in their root.",
        "trunc_text": "By focusing on finding incorrect labels in the original training datasets, we can eliminate erroneous examples in their ",
        "x1": 1.084817886352539,
        "x2": 0.8308223485946655,
        "y1": 4.474538326263428,
        "y2": 3.8255608081817627
      },
      {
        "r": 0,
        "text": "Manually labelling data with high-quality labels is generally a time-consuming and challenging task and often this turns out to be the bottleneck in a machine learning project.",
        "trunc_text": "Manually labelling data with high-quality labels is generally a time-consuming and challenging task and often this turns",
        "x1": 1.4870582818984985,
        "x2": 1.0635595321655273,
        "y1": 4.517737865447998,
        "y2": 3.8855910301208496
      },
      {
        "r": 0,
        "text": "Here we consider algorithms for finding mislabeled examples in multi-label classification datasets.",
        "trunc_text": "Here we consider algorithms for finding mislabeled examples in multi-label classification datasets.",
        "x1": 1.2229326963424683,
        "x2": 0.7145206332206726,
        "y1": 4.332744121551514,
        "y2": 3.9414591789245605
      },
      {
        "r": 0,
        "text": "Negative labels are those that a corresponding data item does not belong.",
        "trunc_text": "Negative labels are those that a corresponding data item does not belong.",
        "x1": 1.3151285648345947,
        "x2": 0.841545820236206,
        "y1": 4.198698997497559,
        "y2": 4.050596237182617
      },
      {
        "r": 0,
        "text": "This issue is due to biased labeling preferences at multiple clients and is a typical setting of data heterogeneity.",
        "trunc_text": "This issue is due to biased labeling preferences at multiple clients and is a typical setting of data heterogeneity.",
        "x1": 1.4307705163955688,
        "x2": 0.854096531867981,
        "y1": 4.11042594909668,
        "y2": 4.244763374328613
      },
      {
        "r": 0,
        "text": "However, noisy samples (i.e., with wrong labels) in the training set induce confusion and cause the network to learn the incorrect representation.",
        "trunc_text": "However, noisy samples (i.e., with wrong labels) in the training set induce confusion and cause the network to learn the",
        "x1": 1.10179603099823,
        "x2": 0.7993497252464294,
        "y1": 4.614108085632324,
        "y2": 3.7657883167266846
      },
      {
        "r": 0,
        "text": "Mislabeled examples are a common issue in real-world data, particularly for tasks like token classification where many labels must be chosen on a fine-grained basis.",
        "trunc_text": "Mislabeled examples are a common issue in real-world data, particularly for tasks like token classification where many l",
        "x1": 1.165327548980713,
        "x2": 0.72903972864151,
        "y1": 4.256118297576904,
        "y2": 3.955397605895996
      },
      {
        "r": 0,
        "text": "We also introduced robust loss to reduce the noise effects of inaccurate labels generated in semi-supervised learning.",
        "trunc_text": "We also introduced robust loss to reduce the noise effects of inaccurate labels generated in semi-supervised learning.",
        "x1": 1.2925291061401367,
        "x2": 1.073124647140503,
        "y1": 4.836464881896973,
        "y2": 3.542257308959961
      },
      {
        "r": 0,
        "text": "The main anomaly was found by the autoencoder and automatically created labels and was also recorded in the log files.",
        "trunc_text": "The main anomaly was found by the autoencoder and automatically created labels and was also recorded in the log files.",
        "x1": 1.329445481300354,
        "x2": 0.7632624506950378,
        "y1": 4.129093647003174,
        "y2": 4.206085205078125
      },
      {
        "r": 0,
        "text": "About 0.2% of the images could not be assigned a label, while for 5.1% the reviewers were uncertain, or they assigned an invalid label.",
        "trunc_text": "About 0.2% of the images could not be assigned a label, while for 5.1% the reviewers were uncertain, or they assigned an",
        "x1": 1.445136547088623,
        "x2": 0.8785406351089478,
        "y1": 4.068277835845947,
        "y2": 4.151516914367676
      },
      {
        "r": 0,
        "text": "We find that the above issues are caused by the training dataset's pose imbalance.   ",
        "trunc_text": "We find that the above issues are caused by the training dataset's pose imbalance.   ",
        "x1": 1.4376842975616455,
        "x2": 0.7914334535598755,
        "y1": 4.081370830535889,
        "y2": 4.197567939758301
      },
      {
        "r": 0,
        "text": "The labor-intensive annotation process of semantic segmentation datasets is often prone to errors, since humans struggle to label every pixel correctly.",
        "trunc_text": "The labor-intensive annotation process of semantic segmentation datasets is often prone to errors, since humans struggle",
        "x1": 1.4896780252456665,
        "x2": 1.0654352903366089,
        "y1": 4.085546493530273,
        "y2": 4.429830551147461
      },
      {
        "r": 0,
        "text": "We study algorithms to automatically detect such annotation errors, in particular methods to score label quality, such that the images with the lowest scores are least likely to be correctly labeled.",
        "trunc_text": "We study algorithms to automatically detect such annotation errors, in particular methods to score label quality, such t",
        "x1": 1.3054003715515137,
        "x2": 0.9587587118148804,
        "y1": 4.340765476226807,
        "y2": 4.131924152374268
      },
      {
        "r": 0,
        "text": "Widely applicable, our label quality scores rely on probabilistic predictions from a trained segmentation model -- any model architecture and training procedure can be utilized.",
        "trunc_text": "Widely applicable, our label quality scores rely on probabilistic predictions from a trained segmentation model -- any m",
        "x1": 1.2800952196121216,
        "x2": 1.0197988748550415,
        "y1": 4.581414699554443,
        "y2": 3.7886035442352295
      },
      {
        "r": 0,
        "text": "Here we study 7 different label quality scoring methods used in conjunction with a DeepLabV3+ or a FPN segmentation model to detect annotation errors in a version of the SYNTHIA dataset.",
        "trunc_text": "Here we study 7 different label quality scoring methods used in conjunction with a DeepLabV3+ or a FPN segmentation mode",
        "x1": 1.3106257915496826,
        "x2": 0.9992606043815613,
        "y1": 4.375074863433838,
        "y2": 4.051216125488281
      },
      {
        "r": 0,
        "text": "Precision-recall evaluations reveal a score -- the soft-minimum of the model-estimated likelihoods of each pixel's annotated class -- that is particularly effective to identify images that are mislabeled, across multiple types of annotation error.",
        "trunc_text": "Precision-recall evaluations reveal a score -- the soft-minimum of the model-estimated likelihoods of each pixel's annot",
        "x1": 1.3741847276687622,
        "x2": 1.0513863563537598,
        "y1": 4.335787296295166,
        "y2": 4.329414367675781
      },
      {
        "r": 0,
        "text": "In recent years, research on learning with noisy labels has focused on devising novel algorithms that can achieve robustness to noisy training labels while generalizing to clean data.",
        "trunc_text": "In recent years, research on learning with noisy labels has focused on devising novel algorithms that can achieve robust",
        "x1": 1.1648350954055786,
        "x2": 0.9561638236045837,
        "y1": 4.858255386352539,
        "y2": 3.4860637187957764
      },
      {
        "r": 0,
        "text": "While some of these regularization strategies have been utilized in previous noisy label learning research, their full potential has not been thoroughly explored.",
        "trunc_text": "While some of these regularization strategies have been utilized in previous noisy label learning research, their full p",
        "x1": 1.3009039163589478,
        "x2": 1.015763759613037,
        "y1": 4.782287120819092,
        "y2": 3.467332601547241
      },
      {
        "r": 0,
        "text": "We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples.",
        "trunc_text": "We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show ",
        "x1": 1.1229095458984375,
        "x2": 0.8434463739395142,
        "y1": 4.676259517669678,
        "y2": 3.636540651321411
      },
      {
        "r": 0,
        "text": "Further analysis shows that these gains come from an improved decision boundary after cleaning the label errors existed in the training data.",
        "trunc_text": "Further analysis shows that these gains come from an improved decision boundary after cleaning the label errors existed ",
        "x1": 1.4131978750228882,
        "x2": 0.9399067163467407,
        "y1": 4.6476664543151855,
        "y2": 3.8539416790008545
      },
      {
        "r": 0,
        "text": "Nevertheless, few papers have tackled the data shift problem in labeled training sets, which occurs when there is a mismatch between the data distribution in the training set and the testing set.",
        "trunc_text": "Nevertheless, few papers have tackled the data shift problem in labeled training sets, which occurs when there is a mism",
        "x1": 1.025662899017334,
        "x2": 0.6982258558273315,
        "y1": 4.603549480438232,
        "y2": 3.8007969856262207
      },
      {
        "r": 0,
        "text": "In this work, we examine the problem for both labeled and unlabeled settings.",
        "trunc_text": "In this work, we examine the problem for both labeled and unlabeled settings.",
        "x1": 1.2807928323745728,
        "x2": 0.7973076105117798,
        "y1": 4.244944095611572,
        "y2": 4.111404895782471
      },
      {
        "r": 0,
        "text": "It is crucial to correctly predict areas that deviate from the background noise, in both the train and test sets of labels.   ",
        "trunc_text": "It is crucial to correctly predict areas that deviate from the background noise, in both the train and test sets of labe",
        "x1": 1.0903511047363281,
        "x2": 0.7141884565353394,
        "y1": 4.671914100646973,
        "y2": 3.735987424850464
      },
      {
        "r": 0,
        "text": "Data completeness is ensured through the label provided during training.",
        "trunc_text": "Data completeness is ensured through the label provided during training.",
        "x1": 1.5473320484161377,
        "x2": 1.0317517518997192,
        "y1": 4.424580097198486,
        "y2": 4.009219646453857
      },
      {
        "r": 0,
        "text": "Trustworthy pseudo labels on unlabeled data are generated after uncertainty estimation.",
        "trunc_text": "Trustworthy pseudo labels on unlabeled data are generated after uncertainty estimation.",
        "x1": 1.4488922357559204,
        "x2": 1.089038372039795,
        "y1": 4.3766865730285645,
        "y2": 3.872894525527954
      },
      {
        "r": 0,
        "text": "When random label noise is added to a training dataset, the prediction error of a neural network on a label-noise-free test dataset initially improves during early training but eventually deteriorates, following a U-shaped dependence on training time.",
        "trunc_text": "When random label noise is added to a training dataset, the prediction error of a neural network on a label-noise-free t",
        "x1": 1.2120171785354614,
        "x2": 0.8742138743400574,
        "y1": 4.722177982330322,
        "y2": 3.6630184650421143
      },
      {
        "r": 0,
        "text": "In this paper, we try to deal with error accumulation in noisy label learning from both model and data perspectives.",
        "trunc_text": "In this paper, we try to deal with error accumulation in noisy label learning from both model and data perspectives.",
        "x1": 1.1798828840255737,
        "x2": 0.9030884504318237,
        "y1": 4.817418575286865,
        "y2": 3.5960116386413574
      },
      {
        "r": 0,
        "text": "In our analysis, we find that SoundDesc contains several duplicates that cause leakage of training data to the evaluation data.",
        "trunc_text": "In our analysis, we find that SoundDesc contains several duplicates that cause leakage of training data to the evaluatio",
        "x1": 1.4332770109176636,
        "x2": 0.9100729823112488,
        "y1": 4.728957653045654,
        "y2": 3.7390294075012207
      },
      {
        "r": 0,
        "text": "However, in many situations, language can be ambiguous and ineffective in describing specific image edits.",
        "trunc_text": "However, in many situations, language can be ambiguous and ineffective in describing specific image edits.",
        "x1": 4.133939266204834,
        "x2": 2.722339391708374,
        "y1": 3.794924020767212,
        "y2": 4.7828145027160645
      },
      {
        "r": 0,
        "text": "We propose an automatic metric to test the prevalence of the opinions that a summary expresses, based on counting the number of reviews that are consistent with each statement in the summary, while discrediting trivial or redundant statements.",
        "trunc_text": "We propose an automatic metric to test the prevalence of the opinions that a summary expresses, based on counting the nu",
        "x1": 2.6521832942962646,
        "x2": 1.7438102960586548,
        "y1": 3.872727155685425,
        "y2": 5.324699401855469
      },
      {
        "r": 0,
        "text": "To formulate this opinion prevalence metric, we consider several existing methods to score the factual consistency of a summary statement with respect to each individual source review.",
        "trunc_text": "To formulate this opinion prevalence metric, we consider several existing methods to score the factual consistency of a ",
        "x1": 2.710181713104248,
        "x2": 1.7989501953125,
        "y1": 3.8770618438720703,
        "y2": 5.31026029586792
      },
      {
        "r": 0,
        "text": "On a corpus of Amazon product reviews, we gather multiple human judgments of the opinion consistency, to determine which automatic metric best expresses consistency in product reviews.",
        "trunc_text": "On a corpus of Amazon product reviews, we gather multiple human judgments of the opinion consistency, to determine which",
        "x1": 2.566878080368042,
        "x2": 1.7038135528564453,
        "y1": 3.929170608520508,
        "y2": 5.247227191925049
      },
      {
        "r": 0,
        "text": "The system utilizes a weakly supervised technique that employs a fine-grained annotation scheme to identify verbally formulated uncertainty at the sentence level in scientific texts.",
        "trunc_text": "The system utilizes a weakly supervised technique that employs a fine-grained annotation scheme to identify verbally for",
        "x1": 2.4502201080322266,
        "x2": 1.4277013540267944,
        "y1": 3.5767874717712402,
        "y2": 5.300434589385986
      },
      {
        "r": 0,
        "text": "Additionally, UnScientify provides interpretable results, aiding in the comprehension of identified instances of scientific uncertainty in text.",
        "trunc_text": "Additionally, UnScientify provides interpretable results, aiding in the comprehension of identified instances of scienti",
        "x1": 2.4039337635040283,
        "x2": 1.373960018157959,
        "y1": 3.68428373336792,
        "y2": 5.056083679199219
      },
      {
        "r": 0,
        "text": "Recent work in Machine Learning and Computer Vision has highlighted the presence of various types of systematic flaws inside ground truth object recognition benchmark datasets.",
        "trunc_text": "Recent work in Machine Learning and Computer Vision has highlighted the presence of various types of systematic flaws in",
        "x1": 0.752200722694397,
        "x2": 2.0961835384368896,
        "y1": 7.404778957366943,
        "y2": 1.6286711692810059
      },
      {
        "r": 0,
        "text": "The net consequence is that the current annotation process is largely under-specified, thus leaving too much freedom to the subjective judgment of annotators.",
        "trunc_text": "The net consequence is that the current annotation process is largely under-specified, thus leaving too much freedom to ",
        "x1": 2.2089686393737793,
        "x2": 1.2187864780426025,
        "y1": 3.633356809616089,
        "y2": 5.060505390167236
      },
      {
        "r": 0,
        "text": "Motivated by the optimal strategy, we introduce double-score OOD methods that leverage uncertainty scores from two chosen OOD detectors: one focused on OOD/ID discrimination and the other on misclassification detection.",
        "trunc_text": "Motivated by the optimal strategy, we introduce double-score OOD methods that leverage uncertainty scores from two chose",
        "x1": 3.5713911056518555,
        "x2": 3.937666416168213,
        "y1": 5.841258525848389,
        "y2": 3.8327479362487793
      },
      {
        "r": 0,
        "text": "The optimal prediction strategy for out-of-distribution (OOD) setups is a fundamental question in machine learning.",
        "trunc_text": "The optimal prediction strategy for out-of-distribution (OOD) setups is a fundamental question in machine learning.",
        "x1": 3.7262754440307617,
        "x2": 4.003433704376221,
        "y1": 5.845475673675537,
        "y2": 3.8981516361236572
      },
      {
        "r": 0,
        "text": "In this paper, we address this question and present several contributions.",
        "trunc_text": "In this paper, we address this question and present several contributions.",
        "x1": 4.9424824714660645,
        "x2": 6.130855560302734,
        "y1": 8.162092208862305,
        "y2": 4.199103355407715
      },
      {
        "r": 0,
        "text": "We propose three reject option models for OOD setups: the Cost-based model, the Bounded TPR-FPR model, and the Bounded Precision-Recall model.",
        "trunc_text": "We propose three reject option models for OOD setups: the Cost-based model, the Bounded TPR-FPR model, and the Bounded P",
        "x1": 3.780949831008911,
        "x2": 3.9579617977142334,
        "y1": 5.856971740722656,
        "y2": 3.9185733795166016
      },
      {
        "r": 0,
        "text": "These models extend the standard reject option models used in non-OOD setups and define the notion of an optimal OOD selective classifier.",
        "trunc_text": "These models extend the standard reject option models used in non-OOD setups and define the notion of an optimal OOD sel",
        "x1": 3.7267282009124756,
        "x2": 3.9781973361968994,
        "y1": 5.834372520446777,
        "y2": 3.922964572906494
      },
      {
        "r": 0,
        "text": "We establish that all the proposed models, despite their different formulations, share a common class of optimal strategies.  ",
        "trunc_text": "We establish that all the proposed models, despite their different formulations, share a common class of optimal strateg",
        "x1": 3.756401300430298,
        "x2": 3.907088041305542,
        "y1": 5.421505928039551,
        "y2": 4.454761505126953
      },
      {
        "r": 0,
        "text": "The experimental results consistently demonstrate the superior performance of this simple strategy compared to state-of-the-art methods.",
        "trunc_text": "The experimental results consistently demonstrate the superior performance of this simple strategy compared to state-of-",
        "x1": 5.3746562004089355,
        "x2": 6.472570419311523,
        "y1": 8.77299690246582,
        "y2": 4.821121692657471
      },
      {
        "r": 0,
        "text": "Additionally, we propose novel evaluation metrics derived from the definition of the optimal strategy under the proposed OOD rejection models.",
        "trunc_text": "Additionally, we propose novel evaluation metrics derived from the definition of the optimal strategy under the proposed",
        "x1": 3.684338331222534,
        "x2": 3.971332550048828,
        "y1": 5.83640193939209,
        "y2": 3.9166102409362793
      },
      {
        "r": 0,
        "text": "These new metrics provide a comprehensive and reliable assessment of OOD methods without the deficiencies observed in existing evaluation approaches.",
        "trunc_text": "These new metrics provide a comprehensive and reliable assessment of OOD methods without the deficiencies observed in ex",
        "x1": 3.808814764022827,
        "x2": 4.112974643707275,
        "y1": 5.951656818389893,
        "y2": 3.8140294551849365
      },
      {
        "r": 0,
        "text": "This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels",
        "trunc_text": "This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplic",
        "x1": 2.4369170665740967,
        "x2": 4.029406547546387,
        "y1": 7.056494235992432,
        "y2": 2.191927909851074
      },
      {
        "r": 0,
        "text": "Neural networks are overparametrized and easily overfit the datasets they train on.",
        "trunc_text": "Neural networks are overparametrized and easily overfit the datasets they train on.",
        "x1": 4.1228861808776855,
        "x2": 5.238649845123291,
        "y1": 5.7766337394714355,
        "y2": 3.6355674266815186
      },
      {
        "r": 0,
        "text": "In the extreme case, it is shown that they can memorize a training set with fully randomized labels.",
        "trunc_text": "In the extreme case, it is shown that they can memorize a training set with fully randomized labels.",
        "x1": 2.581221580505371,
        "x2": 3.397634983062744,
        "y1": 6.1503376960754395,
        "y2": 2.7103383541107178
      },
      {
        "r": 0,
        "text": "We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs.",
        "trunc_text": "We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged ov",
        "x1": 2.708097219467163,
        "x2": 3.443150520324707,
        "y1": 6.148776531219482,
        "y2": 2.8460049629211426
      },
      {
        "r": 0,
        "text": "We use this to study the generalization versus memorization properties of different samples in popular image datasets.",
        "trunc_text": "We use this to study the generalization versus memorization properties of different samples in popular image datasets.",
        "x1": 2.7429797649383545,
        "x2": 3.553194761276245,
        "y1": 6.325644016265869,
        "y2": 2.7960445880889893
      },
      {
        "r": 0,
        "text": "We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples. .",
        "trunc_text": "We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tai",
        "x1": 1.2365580797195435,
        "x2": 0.9813453555107117,
        "y1": 4.890194892883301,
        "y2": 3.34266996383667
      },
      {
        "r": 0,
        "text": "We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields",
        "trunc_text": "We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show ",
        "x1": 1.1078293323516846,
        "x2": 0.8567590117454529,
        "y1": 4.736433982849121,
        "y2": 3.560281991958618
      },
      {
        "r": 0,
        "text": "Medical image classification is a challenging task due to the scarcity of labeled samples and class imbalance caused by the high variance in disease prevalence.",
        "trunc_text": "Medical image classification is a challenging task due to the scarcity of labeled samples and class imbalance caused by ",
        "x1": 1.5018295049667358,
        "x2": 2.1977789402008057,
        "y1": 6.026821136474609,
        "y2": 2.582138776779175
      },
      {
        "r": 0,
        "text": "Semi-supervised learning (SSL) methods can mitigate these challenges by leveraging both labeled and unlabeled data.",
        "trunc_text": "Semi-supervised learning (SSL) methods can mitigate these challenges by leveraging both labeled and unlabeled data.",
        "x1": 1.4730829000473022,
        "x2": 1.5560723543167114,
        "y1": 4.924098491668701,
        "y2": 3.2422165870666504
      },
      {
        "r": 0,
        "text": "However, SSL methods for medical image classification need to address two key challenges: (1) estimating reliable pseudo-labels for the images in the unlabeled dataset and (2) reducing biases caused by class imbalance.",
        "trunc_text": "However, SSL methods for medical image classification need to address two key challenges: (1) estimating reliable pseudo",
        "x1": 1.5620741844177246,
        "x2": 1.5736472606658936,
        "y1": 5.101099014282227,
        "y2": 3.0981390476226807
      },
      {
        "r": 0,
        "text": "In this paper, we propose a novel SSL approach, SPLAL, that effectively addresses these challenges.",
        "trunc_text": "In this paper, we propose a novel SSL approach, SPLAL, that effectively addresses these challenges.",
        "x1": 5.900209426879883,
        "x2": 1.6226084232330322,
        "y1": 5.489798069000244,
        "y2": 2.9418094158172607
      },
      {
        "r": 0,
        "text": "SPLAL leverages class prototypes and a weighted combination of classifiers to predict reliable pseudo-labels over a subset of unlabeled images.",
        "trunc_text": "SPLAL leverages class prototypes and a weighted combination of classifiers to predict reliable pseudo-labels over a subs",
        "x1": 1.3676605224609375,
        "x2": 1.2280521392822266,
        "y1": 4.488833427429199,
        "y2": 3.796199321746826
      },
      {
        "r": 0,
        "text": "Additionally, we introduce alignment loss to mitigate model biases toward majority classes.",
        "trunc_text": "Additionally, we introduce alignment loss to mitigate model biases toward majority classes.",
        "x1": 3.2991859912872314,
        "x2": 3.489288806915283,
        "y1": 6.026499271392822,
        "y2": 3.9181134700775146
      },
      {
        "r": 0,
        "text": "To evaluate the performance of our proposed approach, we conduct experiments on two publicly available medical image classification benchmark datasets: the skin lesion classification (ISIC 2018) and the blood cell classification dataset (BCCD).",
        "trunc_text": "To evaluate the performance of our proposed approach, we conduct experiments on two publicly available medical image cla",
        "x1": 3.192481756210327,
        "x2": 4.161755561828613,
        "y1": 7.506954669952393,
        "y2": 2.356980562210083
      },
      {
        "r": 0,
        "text": "The experimental results empirically demonstrate that our approach outperforms several state-of-the-art SSL methods over various evaluation metrics.",
        "trunc_text": "The experimental results empirically demonstrate that our approach outperforms several state-of-the-art SSL methods over",
        "x1": 5.79074239730835,
        "x2": 1.6259474754333496,
        "y1": 5.541418552398682,
        "y2": 2.898911237716675
      },
      {
        "r": 0,
        "text": "Specifically, our proposed approach achieves a significant improvement over the state-of-the-art approach on the ISIC 2018 dataset in both Accuracy and F1 score, with relative margins of 2.24\\% and 11.40\\%, respectively.",
        "trunc_text": "Specifically, our proposed approach achieves a significant improvement over the state-of-the-art approach on the ISIC 20",
        "x1": 4.189556121826172,
        "x2": 4.924854278564453,
        "y1": 6.25363826751709,
        "y2": 3.6141369342803955
      },
      {
        "r": 0,
        "text": "Finally, we conduct extensive ablation experiments to examine the contribution of different components of our approach, validating its effectiveness.",
        "trunc_text": "Finally, we conduct extensive ablation experiments to examine the contribution of different components of our approach, ",
        "x1": 5.818718433380127,
        "x2": 6.660104751586914,
        "y1": 8.708637237548828,
        "y2": 5.253708839416504
      },
      {
        "r": 0,
        "text": "Textual noise, such as typos or abbreviations, is a well-known issue that penalizes vanilla Transformers for most downstream tasks",
        "trunc_text": "Textual noise, such as typos or abbreviations, is a well-known issue that penalizes vanilla Transformers for most downst",
        "x1": 3.722355604171753,
        "x2": 2.314546823501587,
        "y1": 3.5277841091156006,
        "y2": 5.49077844619751
      },
      {
        "r": 0,
        "text": "Previous works addressing the noise issue mainly rely on data augmentation strategies, showing improved robustness when dealing with corrupted samples that are similar to the ones used for training.",
        "trunc_text": "Previous works addressing the noise issue mainly rely on data augmentation strategies, showing improved robustness when ",
        "x1": 1.062437653541565,
        "x2": 0.9612100720405579,
        "y1": 4.9580793380737305,
        "y2": 3.407785415649414
      },
      {
        "r": 0,
        "text": "However, all these methods still suffer from the token distribution shift induced by typos",
        "trunc_text": "However, all these methods still suffer from the token distribution shift induced by typos",
        "x1": 3.725743055343628,
        "x2": 2.3550472259521484,
        "y1": 3.4252145290374756,
        "y2": 5.620779037475586
      },
      {
        "r": 0,
        "text": "We show that this is also the case for sentence similarity, a fundamental task in multiple domains, e.g. matching, retrieval or paraphrasing.",
        "trunc_text": "We show that this is also the case for sentence similarity, a fundamental task in multiple domains, e.g. matching, retri",
        "x1": 3.534883975982666,
        "x2": 2.4918322563171387,
        "y1": 3.36822772026062,
        "y2": 5.83308219909668
      },
      {
        "r": 0,
        "text": "Sentence similarity can be approached using cross-encoders, where the two sentences are concatenated in the input allowing the model to exploit the inter-relations between them.",
        "trunc_text": "Sentence similarity can be approached using cross-encoders, where the two sentences are concatenated in the input allowi",
        "x1": 3.604038953781128,
        "x2": 2.6435508728027344,
        "y1": 3.5231871604919434,
        "y2": 5.671363353729248
      },
      {
        "r": 0,
        "text": "Previous works addressing the noise issue mainly rely on data augmentation strategies, showing improved robustness when dealing wixtual noise by equipping cross-encoders with a novel LExical-aware Attention module (LEA) that incorporates lexical similarities between words in both sentences.",
        "trunc_text": "Previous works addressing the noise issue mainly rely on data augmentation strategies, showing improved robustness when ",
        "x1": 3.87611985206604,
        "x2": 2.5036885738372803,
        "y1": 3.6172194480895996,
        "y2": 5.096656322479248
      },
      {
        "r": 0,
        "text": "By using raw text similarities, our ae that the attention bias introduced by LEA helps cross-encoders to tackle complex scenarios with textual noise, specially in domains with short-text descriptions and limited context.",
        "trunc_text": "By using raw text similarities, our ae that the attention bias introduced by LEA helps cross-encoders to tackle complex ",
        "x1": 3.887265205383301,
        "x2": 2.566789150238037,
        "y1": 3.662515878677368,
        "y2": 5.168957233428955
      },
      {
        "r": 0,
        "text": "Experiments using three popular Transformer encoders in five e-commerce datasets for product matching show that LEA consistently boosts performance under the presence of noise, while remaining competitive on the original (clean) splits.",
        "trunc_text": "Experiments using three popular Transformer encoders in five e-commerce datasets for product matching show that LEA cons",
        "x1": 4.353145599365234,
        "x2": 4.023463726043701,
        "y1": 4.7864837646484375,
        "y2": 5.070657253265381
      },
      {
        "r": 0,
        "text": "We also evaluate our approach in two datasets for textual entailment and paraphrasing showing that LEA is robust to typos in domains with longer sentences and more natural context.",
        "trunc_text": "We also evaluate our approach in two datasets for textual entailment and paraphrasing showing that LEA is robust to typo",
        "x1": 3.3749349117279053,
        "x2": 2.493041515350342,
        "y1": 3.4745922088623047,
        "y2": 5.740031719207764
      },
      {
        "r": 0,
        "text": "Additionally, we thoroughly analyze several design choices in our approach, providing insights about the impact of the decisions made and fostering future research in cross-encoders dealing with typos.",
        "trunc_text": "Additionally, we thoroughly analyze several design choices in our approach, providing insights about the impact of the d",
        "x1": 3.7446298599243164,
        "x2": 2.4123871326446533,
        "y1": 3.5388400554656982,
        "y2": 5.301920413970947
      },
      {
        "r": 0,
        "text": "For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ground truth that is assumed certain.",
        "trunc_text": "For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ",
        "x1": 3.7550294399261475,
        "x2": 3.899700403213501,
        "y1": 5.9153971672058105,
        "y2": 4.340871334075928
      },
      {
        "r": 0,
        "text": "However, this is actually not the case and the ground truth may be uncertain.",
        "trunc_text": "However, this is actually not the case and the ground truth may be uncertain.",
        "x1": 2.0367367267608643,
        "x2": 1.4237301349639893,
        "y1": 4.243837356567383,
        "y2": 4.493960380554199
      },
      {
        "r": 0,
        "text": "Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as overestimating the future performance.",
        "trunc_text": "Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as over",
        "x1": 3.834899663925171,
        "x2": 3.786989688873291,
        "y1": 5.803359508514404,
        "y2": 4.416810989379883
      },
      {
        "r": 0,
        "text": "To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: annotation uncertainty which stems from the lack of reliable annotations, and inherent uncertainty due to limited observational information.",
        "trunc_text": "To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: ",
        "x1": 2.1280784606933594,
        "x2": 1.4358216524124146,
        "y1": 4.141843318939209,
        "y2": 4.54749870300293
      },
      {
        "r": 0,
        "text": "This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, e.g., by majority voting or averaging.",
        "trunc_text": "This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, ",
        "x1": 2.0750370025634766,
        "x2": 1.4703229665756226,
        "y1": 4.243069171905518,
        "y2": 4.446512222290039
      },
      {
        "r": 0,
        "text": "In contrast, we propose a framework where aggregation is done using a statistical model.  ",
        "trunc_text": "In contrast, we propose a framework where aggregation is done using a statistical model.  ",
        "x1": 4.0118303298950195,
        "x2": 5.165231227874756,
        "y1": 6.233093738555908,
        "y2": 3.4766435623168945
      },
      {
        "r": 0,
        "text": "We present a case study applying our framework to skin condition classification fromtion (IRN) from previous work ignores ground truth uncertainty in evaluation.",
        "trunc_text": "We present a case study applying our framework to skin condition classification fromtion (IRN) from previous work ignore",
        "x1": 3.142427444458008,
        "x2": 4.302433013916016,
        "y1": 7.684358596801758,
        "y2": 1.93464994430542
      },
      {
        "r": 0,
        "text": "Instead, we present two alternative statistical models: a probabilistic version of IRN and a Plackett-Luce-based model.",
        "trunc_text": "Instead, we present two alternative statistical models: a probabilistic version of IRN and a Plackett-Luce-based model.",
        "x1": 4.036978721618652,
        "x2": 4.863900661468506,
        "y1": 6.135200023651123,
        "y2": 3.585853338241577
      },
      {
        "r": 0,
        "text": "We find that a large portion of the dataset exhibits significant ground truth uncertainty and standard IRN-based evaluation severely over-estimates performance without providing uncertainty estimates.",
        "trunc_text": "We find that a large portion of the dataset exhibits significant ground truth uncertainty and standard IRN-based evaluat",
        "x1": 2.0601367950439453,
        "x2": 1.500626564025879,
        "y1": 4.287288188934326,
        "y2": 4.434758186340332
      },
      {
        "r": 0,
        "text": "To systematically combat confirmation bias for pseudo-labeling-based entity alignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment (UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the accuracy of entity alignment",
        "trunc_text": "To systematically combat confirmation bias for pseudo-labeling-based entity alignment, we propose a Unified Pseudo-Label",
        "x1": 1.7886161804199219,
        "x2": 1.4177063703536987,
        "y1": 3.9584877490997314,
        "y2": 4.306463241577148
      },
      {
        "r": 0,
        "text": "The two components are respectively designed to eliminate Type I and Type II pseudo-labeling errors identified through our analyse.",
        "trunc_text": "The two components are respectively designed to eliminate Type I and Type II pseudo-labeling errors identified through o",
        "x1": 1.5698906183242798,
        "x2": 1.030910611152649,
        "y1": 4.0583720207214355,
        "y2": 4.166988372802734
      },
      {
        "r": 0,
        "text": "The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both theoretically supported and experimentally validated.",
        "trunc_text": "The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both theoretically supported and experimentally val",
        "x1": 1.629009485244751,
        "x2": 1.1114295721054077,
        "y1": 3.998248815536499,
        "y2": 4.157309532165527
      },
      {
        "r": 0,
        "text": "Entity alignment (EA) aims at identifying equivalent entity pairs across different knowledge graphs (KGs) that refer to the same real-world identity. .",
        "trunc_text": "Entity alignment (EA) aims at identifying equivalent entity pairs across different knowledge graphs (KGs) that refer to ",
        "x1": 1.9860864877700806,
        "x2": 1.6657072305679321,
        "y1": 3.936276435852051,
        "y2": 4.375977039337158
      },
      {
        "r": 0,
        "text": "UPL-EA consists of two complementary components: (1) The Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as an effective means to enable more accurate determination of entity correspondences across two KGs and to mitigate the adverse impact of erroneous matches.",
        "trunc_text": "UPL-EA consists of two complementary components: (1) The Optimal Transport (OT)-based pseudo-labeling uses discrete OT m",
        "x1": 1.822018027305603,
        "x2": 1.2940073013305664,
        "y1": 3.9176323413848877,
        "y2": 4.356104850769043
      },
      {
        "r": 0,
        "text": "A simple but highly effective criterion is further devised to derive pseudo-labeled entity pairs that satisfy one-to-one correspondences at each iteration.",
        "trunc_text": "A simple but highly effective criterion is further devised to derive pseudo-labeled entity pairs that satisfy one-to-one",
        "x1": 1.9287879467010498,
        "x2": 1.512602686882019,
        "y1": 3.939877986907959,
        "y2": 4.337332248687744
      },
      {
        "r": 0,
        "text": "(2) The cross-iteration pseudo-label calibration operates across multiple consecutive iterations to further improve the pseudo-labeling precision rate by reducing the local pseudo-label selection variability with a theoretical guarantee.",
        "trunc_text": "(2) The cross-iteration pseudo-label calibration operates across multiple consecutive iterations to further improve the ",
        "x1": 1.5821236371994019,
        "x2": 1.2576696872711182,
        "y1": 4.388609409332275,
        "y2": 3.9662654399871826
      },
      {
        "r": 0,
        "text": "The calibrated pseudo-labels are thereafter used to augment prior alignment seeds to reinforce subsequent model training fomentally validated.",
        "trunc_text": "The calibrated pseudo-labels are thereafter used to augment prior alignment seeds to reinforce subsequent model training",
        "x1": 1.5967544317245483,
        "x2": 1.2848511934280396,
        "y1": 4.460693359375,
        "y2": 4.002626419067383
      },
      {
        "r": 0,
        "text": "The experimental results show that our approach achieves competitive performance with limited prior alignment seeds.",
        "trunc_text": "The experimental results show that our approach achieves competitive performance with limited prior alignment seeds.",
        "x1": 3.5389792919158936,
        "x2": 3.673607349395752,
        "y1": 6.160285472869873,
        "y2": 3.980980396270752
      },
      {
        "r": 0,
        "text": "A novel annotation method was used to collect three separate annotations for each region of interest, and these annotations were performed in a fully transparent setting using a web-based annotation tool.",
        "trunc_text": "A novel annotation method was used to collect three separate annotations for each region of interest, and these annotati",
        "x1": 2.296050786972046,
        "x2": 1.3691205978393555,
        "y1": 3.558257818222046,
        "y2": 5.320873737335205
      },
      {
        "r": 0,
        "text": "This paper presents the challenge report for the 2021 Kidney and Kidney Tumor Segmentation Challenge (KiTS21) held in conjunction with the 2021 international conference on Medical Image Computing and Computer Assisted Interventions (MICCAI).",
        "trunc_text": "This paper presents the challenge report for the 2021 Kidney and Kidney Tumor Segmentation Challenge (KiTS21) held in co",
        "x1": 0.8082640767097473,
        "x2": 2.607816457748413,
        "y1": 7.045889377593994,
        "y2": 1.4617329835891724
      },
      {
        "r": 0,
        "text": "KiTS21 is a sequel to its first edition in 2019, and it features a variety of innovations in how the challenge was designed, in addition to a larger dataset.  ",
        "trunc_text": "KiTS21 is a sequel to its first edition in 2019, and it features a variety of innovations in how the challenge was desig",
        "x1": 4.90456485748291,
        "x2": 5.141799449920654,
        "y1": 7.584686756134033,
        "y2": 2.3185956478118896
      },
      {
        "r": 0,
        "text": "Further, the KiTS21 test set was collected from an outside institution, challenging participants to develop methods that generalize well to new populations.",
        "trunc_text": "Further, the KiTS21 test set was collected from an outside institution, challenging participants to develop methods that",
        "x1": 4.837043285369873,
        "x2": 5.128446102142334,
        "y1": 7.623696804046631,
        "y2": 2.3100855350494385
      },
      {
        "r": 0,
        "text": "Nonetheless, the top-performing teams achieved a significant improvement over the state of the art set in 2019, and this performance is shown to inch ever closer to human-level performance.",
        "trunc_text": "Nonetheless, the top-performing teams achieved a significant improvement over the state of the art set in 2019, and this",
        "x1": 3.92399001121521,
        "x2": 4.352541446685791,
        "y1": 6.257164001464844,
        "y2": 3.611525058746338
      },
      {
        "r": 0,
        "text": "An in-depth meta-analysis is presented describing which methods were used and how they faired on the leaderboard, as well as the characteristics of which cases generally saw good performance, and which did not.",
        "trunc_text": "An in-depth meta-analysis is presented describing which methods were used and how they faired on the leaderboard, as wel",
        "x1": 4.587944507598877,
        "x2": 4.882124423980713,
        "y1": 6.317115306854248,
        "y2": 3.9287970066070557
      },
      {
        "r": 0,
        "text": "Overall KiTS21 facilitated a significant advancement in the state of the art in kidney tumor segmentation, and provides useful insights that are applicable to the field of semantic segmentation as a whole.",
        "trunc_text": "Overall KiTS21 facilitated a significant advancement in the state of the art in kidney tumor segmentation, and provides ",
        "x1": 0.7829458713531494,
        "x2": 2.5956475734710693,
        "y1": 7.004218101501465,
        "y2": 1.455623745918274
      },
      {
        "r": 0,
        "text": "Additionally, label noise is inevitable in large-scale annotations and hinders the applications of learning-based models.",
        "trunc_text": "Additionally, label noise is inevitable in large-scale annotations and hinders the applications of learning-based models",
        "x1": 1.4540648460388184,
        "x2": 1.1588109731674194,
        "y1": 4.817701816558838,
        "y2": 3.72249174118042
      },
      {
        "r": 0,
        "text": "To tackle such a critical yet thorny problem, this paper focuses on reducing noise based on some inherent properties of multi-label classification and long-tailed learning under noisy cases",
        "trunc_text": "To tackle such a critical yet thorny problem, this paper focuses on reducing noise based on some inherent properties of ",
        "x1": 1.2846091985702515,
        "x2": 0.9317973852157593,
        "y1": 4.750461578369141,
        "y2": 3.468651533126831
      },
      {
        "r": 0,
        "text": "In detail, we propose a Stitch-Up augmentation to synthesize a cleaner sample, which directly reduces multi-label noise by stitching up multiple noisy training samples",
        "trunc_text": "In detail, we propose a Stitch-Up augmentation to synthesize a cleaner sample, which directly reduces multi-label noise ",
        "x1": 1.0641003847122192,
        "x2": 1.027782678604126,
        "y1": 4.99658203125,
        "y2": 3.3950483798980713
      },
      {
        "r": 0,
        "text": "In real-world scenarios, collected and annotated data often exhibit the characteristics of multiple classes and long-tailed distribution.  ",
        "trunc_text": "In real-world scenarios, collected and annotated data often exhibit the characteristics of multiple classes and long-tai",
        "x1": 1.4035329818725586,
        "x2": 0.9072651863098145,
        "y1": 4.644693851470947,
        "y2": 3.4553780555725098
      },
      {
        "r": 0,
        "text": "Although many deep learning based methods have been proposed for handling long-tailed multi-label recognition or label noise respectively, learning with noisy labels in long-tailed multi-label visual data has not been well-studied because of the complexity of long-tailed distribution entangled with multi-label correlation.",
        "trunc_text": "Although many deep learning based methods have been proposed for handling long-tailed multi-label recognition or label n",
        "x1": 1.2943947315216064,
        "x2": 0.9036538600921631,
        "y1": 4.805636405944824,
        "y2": 3.4978625774383545
      },
      {
        "r": 0,
        "text": "To tackle such a critical yet thorny problem, this paper focuses on reducing noise based on some inherent properties of m by stitching up multiple noisy training samples.",
        "trunc_text": "To tackle such a critical yet thorny problem, this paper focuses on reducing noise based on some inherent properties of ",
        "x1": 1.0447540283203125,
        "x2": 0.9578074216842651,
        "y1": 4.953096389770508,
        "y2": 3.389403820037842
      },
      {
        "r": 0,
        "text": "Equipped with Stitch-Up, a Heterogeneous Co-Learning framework is further designed to leverage the inconsistency between long-tailed and balamarks, named VOC-MLT-Noise and COCO-MLT-Noise, respectively.",
        "trunc_text": "Equipped with Stitch-Up, a Heterogeneous Co-Learning framework is further designed to leverage the inconsistency between",
        "x1": 1.0825483798980713,
        "x2": 1.0688261985778809,
        "y1": 5.027028560638428,
        "y2": 3.2470550537109375
      },
      {
        "r": 0,
        "text": "Most of the existing methods adopt a coarse-grained fixed label assignment strategy and suffer from the inconsistency between the classification score and localization accuracy.",
        "trunc_text": "Most of the existing methods adopt a coarse-grained fixed label assignment strategy and suffer from the inconsistency be",
        "x1": 1.293299674987793,
        "x2": 1.0208553075790405,
        "y1": 4.585774898529053,
        "y2": 3.959149122238159
      },
      {
        "r": 0,
        "text": "Second, to further address the inconsistency between classification and localization, we propose a critical feature sampling (CFS) module, which performs localization refinement on the sampling location for classification task to extract critical features accurately",
        "trunc_text": "Second, to further address the inconsistency between classification and localization, we propose a critical feature samp",
        "x1": 0.7573336362838745,
        "x2": 2.3154938220977783,
        "y1": 8.080541610717773,
        "y2": 0.8935601115226746
      },
      {
        "r": 0,
        "text": "Arbitrary-oriented object detection is a relatively emerging but challenging task.",
        "trunc_text": "Arbitrary-oriented object detection is a relatively emerging but challenging task.",
        "x1": 0.5754233002662659,
        "x2": 2.0941977500915527,
        "y1": 7.532894611358643,
        "y2": 1.3495814800262451
      },
      {
        "r": 0,
        "text": "Although remarkable progress has been made, there still remain many unsolved issues due to the large diversity of patterns in orientation, scale, aspect ratio, and visual appearance of objects in aerial images.  ",
        "trunc_text": "Although remarkable progress has been made, there still remain many unsolved issues due to the large diversity of patter",
        "x1": 1.642339825630188,
        "x2": 4.258471965789795,
        "y1": 8.114042282104492,
        "y2": 1.2380168437957764
      },
      {
        "r": 0,
        "text": "First, to align the metric inconsistency between sample selection and regression loss calculation caused by fixed IoU strategy, we introduce affine transformation to evaluate the quality of samples and propose a distance-based label assignment strategy.",
        "trunc_text": "First, to align the metric inconsistency between sample selection and regression loss calculation caused by fixed IoU st",
        "x1": 1.884643793106079,
        "x2": 1.361863613128662,
        "y1": 4.666077613830566,
        "y2": 3.9408464431762695
      },
      {
        "r": 0,
        "text": "The proposed metric-aligned selection (MAS) strategy can dynamically select samples according to the shape and rotation characteristic of objects.",
        "trunc_text": "The proposed metric-aligned selection (MAS) strategy can dynamically select samples according to the shape and rotation ",
        "x1": 3.5704126358032227,
        "x2": 4.014305591583252,
        "y1": 5.922422885894775,
        "y2": 3.8596689701080322
      },
      {
        "r": 0,
        "text": "Second, to further address the inconsistency between classification and localization, we propose a critical feature sampling (CFS) module, which performs localization refinementtics of proposals during training.",
        "trunc_text": "Second, to further address the inconsistency between classification and localization, we propose a critical feature samp",
        "x1": 0.7679427862167358,
        "x2": 2.2850828170776367,
        "y1": 8.02199935913086,
        "y2": 0.9748217463493347
      },
      {
        "r": 0,
        "text": "Extensive experiments are conducted on four challenging rotated object detection datasets DOTA, FAIR1M-1.0, HRSC2016, and UCAS-AOD.",
        "trunc_text": "Extensive experiments are conducted on four challenging rotated object detection datasets DOTA, FAIR1M-1.0, HRSC2016, an",
        "x1": 0.6822510361671448,
        "x2": 2.150249481201172,
        "y1": 7.460860252380371,
        "y2": 1.4751530885696411
      },
      {
        "r": 0,
        "text": "The results show the state-of-the-art accuracy of the proposed detector.",
        "trunc_text": "The results show the state-of-the-art accuracy of the proposed detector.",
        "x1": 0.8663927912712097,
        "x2": 2.3756823539733887,
        "y1": 6.991477966308594,
        "y2": 1.8865333795547485
      },
      {
        "r": 0,
        "text": "However, results from even highly accurate methods require manual verification and correction",
        "trunc_text": "However, results from even highly accurate methods require manual verification and correction",
        "x1": 4.942206859588623,
        "x2": 5.8132171630859375,
        "y1": 7.906135082244873,
        "y2": 4.415291786193848
      },
      {
        "r": 0,
        "text": "The reviewers corrected 62.8% of the labels and agreed with the model label in 31.9% of cases.",
        "trunc_text": "The reviewers corrected 62.8% of the labels and agreed with the model label in 31.9% of cases.",
        "x1": 1.4396014213562012,
        "x2": 0.9109403491020203,
        "y1": 4.141282081604004,
        "y2": 4.1610236167907715
      },
      {
        "r": 0,
        "text": "We learned that our automatic transcription is biased towards the most frequent codes, with a higher degree of misclassification for the lowest frequency codes",
        "trunc_text": "We learned that our automatic transcription is biased towards the most frequent codes, with a higher degree of misclassi",
        "x1": 2.4051856994628906,
        "x2": 1.8835209608078003,
        "y1": 4.463009357452393,
        "y2": 4.562937259674072
      },
      {
        "r": 0,
        "text": "Machine learning methods have proven useful in transcribing historical data. .",
        "trunc_text": "Machine learning methods have proven useful in transcribing historical data. .",
        "x1": 4.353946685791016,
        "x2": 5.110598087310791,
        "y1": 7.3202409744262695,
        "y2": 2.8761632442474365
      },
      {
        "r": 0,
        "text": "Such manual review can be time-consuming and expensive, therefore the objective of this paper was to make it more efficient.",
        "trunc_text": "Such manual review can be time-consuming and expensive, therefore the objective of this paper was to make it more effici",
        "x1": 4.819736957550049,
        "x2": 5.702216148376465,
        "y1": 5.669730186462402,
        "y2": 4.629798889160156
      },
      {
        "r": 0,
        "text": "Previously, we used machine learning to transcribe 2.3 million handwritten occupation codes from the Norwegian 1950 census with high accuracy (97%).",
        "trunc_text": "Previously, we used machine learning to transcribe 2.3 million handwritten occupation codes from the Norwegian 1950 cens",
        "x1": 4.625121116638184,
        "x2": 1.6321616172790527,
        "y1": 7.307800769805908,
        "y2": 6.947302341461182
      },
      {
        "r": 0,
        "text": "We manually reviewed the 90,000 (3%) codes with the lowest model confidence.",
        "trunc_text": "We manually reviewed the 90,000 (3%) codes with the lowest model confidence.",
        "x1": 2.481757879257202,
        "x2": 5.442082405090332,
        "y1": 4.373016834259033,
        "y2": 4.743026256561279
      },
      {
        "r": 0,
        "text": "We allocated those 90,000 codes to human reviewers, who used our annotation tool to review the codes.",
        "trunc_text": "We allocated those 90,000 codes to human reviewers, who used our annotation tool to review the codes.",
        "x1": 2.5679268836975098,
        "x2": 5.509838104248047,
        "y1": 4.285127639770508,
        "y2": 4.812237739562988
      },
      {
        "r": 0,
        "text": "To assess reviewer agreement, some codes were assigned to multiple reviewers.",
        "trunc_text": "To assess reviewer agreement, some codes were assigned to multiple reviewers.",
        "x1": 2.573486089706421,
        "x2": 5.523892402648926,
        "y1": 4.272398471832275,
        "y2": 4.8590497970581055
      },
      {
        "r": 0,
        "text": "We then analyzed the review results to understand the relationship between accuracy improvements and effort.",
        "trunc_text": "We then analyzed the review results to understand the relationship between accuracy improvements and effort.",
        "x1": 4.819299221038818,
        "x2": 5.577016830444336,
        "y1": 5.794455051422119,
        "y2": 4.479891777038574
      },
      {
        "r": 0,
        "text": "Additionally, we interviewed the reviewers to improve the workflow.",
        "trunc_text": "Additionally, we interviewed the reviewers to improve the workflow.",
        "x1": 4.910011291503906,
        "x2": 5.6306562423706055,
        "y1": 5.65638542175293,
        "y2": 4.693981647491455
      },
      {
        "r": 0,
        "text": "The reviewers corrected 62.8% of the labels and agreed with the model label in 31.9% of casescertain, or they assigned an invalid label.",
        "trunc_text": "The reviewers corrected 62.8% of the labels and agreed with the model label in 31.9% of casescertain, or they assigned a",
        "x1": 1.4451935291290283,
        "x2": 0.8947390913963318,
        "y1": 4.089163780212402,
        "y2": 4.1683125495910645
      },
      {
        "r": 0,
        "text": "9,000 images were independently reviewed by multiplds the most frequent codes, with a higher degree of misclassification for the lowest frequency codes.",
        "trunc_text": "9,000 images were independently reviewed by multiplds the most frequent codes, with a higher degree of misclassification",
        "x1": 2.415956735610962,
        "x2": 5.3178863525390625,
        "y1": 4.4936089515686035,
        "y2": 4.731921672821045
      },
      {
        "r": 0,
        "text": "Our interview findings show that the reviewers did internal quality control and found our custom tool well-suited.",
        "trunc_text": "Our interview findings show that the reviewers did internal quality control and found our custom tool well-suited.",
        "x1": 5.143441677093506,
        "x2": 5.596822261810303,
        "y1": 5.669585704803467,
        "y2": 4.719627857208252
      },
      {
        "r": 0,
        "text": "So, only one reviewer is needed, but they shou",
        "trunc_text": "So, only one reviewer is needed, but they shou",
        "x1": 2.674731731414795,
        "x2": 5.601826190948486,
        "y1": 4.22527551651001,
        "y2": 4.743798732757568
      },
      {
        "r": 0,
        "text": " We advocate for the use of IAA in predicting the labeling quality of individual annotators, leading to cost and time efficiency in data production.",
        "trunc_text": " We advocate for the use of IAA in predicting the labeling quality of individual annotators, leading to cost and time ef",
        "x1": 2.0255677700042725,
        "x2": 1.1921950578689575,
        "y1": 3.809687614440918,
        "y2": 4.748747825622559
      },
      {
        "r": 0,
        "text": "Additionally, our work highlights the  IAA's broader application potential in data-driven research optimization and holds significant implications for large-scale data projects prioritizing efficiency, cost reduction, and high-quality data.",
        "trunc_text": "Additionally, our work highlights the  IAA's broader application potential in data-driven research optimization and hold",
        "x1": 5.761192321777344,
        "x2": 6.473188877105713,
        "y1": 6.950461387634277,
        "y2": 2.9184000492095947
      },
      {
        "r": 0,
        "text": "We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while preserving long-range correlation structural information.",
        "trunc_text": "We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while prese",
        "x1": 1.7303757667541504,
        "x2": 3.11594557762146,
        "y1": 6.792911052703857,
        "y2": 1.872400164604187
      },
      {
        "r": 0,
        "text": "Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generative diffusion process.",
        "trunc_text": "Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generat",
        "x1": 1.9756184816360474,
        "x2": 3.195329189300537,
        "y1": 6.9947710037231445,
        "y2": 1.9371771812438965
      },
      {
        "r": 0,
        "text": "The proposed sampling method can be scaled up to any desired image size while only requiring small patches for fast training.",
        "trunc_text": "The proposed sampling method can be scaled up to any desired image size while only requiring small patches for fast trai",
        "x1": 2.0714433193206787,
        "x2": 3.3507070541381836,
        "y1": 7.240478515625,
        "y2": 2.0204849243164062
      },
      {
        "r": 0,
        "text": "Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling artefacts.",
        "trunc_text": "Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling a",
        "x1": 2.031723976135254,
        "x2": 3.4355552196502686,
        "y1": 7.638010025024414,
        "y2": 1.3941431045532227
      },
      {
        "r": 0,
        "text": "The training leverages classifier-free guidance to augment a small, sparsely annotated dataset with unlabelled data.",
        "trunc_text": "The training leverages classifier-free guidance to augment a small, sparsely annotated dataset with unlabelled data.",
        "x1": 1.9661569595336914,
        "x2": 1.6136263608932495,
        "y1": 4.529031753540039,
        "y2": 4.283994197845459
      },
      {
        "r": 0,
        "text": "Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual annotation, and protective data handling.",
        "trunc_text": "Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual an",
        "x1": 1.4112122058868408,
        "x2": 3.1696064472198486,
        "y1": 6.667706489562988,
        "y2": 1.9970810413360596
      },
      {
        "r": 0,
        "text": "The biological plausibility of DiffInfinite data is validated in a survey by ten experienced pathologists as well as a downstream segmentation task.",
        "trunc_text": "The biological plausibility of DiffInfinite data is validated in a survey by ten experienced pathologists as well as a d",
        "x1": 1.0212558507919312,
        "x2": 2.928093194961548,
        "y1": 6.583407402038574,
        "y2": 1.7444965839385986
      },
      {
        "r": 0,
        "text": "Furthermore, the model scores strongly on anti-copying metrics which is beneficial for the protection of patient data.",
        "trunc_text": "Furthermore, the model scores strongly on anti-copying metrics which is beneficial for the protection of patient data.",
        "x1": 3.4695122241973877,
        "x2": 4.280939102172852,
        "y1": 7.281734943389893,
        "y2": 2.3865528106689453
      },
      {
        "r": 0,
        "text": "Understanding this, we, in this paper, first analyze this lack of granular annotations from available pre-annotated datasets to understand the practical inconsistencies and also perform a detailed survey to look into the human perception surrounding annotations.",
        "trunc_text": "Understanding this, we, in this paper, first analyze this lack of granular annotations from available pre-annotated data",
        "x1": 2.29121732711792,
        "x2": 1.3238316774368286,
        "y1": 3.6461057662963867,
        "y2": 5.095404624938965
      },
      {
        "r": 0,
        "text": "Efficient human activity recognition (HAR) using sensor data needs a significant volume of annotated data.",
        "trunc_text": "Efficient human activity recognition (HAR) using sensor data needs a significant volume of annotated data.",
        "x1": 2.9279212951660156,
        "x2": 0.8882049322128296,
        "y1": 9.442270278930664,
        "y2": 5.0523529052734375
      },
      {
        "r": 0,
        "text": "The growing volume of unlabelled sensor data has challenged conventional practices for gathering HAR annotations with human-in-the-loop approaches, often leading to the collection of shallower annotations.",
        "trunc_text": "The growing volume of unlabelled sensor data has challenged conventional practices for gathering HAR annotations with hu",
        "x1": 2.0721683502197266,
        "x2": 1.0695431232452393,
        "y1": 3.718971014022827,
        "y2": 4.942996501922607
      },
      {
        "r": 0,
        "text": "These shallower annotations ignore the fine-grained micro-activities that constitute any complex activities of daily living (ADL).  ",
        "trunc_text": "These shallower annotations ignore the fine-grained micro-activities that constitute any complex activities of daily liv",
        "x1": 2.0836639404296875,
        "x2": 0.8980215787887573,
        "y1": 3.2603325843811035,
        "y2": 5.218700408935547
      },
      {
        "r": 0,
        "text": "Drawing motivations from these, we next develop the framework AmicroN that can automatically generate micro-activity annotations using locomotive signatures and the available coarse-grain macro-activity labels.",
        "trunc_text": "Drawing motivations from these, we next develop the framework AmicroN that can automatically generate micro-activity ann",
        "x1": 2.0612173080444336,
        "x2": 0.8241374492645264,
        "y1": 3.222991466522217,
        "y2": 5.242935657501221
      },
      {
        "r": 0,
        "text": "In the backend, AmicroN applies change-point detection followed by zero-shot learning with activity embeddings to identify the unseen micro-activities in an unsupervised manner.",
        "trunc_text": "In the backend, AmicroN applies change-point detection followed by zero-shot learning with activity embeddings to identi",
        "x1": 2.049314022064209,
        "x2": 0.8400124907493591,
        "y1": 3.2244949340820312,
        "y2": 5.203048229217529
      },
      {
        "r": 0,
        "text": "Rigorous evaluation on publicly available datasets shows that AmicroN can accurately generate micro-activity annotations with a median F1-score of >0.75.",
        "trunc_text": "Rigorous evaluation on publicly available datasets shows that AmicroN can accurately generate micro-activity annotations",
        "x1": 2.0519657135009766,
        "x2": 0.8395802974700928,
        "y1": 3.243314027786255,
        "y2": 5.223570823669434
      },
      {
        "r": 0,
        "text": "Additionally, we also show that AmicroN can be used in a plug-and-play manner with Large Language Models (LLMs) to obtain the micro-activity labels, thus making it more practical for realistic applications.",
        "trunc_text": "Additionally, we also show that AmicroN can be used in a plug-and-play manner with Large Language Models (LLMs) to obtai",
        "x1": 2.1056599617004395,
        "x2": 0.8469122052192688,
        "y1": 3.190493106842041,
        "y2": 5.289583683013916
      },
      {
        "r": 0,
        "text": "This paper presents a large publicly available multi-center lumbar spine magnetic resonance imaging (MRI) dataset with reference segmentations of vertebrae, intervertebral discs (IVDs), and spinal canal.",
        "trunc_text": "This paper presents a large publicly available multi-center lumbar spine magnetic resonance imaging (MRI) dataset with r",
        "x1": 1.074925184249878,
        "x2": 3.0247836112976074,
        "y1": 6.574471473693848,
        "y2": 1.7807329893112183
      },
      {
        "r": 0,
        "text": "The dataset includes 447 sagittal T1 and T2 MRI series from 218 patients with a history of low back pain.",
        "trunc_text": "The dataset includes 447 sagittal T1 and T2 MRI series from 218 patients with a history of low back pain.",
        "x1": 1.1586039066314697,
        "x2": 3.4387757778167725,
        "y1": 6.675510883331299,
        "y2": 1.8686344623565674
      },
      {
        "r": 0,
        "text": "It was collected from four different hospitals and was divided into a training (179 patients) and validation (39 patients) set.",
        "trunc_text": "It was collected from four different hospitals and was divided into a training (179 patients) and validation (39 patient",
        "x1": 3.0860719680786133,
        "x2": 3.9507906436920166,
        "y1": 7.210643768310547,
        "y2": 2.0487217903137207
      },
      {
        "r": 0,
        "text": "An iterative data annotation approach was used by training a segmentation algorithm on a small part of the dataset, enabling semi-automatic segmentation of the remaining images.",
        "trunc_text": "An iterative data annotation approach was used by training a segmentation algorithm on a small part of the dataset, enab",
        "x1": 0.7924694418907166,
        "x2": 2.4375925064086914,
        "y1": 6.936097621917725,
        "y2": 1.582103967666626
      },
      {
        "r": 0,
        "text": "The algorithm provided an initial segmentation, which was subsequently reviewed, manually corrected, and added to the training data.",
        "trunc_text": "The algorithm provided an initial segmentation, which was subsequently reviewed, manually corrected, and added to the tr",
        "x1": 0.7995842099189758,
        "x2": 2.468876600265503,
        "y1": 6.866183280944824,
        "y2": 1.6221100091934204
      },
      {
        "r": 0,
        "text": "We provide reference performance values for this baseline algorithm and nnU-Net, which performed comparably.",
        "trunc_text": "We provide reference performance values for this baseline algorithm and nnU-Net, which performed comparably.",
        "x1": 3.8194544315338135,
        "x2": 4.76920223236084,
        "y1": 6.544800281524658,
        "y2": 4.092127799987793
      },
      {
        "r": 0,
        "text": "We set up a continuous segmentation challenge to allow for a fair comparison of different segmentation algorithms.",
        "trunc_text": "We set up a continuous segmentation challenge to allow for a fair comparison of different segmentation algorithms.",
        "x1": 0.8114533424377441,
        "x2": 2.4967048168182373,
        "y1": 7.0634765625,
        "y2": 1.4310582876205444
      },
      {
        "r": 0,
        "text": "This study may encourage wider collaboration in the field of spine segmentation, and improve the diagnostic value of lumbar spine MRI.",
        "trunc_text": "This study may encourage wider collaboration in the field of spine segmentation, and improve the diagnostic value of lum",
        "x1": 1.0494987964630127,
        "x2": 2.911386728286743,
        "y1": 6.608325004577637,
        "y2": 1.7724062204360962
      },
      {
        "r": 0,
        "text": "But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels",
        "trunc_text": "But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more ",
        "x1": 1.0492744445800781,
        "x2": 0.8363610506057739,
        "y1": 4.8740973472595215,
        "y2": 3.6124253273010254
      },
      {
        "r": 0,
        "text": "Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings",
        "trunc_text": "Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings",
        "x1": 0.9930640459060669,
        "x2": 0.8795703053474426,
        "y1": 4.8523454666137695,
        "y2": 3.5077595710754395
      },
      {
        "r": 0,
        "text": "Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future.",
        "trunc_text": "Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging",
        "x1": 1.2009639739990234,
        "x2": 0.9749548435211182,
        "y1": 4.843966960906982,
        "y2": 3.5537216663360596
      },
      {
        "r": 0,
        "text": "We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federated noisy label learning.",
        "trunc_text": "We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeli",
        "x1": 1.1681801080703735,
        "x2": 1.008623480796814,
        "y1": 4.795889854431152,
        "y2": 3.499087333679199
      },
      {
        "r": 0,
        "text": "Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. .",
        "trunc_text": "Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. .",
        "x1": 0.39511969685554504,
        "x2": 7.327653884887695,
        "y1": 4.913420677185059,
        "y2": 4.8652520179748535
      },
      {
        "r": 0,
        "text": "Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings.",
        "trunc_text": "Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings.",
        "x1": 0.9894697666168213,
        "x2": 0.9006460309028625,
        "y1": 4.854424476623535,
        "y2": 3.5123860836029053
      },
      {
        "r": 0,
        "text": "However, there is a lack of a benchis work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings.",
        "trunc_text": "However, there is a lack of a benchis work, we serve the first standardized benchmark that can help researchers fully ex",
        "x1": 0.9697820544242859,
        "x2": 0.9951025247573853,
        "y1": 4.89683723449707,
        "y2": 3.4294166564941406
      },
      {
        "r": 0,
        "text": "We highlight the 20 basic settings f \\texttt{FedNoisy} is available at \\codeword{https://github.com/SMILELab-FL/FedNoisy}.",
        "trunc_text": "We highlight the 20 basic settings f \\texttt{FedNoisy} is available at \\codeword{https://github.com/SMILELab-FL/FedNoisy",
        "x1": 8.292299270629883,
        "x2": 8.848490715026855,
        "y1": 7.556044578552246,
        "y2": 2.4278879165649414
      },
      {
        "r": 0,
        "text": "In this paper, we explore different ways of training a model for handwritten text recognition when multiple imperfect or noisy transcriptions are available",
        "trunc_text": "In this paper, we explore different ways of training a model for handwritten text recognition when multiple imperfect or",
        "x1": 4.020484924316406,
        "x2": 1.6435068845748901,
        "y1": 1.8510955572128296,
        "y2": 6.985163688659668
      },
      {
        "r": 0,
        "text": "We consider various training configurations, such as selecting a single transcription, retaining all transcriptions, or computing an aggregated transcription from all available annotations.",
        "trunc_text": "We consider various training configurations, such as selecting a single transcription, retaining all transcriptions, or ",
        "x1": 2.4280965328216553,
        "x2": 1.915838599205017,
        "y1": 4.522623062133789,
        "y2": 4.507287979125977
      },
      {
        "r": 0,
        "text": "In addition, we evaluate the impact of quality-based data selection, where samples with low agreement are removed from the training set.",
        "trunc_text": "In addition, we evaluate the impact of quality-based data selection, where samples with low agreement are removed from t",
        "x1": 3.629497766494751,
        "x2": 4.069867134094238,
        "y1": 5.739863872528076,
        "y2": 3.963717222213745
      },
      {
        "r": 0,
        "text": "Our experiments are carried out on municipal registers of the city of Belfort (France) written between 1790 and 1946.",
        "trunc_text": "Our experiments are carried out on municipal registers of the city of Belfort (France) written between 1790 and 1946.",
        "x1": 7.894466400146484,
        "x2": 8.35954475402832,
        "y1": 7.197316646575928,
        "y2": 2.838134288787842
      },
      {
        "r": 0,
        "text": "% results The results show that computing a consensus transcription or training on multiple transcriptions are good alternatives.",
        "trunc_text": "% results The results show that computing a consensus transcription or training on multiple transcriptions are good alte",
        "x1": 2.485356569290161,
        "x2": 1.9163663387298584,
        "y1": 4.4936604499816895,
        "y2": 4.522714614868164
      },
      {
        "r": 0,
        "text": "However, selecting training samples based on the degree of agreement between annotators introduces a bias in the training data and does not improve the res",
        "trunc_text": "However, selecting training samples based on the degree of agreement between annotators introduces a bias in the trainin",
        "x1": 2.2968387603759766,
        "x2": 1.442796230316162,
        "y1": 3.9495797157287598,
        "y2": 4.86030387878418
      },
      {
        "r": 0,
        "text": "The aim of the experiment is to judge the final annotation quality when pre-annotation is used.",
        "trunc_text": "The aim of the experiment is to judge the final annotation quality when pre-annotation is used.",
        "x1": 2.2387804985046387,
        "x2": 1.2571207284927368,
        "y1": 3.592439651489258,
        "y2": 5.157897472381592
      },
      {
        "r": 0,
        "text": "In addition, it evaluates the effect of automatic linguistically-based (rule-formulated) checks and another annotation on the same data available to the annotators, and their influence on annotation quality and efficiency.",
        "trunc_text": "In addition, it evaluates the effect of automatic linguistically-based (rule-formulated) checks and another annotation o",
        "x1": 2.402632713317871,
        "x2": 1.2878470420837402,
        "y1": 3.471045732498169,
        "y2": 5.168791770935059
      },
      {
        "r": 0,
        "text": "This paper presents an analysis of annotation using an automatic pre-annotation for a mid-level annotation complexity task -- dependency syntax annotation.",
        "trunc_text": "This paper presents an analysis of annotation using an automatic pre-annotation for a mid-level annotation complexity ta",
        "x1": 2.4930179119110107,
        "x2": 1.373512864112854,
        "y1": 3.4440317153930664,
        "y2": 5.297154426574707
      },
      {
        "r": 0,
        "text": "It compares the annotation efforts made by annotators using a pre-annotated version (with a high-accuracy parser) and those made by fully manual annotation.  ",
        "trunc_text": "It compares the annotation efforts made by annotators using a pre-annotated version (with a high-accuracy parser) and th",
        "x1": 2.3042078018188477,
        "x2": 1.2479920387268066,
        "y1": 3.5094590187072754,
        "y2": 5.249031066894531
      },
      {
        "r": 0,
        "text": "In addition, it evaluates the effect of automatic linguistically-based (rule-formulated) checkstic annotation which increases the consistency of the resulting annotation without reducing its quality.",
        "trunc_text": "In addition, it evaluates the effect of automatic linguistically-based (rule-formulated) checkstic annotation which incr",
        "x1": 2.3310065269470215,
        "x2": 1.348453164100647,
        "y1": 3.5771074295043945,
        "y2": 5.252386569976807
      },
      {
        "r": 0,
        "text": "Our code and dataset is available here.",
        "trunc_text": "Our code and dataset is available here.",
        "x1": 7.248522758483887,
        "x2": 7.717958927154541,
        "y1": 7.752810478210449,
        "y2": 2.111746072769165
      },
      {
        "r": 0,
        "text": "We will release the dataset and code to facilitate future endeavors.",
        "trunc_text": "We will release the dataset and code to facilitate future endeavors.",
        "x1": 6.652623176574707,
        "x2": 7.190438747406006,
        "y1": 7.395591735839844,
        "y2": 2.4476823806762695
      },
      {
        "r": 0,
        "text": "We release our dataset for others to use and build on.",
        "trunc_text": "We release our dataset for others to use and build on.",
        "x1": 6.191713333129883,
        "x2": 6.794234275817871,
        "y1": 7.456149578094482,
        "y2": 2.4213149547576904
      },
      {
        "r": 0,
        "text": "Our dataset is available online.",
        "trunc_text": "Our dataset is available online.",
        "x1": 6.562610149383545,
        "x2": 7.074800491333008,
        "y1": 7.593749523162842,
        "y2": 2.213141679763794
      },
      {
        "r": 0,
        "text": "We release the generated dataset and used prompts to facilitate future research.",
        "trunc_text": "We release the generated dataset and used prompts to facilitate future research.",
        "x1": 5.712812900543213,
        "x2": 6.382719993591309,
        "y1": 7.3665900230407715,
        "y2": 2.4289214611053467
      },
      {
        "r": 0,
        "text": "Code and dataset will be available.",
        "trunc_text": "Code and dataset will be available.",
        "x1": 7.138716220855713,
        "x2": 7.5959320068359375,
        "y1": 7.6150898933410645,
        "y2": 2.1602349281311035
      },
      {
        "r": 0,
        "text": "We demonstrate the value of the created dataset by performing a quantitative and qualitative analysis on the models' results.",
        "trunc_text": "We demonstrate the value of the created dataset by performing a quantitative and qualitative analysis on the models' res",
        "x1": 5.209513187408447,
        "x2": 5.8614349365234375,
        "y1": 7.305776119232178,
        "y2": 2.5773894786834717
      },
      {
        "r": 0,
        "text": "We train our model on a new synthetic image dataset, that we release.",
        "trunc_text": "We train our model on a new synthetic image dataset, that we release.",
        "x1": 2.04424786567688,
        "x2": 3.5659782886505127,
        "y1": 7.398375511169434,
        "y2": 1.7263720035552979
      },
      {
        "r": 0,
        "text": "The code and new synthetic dataset will be released for better reproducibility of our results.",
        "trunc_text": "The code and new synthetic dataset will be released for better reproducibility of our results.",
        "x1": 5.5237226486206055,
        "x2": 5.448734760284424,
        "y1": 7.131294250488281,
        "y2": 2.2631871700286865
      },
      {
        "r": 0,
        "text": "From this point, we can note the importance of building a new structured dataset to solve the lack of structured data.",
        "trunc_text": "From this point, we can note the importance of building a new structured dataset to solve the lack of structured data.",
        "x1": 5.839109897613525,
        "x2": 6.240015506744385,
        "y1": 6.803652286529541,
        "y2": 2.9813687801361084
      },
      {
        "r": 0,
        "text": "Previous datasets are created by either capturing real scenes by event cameras or synthesizing from images with pasted foreground objects.",
        "trunc_text": "Previous datasets are created by either capturing real scenes by event cameras or synthesizing from images with pasted f",
        "x1": 5.631847381591797,
        "x2": 6.216346740722656,
        "y1": 7.539546489715576,
        "y2": 1.996832013130188
      },
      {
        "r": 0,
        "text": "These datasets included the latest second and third generation deepfake datasets.",
        "trunc_text": "These datasets included the latest second and third generation deepfake datasets.",
        "x1": 6.746501922607422,
        "x2": 7.052856922149658,
        "y1": 7.694108009338379,
        "y2": 1.9461811780929565
      },
      {
        "r": 0,
        "text": "To our knowledge, this is the first aligned dataset of its kind and is the largest dataset ever released in the heritage domain.",
        "trunc_text": "To our knowledge, this is the first aligned dataset of its kind and is the largest dataset ever released in the heritage",
        "x1": 5.291744232177734,
        "x2": 5.980810642242432,
        "y1": 7.521916389465332,
        "y2": 2.3937461376190186
      },
      {
        "r": 0,
        "text": "The code and dataset will be released publicly.",
        "trunc_text": "The code and dataset will be released publicly.",
        "x1": 7.184911251068115,
        "x2": 7.671536445617676,
        "y1": 7.380792617797852,
        "y2": 2.43929123878479
      },
      {
        "r": 0,
        "text": "We have released the code and dataset used in the present approach to generate synthetic data.",
        "trunc_text": "We have released the code and dataset used in the present approach to generate synthetic data.",
        "x1": 5.316816329956055,
        "x2": 5.325564861297607,
        "y1": 6.929906368255615,
        "y2": 2.284996747970581
      },
      {
        "r": 0,
        "text": "Our dataset is publicly available and can be freely modified and re-distributed.",
        "trunc_text": "Our dataset is publicly available and can be freely modified and re-distributed.",
        "x1": 7.064828395843506,
        "x2": 7.2254462242126465,
        "y1": 7.213279724121094,
        "y2": 2.400441884994507
      },
      {
        "r": 0,
        "text": "The community has recognized the critical role that training datasets play in this context and has developed various techniques to improve dataset curation to overcome this problem.",
        "trunc_text": "The community has recognized the critical role that training datasets play in this context and has developed various tec",
        "x1": 5.162109375,
        "x2": 5.7136759757995605,
        "y1": 6.872035980224609,
        "y2": 2.993436098098755
      },
      {
        "r": 0,
        "text": "Our code and dataset will be released at https://github.com/SiyuanYan1/EPVT.",
        "trunc_text": "Our code and dataset will be released at https://github.com/SiyuanYan1/EPVT.",
        "x1": 7.409151554107666,
        "x2": 7.999979019165039,
        "y1": 7.8307576179504395,
        "y2": 2.0842137336730957
      },
      {
        "r": 0,
        "text": "The 3RL dataset was created, which contains approximately 24K images and will be publicly available, to overcome previously available dataset problems.",
        "trunc_text": "The 3RL dataset was created, which contains approximately 24K images and will be publicly available, to overcome previou",
        "x1": 6.8258056640625,
        "x2": 7.20072078704834,
        "y1": 7.514794826507568,
        "y2": 2.003387689590454
      },
      {
        "r": 0,
        "text": "We use a public dataset for model development.",
        "trunc_text": "We use a public dataset for model development.",
        "x1": 7.626285076141357,
        "x2": 7.891787052154541,
        "y1": 6.922661304473877,
        "y2": 2.8672595024108887
      },
      {
        "r": 0,
        "text": "The related datasets and the source code will be released in the future.",
        "trunc_text": "The related datasets and the source code will be released in the future.",
        "x1": 6.939120769500732,
        "x2": 7.547840595245361,
        "y1": 7.6234307289123535,
        "y2": 2.28432297706604
      },
      {
        "r": 0,
        "text": "We also collect a new large-scale dataset to serve as the new benchmark for this task.",
        "trunc_text": "We also collect a new large-scale dataset to serve as the new benchmark for this task.",
        "x1": 4.444761753082275,
        "x2": 4.292701721191406,
        "y1": 6.656078338623047,
        "y2": 3.1074259281158447
      },
      {
        "r": 0,
        "text": "The complete dataset engineered for this study, referred to as the CIFAKE dataset, is made publicly available to the research community for future work.",
        "trunc_text": "The complete dataset engineered for this study, referred to as the CIFAKE dataset, is made publicly available to the res",
        "x1": 5.898759365081787,
        "x2": 6.377476215362549,
        "y1": 7.523773193359375,
        "y2": 2.2683913707733154
      },
      {
        "r": 0,
        "text": "The dataset with accompanying code can be downloaded from our website.",
        "trunc_text": "The dataset with accompanying code can be downloaded from our website.",
        "x1": 7.268698215484619,
        "x2": 7.657327651977539,
        "y1": 7.595489501953125,
        "y2": 2.2244608402252197
      },
      {
        "r": 0,
        "text": "We propose new training, validation, and testing splits for the dataset that we make available online.",
        "trunc_text": "We propose new training, validation, and testing splits for the dataset that we make available online.",
        "x1": 4.747744083404541,
        "x2": 5.458714962005615,
        "y1": 6.667167663574219,
        "y2": 3.1487817764282227
      },
      {
        "r": 0,
        "text": "Our code and unique datasets are available on the project's website.",
        "trunc_text": "Our code and unique datasets are available on the project's website.",
        "x1": 7.357852935791016,
        "x2": 7.7822747230529785,
        "y1": 7.6639204025268555,
        "y2": 2.243673086166382
      },
      {
        "r": 0,
        "text": "We make our data available.",
        "trunc_text": "We make our data available.",
        "x1": 6.214656352996826,
        "x2": 7.017545700073242,
        "y1": 7.6543989181518555,
        "y2": 2.6544456481933594
      },
      {
        "r": 0,
        "text": "To facilitate research in this field, we will share our dataset and code with the community.",
        "trunc_text": "To facilitate research in this field, we will share our dataset and code with the community.",
        "x1": 6.661214828491211,
        "x2": 7.301543235778809,
        "y1": 7.341615676879883,
        "y2": 2.442866802215576
      },
      {
        "r": 0,
        "text": "The dataset, related codes and models will be publicly available at https://github.com/hitachinsk/THA.",
        "trunc_text": "The dataset, related codes and models will be publicly available at https://github.com/hitachinsk/THA.",
        "x1": 7.310956954956055,
        "x2": 7.690662860870361,
        "y1": 7.772546291351318,
        "y2": 2.114365577697754
      },
      {
        "r": 0,
        "text": "We have developed a systematic method to synthesize such training datasets.",
        "trunc_text": "We have developed a systematic method to synthesize such training datasets.",
        "x1": 5.153321266174316,
        "x2": 5.556909084320068,
        "y1": 6.911520004272461,
        "y2": 2.5554652214050293
      },
      {
        "r": 0,
        "text": "In addition to the dataset itself, we also present some basic analysis of its content, certain temporal features, and its network.",
        "trunc_text": "In addition to the dataset itself, we also present some basic analysis of its content, certain temporal features, and it",
        "x1": 5.034774303436279,
        "x2": 5.9493727684021,
        "y1": 7.568187713623047,
        "y2": 2.2418911457061768
      },
      {
        "r": 0,
        "text": "In fact, to date, there is no dataset that we are aware of that addresses this issue.",
        "trunc_text": "In fact, to date, there is no dataset that we are aware of that addresses this issue.",
        "x1": 5.695139408111572,
        "x2": 6.871535778045654,
        "y1": 7.5876784324646,
        "y2": 2.3144965171813965
      },
      {
        "r": 0,
        "text": "We are committed to open-sourcing our meticulously curated dataset, as well as a comprehensive toolkit designed to aid researchers in the extensive collection and preprocessing of their own datasets.",
        "trunc_text": "We are committed to open-sourcing our meticulously curated dataset, as well as a comprehensive toolkit designed to aid r",
        "x1": 5.686274528503418,
        "x2": 6.395606994628906,
        "y1": 7.182799339294434,
        "y2": 2.614708662033081
      },
      {
        "r": 0,
        "text": "Third, we provide a dataset of scenario based on our data generated.",
        "trunc_text": "Third, we provide a dataset of scenario based on our data generated.",
        "x1": 5.544469356536865,
        "x2": 6.194564342498779,
        "y1": 7.368808746337891,
        "y2": 2.3524723052978516
      },
      {
        "r": 0,
        "text": "Our source code and dataset will be made publicly available.",
        "trunc_text": "Our source code and dataset will be made publicly available.",
        "x1": 7.576239585876465,
        "x2": 8.03678035736084,
        "y1": 7.197875022888184,
        "y2": 2.6510260105133057
      },
      {
        "r": 0,
        "text": "To this end, we first collect a new dataset, CAMO-FS, for the benchmark.",
        "trunc_text": "To this end, we first collect a new dataset, CAMO-FS, for the benchmark.",
        "x1": 4.4301886558532715,
        "x2": 4.220235824584961,
        "y1": 6.726036071777344,
        "y2": 3.1732418537139893
      },
      {
        "r": 0,
        "text": "In this paper, we propose a framework for enhancing the data quality of original datasets.",
        "trunc_text": "In this paper, we propose a framework for enhancing the data quality of original datasets.",
        "x1": 5.163556098937988,
        "x2": 5.590089321136475,
        "y1": 6.143953323364258,
        "y2": 3.577061891555786
      },
      {
        "r": 0,
        "text": "We release a demo of our tools at dataportraits.org and call on dataset and model creators to release Data Portraits as a complement to current documentation practices.",
        "trunc_text": "We release a demo of our tools at dataportraits.org and call on dataset and model creators to release Data Portraits as ",
        "x1": 6.2023515701293945,
        "x2": 6.743778228759766,
        "y1": 7.308393478393555,
        "y2": 2.4648330211639404
      },
      {
        "r": 0,
        "text": "The source code and dataset will be public.",
        "trunc_text": "The source code and dataset will be public.",
        "x1": 7.495930194854736,
        "x2": 7.908903121948242,
        "y1": 7.269903182983398,
        "y2": 2.5372984409332275
      },
      {
        "r": 0,
        "text": "To the best of our knowledge, only two datasets are available, with one based on the other.",
        "trunc_text": "To the best of our knowledge, only two datasets are available, with one based on the other.",
        "x1": 6.606968402862549,
        "x2": 7.192856311798096,
        "y1": 7.588231563568115,
        "y2": 2.143476724624634
      },
      {
        "r": 0,
        "text": "We validate our method on two widely used datasets.",
        "trunc_text": "We validate our method on two widely used datasets.",
        "x1": 4.503182888031006,
        "x2": 5.204429626464844,
        "y1": 6.80839729309082,
        "y2": 3.200718879699707
      },
      {
        "r": 0,
        "text": "We introduce the VISION Datasets, a diverse collection of 14 industrial inspection datasets, uniquely poised to meet these challenges.",
        "trunc_text": "We introduce the VISION Datasets, a diverse collection of 14 industrial inspection datasets, uniquely poised to meet the",
        "x1": 1.5389055013656616,
        "x2": 3.6191694736480713,
        "y1": 6.901383399963379,
        "y2": 1.8314462900161743
      },
      {
        "r": 0,
        "text": "The dataset is available at https://www.tu-ilmenau.de/neurob/data-sets-code/attach-dataset .",
        "trunc_text": "The dataset is available at https://www.tu-ilmenau.de/neurob/data-sets-code/attach-dataset .",
        "x1": 7.092175483703613,
        "x2": 7.542024612426758,
        "y1": 8.035758972167969,
        "y2": 1.8823139667510986
      },
      {
        "r": 0,
        "text": "The dataset and code are available at \\url{https://github.com/littleYaang/HQ-50K}.",
        "trunc_text": "The dataset and code are available at \\url{https://github.com/littleYaang/HQ-50K}.",
        "x1": 7.468723297119141,
        "x2": 7.863394737243652,
        "y1": 7.740330696105957,
        "y2": 2.034876823425293
      },
      {
        "r": 0,
        "text": "The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available at our Github repo here: https://github.com/som-shahlab/ehrshot-benchmark",
        "trunc_text": "The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available a",
        "x1": 4.413788795471191,
        "x2": 4.312977313995361,
        "y1": 6.800251007080078,
        "y2": 2.9699463844299316
      },
      {
        "r": 0,
        "text": "Our SCB-dataset can be downloaded from: https://github.com/Whiffe/SCB-dataset",
        "trunc_text": "Our SCB-dataset can be downloaded from: https://github.com/Whiffe/SCB-dataset",
        "x1": 7.260349273681641,
        "x2": 7.653268814086914,
        "y1": 7.828682899475098,
        "y2": 1.935380220413208
      },
      {
        "r": 0,
        "text": "The dataset is available for download at: https://ustc-flicar.github.io.",
        "trunc_text": "The dataset is available for download at: https://ustc-flicar.github.io.",
        "x1": 7.262663841247559,
        "x2": 7.635961055755615,
        "y1": 7.954669952392578,
        "y2": 1.8542424440383911
      },
      {
        "r": 0,
        "text": "To access our dataset and code, visit our GitHub repository: \\url{https://github.com/styxsys0927/Med-MMHL}.",
        "trunc_text": "To access our dataset and code, visit our GitHub repository: \\url{https://github.com/styxsys0927/Med-MMHL}.",
        "x1": 7.5719499588012695,
        "x2": 7.927404880523682,
        "y1": 7.767961502075195,
        "y2": 2.056122064590454
      },
      {
        "r": 0,
        "text": "Code can be downloaded from https://github.com/Zhang-VISLab.",
        "trunc_text": "Code can be downloaded from https://github.com/Zhang-VISLab.",
        "x1": 8.113791465759277,
        "x2": 8.710381507873535,
        "y1": 7.690563678741455,
        "y2": 2.226534605026245
      },
      {
        "r": 0,
        "text": "The data products and codes can be downloaded from this https://github.com/sriniraghunathan/cross_ilc_methods_paper.",
        "trunc_text": "The data products and codes can be downloaded from this https://github.com/sriniraghunathan/cross_ilc_methods_paper.",
        "x1": 7.58644437789917,
        "x2": 7.944178581237793,
        "y1": 7.649195194244385,
        "y2": 2.1224284172058105
      },
      {
        "r": 0,
        "text": "To download the data please visit https://stanford-tml.github.io/circle_dataset/.",
        "trunc_text": "To download the data please visit https://stanford-tml.github.io/circle_dataset/.",
        "x1": 7.445541858673096,
        "x2": 7.734261989593506,
        "y1": 7.789803504943848,
        "y2": 1.9440923929214478
      },
      {
        "r": 0,
        "text": "Our code is available at Github.",
        "trunc_text": "Our code is available at Github.",
        "x1": 8.252654075622559,
        "x2": 8.785863876342773,
        "y1": 7.602994441986084,
        "y2": 2.2823309898376465
      },
      {
        "r": 0,
        "text": "All code is available on GitHub.",
        "trunc_text": "All code is available on GitHub.",
        "x1": 8.261122703552246,
        "x2": 8.78137493133545,
        "y1": 7.557599067687988,
        "y2": 2.290741443634033
      },
      {
        "r": 0,
        "text": "With these new techniques, our proposed \\Ours{} achieves state-of-the-art results on FUNSD and XFUND datasets, outperforming the previous best-performing method by 7.2\\% and 13.2\\% in F1 score, respectively.",
        "trunc_text": "With these new techniques, our proposed \\Ours{} achieves state-of-the-art results on FUNSD and XFUND datasets, outperfor",
        "x1": 4.174971103668213,
        "x2": 5.018627166748047,
        "y1": 6.255496978759766,
        "y2": 3.6001086235046387
      },
      {
        "r": 0,
        "text": "We make a python package with the code available to download at https://pypi.org/project/hypertab/",
        "trunc_text": "We make a python package with the code available to download at https://pypi.org/project/hypertab/",
        "x1": 8.036839485168457,
        "x2": 8.649458885192871,
        "y1": 7.745004653930664,
        "y2": 2.1624534130096436
      },
      {
        "r": 0,
        "text": "The air pollution data was downloaded from an online database (UCL).",
        "trunc_text": "The air pollution data was downloaded from an online database (UCL).",
        "x1": 7.238724708557129,
        "x2": 7.673198699951172,
        "y1": 7.770692825317383,
        "y2": 2.0364229679107666
      },
      {
        "r": 0,
        "text": "The SA3 dataset and scripts (R/Python) to develop these indices have been made available on my GitHub account: https://github.com/lpinzari/homogeneity-location-index",
        "trunc_text": "The SA3 dataset and scripts (R/Python) to develop these indices have been made available on my GitHub account: https://g",
        "x1": 7.15261173248291,
        "x2": 7.627964496612549,
        "y1": 8.03598403930664,
        "y2": 1.763292670249939
      },
      {
        "r": 0,
        "text": "We make the code available at github.",
        "trunc_text": "We make the code available at github.",
        "x1": 8.228816986083984,
        "x2": 8.783647537231445,
        "y1": 7.558548927307129,
        "y2": 2.3081109523773193
      },
      {
        "r": 0,
        "text": "All the source code is available on Github.",
        "trunc_text": "All the source code is available on Github.",
        "x1": 8.251019477844238,
        "x2": 8.811328887939453,
        "y1": 7.471670627593994,
        "y2": 2.438777446746826
      },
      {
        "r": 0,
        "text": "The code has been deposited on GitHub (\\url{https://github.com/hyguozz}).",
        "trunc_text": "The code has been deposited on GitHub (\\url{https://github.com/hyguozz}).",
        "x1": 8.071996688842773,
        "x2": 8.681452751159668,
        "y1": 7.740694046020508,
        "y2": 2.1950438022613525
      },
      {
        "r": 0,
        "text": "The code is on github at https://github.com/IDU-CVLab/COV19D_3rd",
        "trunc_text": "The code is on github at https://github.com/IDU-CVLab/COV19D_3rd",
        "x1": 8.079480171203613,
        "x2": 8.669203758239746,
        "y1": 7.713599681854248,
        "y2": 2.227114677429199
      },
      {
        "r": 0,
        "text": "Its features, e.g., no need to download and no installation, have made it popular rapidly.",
        "trunc_text": "Its features, e.g., no need to download and no installation, have made it popular rapidly.",
        "x1": 6.2121734619140625,
        "x2": 6.627661228179932,
        "y1": 6.5156097412109375,
        "y2": 3.5194427967071533
      },
      {
        "r": 0,
        "text": "We outperform all baselines and demonstrate that among the considered methods, ours is the only one that detects label errors of all four types efficiently.",
        "trunc_text": "We outperform all baselines and demonstrate that among the considered methods, ours is the only one that detects label e",
        "x1": 1.7378802299499512,
        "x2": 1.1458388566970825,
        "y1": 4.399770736694336,
        "y2": 3.973139762878418
      },
      {
        "r": 0,
        "text": "By leveraging this diversity, the collected dataset and the collection system aim to achieve higher recognition accuracy.",
        "trunc_text": "By leveraging this diversity, the collected dataset and the collection system aim to achieve higher recognition accuracy",
        "x1": 4.014874458312988,
        "x2": 5.053195476531982,
        "y1": 7.504930019378662,
        "y2": 2.638019323348999
      },
      {
        "r": 0,
        "text": "The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity.",
        "trunc_text": "The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity.",
        "x1": 6.71209716796875,
        "x2": 7.301760196685791,
        "y1": 7.494518756866455,
        "y2": 2.1452198028564453
      },
      {
        "r": 0,
        "text": "Our proposed dataset can support two different computer vision tasks, i.e., semantic segmentation and object detection.",
        "trunc_text": "Our proposed dataset can support two different computer vision tasks, i.e., semantic segmentation and object detection.",
        "x1": 0.8838397860527039,
        "x2": 2.446256637573242,
        "y1": 7.382996082305908,
        "y2": 1.4354287385940552
      },
      {
        "r": 0,
        "text": "To foster further research on the digitization of statistical graphs, we will make the dataset, code, and models publicly available to the community.",
        "trunc_text": "To foster further research on the digitization of statistical graphs, we will make the dataset, code, and models publicl",
        "x1": 7.023159980773926,
        "x2": 7.458836555480957,
        "y1": 7.147737979888916,
        "y2": 2.657597780227661
      },
      {
        "r": 0,
        "text": "This collection includes a subset of the large-scale instruction dataset known as FLAN, as well as various code-related datasets and conversational datasets derived from ChatGPT/GPT-4.",
        "trunc_text": "This collection includes a subset of the large-scale instruction dataset known as FLAN, as well as various code-related ",
        "x1": 6.465948104858398,
        "x2": 4.71940803527832,
        "y1": 3.8554468154907227,
        "y2": 6.740691184997559
      },
      {
        "r": 0,
        "text": "FLACUNA is publicly available at https://huggingface.co/declare-lab/flacuna-13b-v1.0.",
        "trunc_text": "FLACUNA is publicly available at https://huggingface.co/declare-lab/flacuna-13b-v1.0.",
        "x1": 7.330475807189941,
        "x2": 7.853784084320068,
        "y1": 7.8949456214904785,
        "y2": 1.817702054977417
      },
      {
        "r": 0,
        "text": "First, we publish a new dataset, EHRSHOT, containing de-identified structured data from the electronic health records (EHRs) of 6,712 patients from Stanford Medicine.",
        "trunc_text": "First, we publish a new dataset, EHRSHOT, containing de-identified structured data from the electronic health records (E",
        "x1": 3.419215202331543,
        "x2": 4.310431957244873,
        "y1": 7.328114986419678,
        "y2": 2.2853822708129883
      },
      {
        "r": 0,
        "text": "In this paper, we define a unified setting termed as open-set semantic segmentation (O3S), which aims to learn seen and unseen semantics from both visual examples and textual names.",
        "trunc_text": "In this paper, we define a unified setting termed as open-set semantic segmentation (O3S), which aims to learn seen and ",
        "x1": 0.8914960026741028,
        "x2": 2.4884607791900635,
        "y1": 7.354953289031982,
        "y2": 1.2972683906555176
      },
      {
        "r": 0,
        "text": "Experimental results, carried out on three sets of the Shape COSEG Dataset, on the human segmentation dataset proposed in Maron et al., 2017 and on the ShapeNet benchmark, show how the proposed approach yields state-of-the-art performance on semantic segmentation of 3D meshes.",
        "trunc_text": "Experimental results, carried out on three sets of the Shape COSEG Dataset, on the human segmentation dataset proposed i",
        "x1": 0.8912753462791443,
        "x2": 2.5621395111083984,
        "y1": 7.196195602416992,
        "y2": 1.3173671960830688
      },
      {
        "r": 0,
        "text": "Our code and data are available at https://github.com/sergiotasconmorales/locvqa.",
        "trunc_text": "Our code and data are available at https://github.com/sergiotasconmorales/locvqa.",
        "x1": 7.6972455978393555,
        "x2": 8.048433303833008,
        "y1": 7.835575103759766,
        "y2": 2.0219130516052246
      },
      {
        "r": 0,
        "text": "The dataset comprises high-resolution RGB-D images and pixel-level annotations of the fruits.",
        "trunc_text": "The dataset comprises high-resolution RGB-D images and pixel-level annotations of the fruits.",
        "x1": 1.3728266954421997,
        "x2": 4.968609809875488,
        "y1": 7.6118035316467285,
        "y2": 1.5371482372283936
      },
      {
        "r": 0,
        "text": "TGB datasets, data loaders, example codes, evaluation setup, and leaderboards are publicly available at https://tgb.complexdatalab.com/ .",
        "trunc_text": "TGB datasets, data loaders, example codes, evaluation setup, and leaderboards are publicly available at https://tgb.comp",
        "x1": 7.353553295135498,
        "x2": 7.786431789398193,
        "y1": 7.478731632232666,
        "y2": 2.1012015342712402
      },
      {
        "r": 0,
        "text": "Our code, models, instruction-sets and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.",
        "trunc_text": "Our code, models, instruction-sets and demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.",
        "x1": 6.762180328369141,
        "x2": 8.598163604736328,
        "y1": 3.839895486831665,
        "y2": 2.225935459136963
      },
      {
        "r": 0,
        "text": "We extensively evaluate SeaLog on two public datasets and an industrial dataset.",
        "trunc_text": "We extensively evaluate SeaLog on two public datasets and an industrial dataset.",
        "x1": 4.524452209472656,
        "x2": 5.438839435577393,
        "y1": 7.370542049407959,
        "y2": 2.7411715984344482
      },
      {
        "r": 0,
        "text": "Using the COVID-19 Open Research Dataset (CORD-19), we produced two datasets: (1) synCovid, which uses a combination of handwritten prompts and synthetic prompts generated using OpenAI, and (2) real abstracts, which contains abstract and title pairs.",
        "trunc_text": "Using the COVID-19 Open Research Dataset (CORD-19), we produced two datasets: (1) synCovid, which uses a combination of ",
        "x1": 5.871382713317871,
        "x2": 6.3270487785339355,
        "y1": 7.34822940826416,
        "y2": 2.2778995037078857
      },
      {
        "r": 0,
        "text": "Videos are available at: https://kristery.github.io/edt/",
        "trunc_text": "Videos are available at: https://kristery.github.io/edt/",
        "x1": 7.669958591461182,
        "x2": 8.507011413574219,
        "y1": 7.329948902130127,
        "y2": 2.1509716510772705
      },
      {
        "r": 0,
        "text": "This paper presents an end-to-end methodology for collecting datasets to recognize handwritten English alphabets by utilizing Inertial Measurement Units (IMUs) and leveraging the diversity present in the Indian writing style.",
        "trunc_text": "This paper presents an end-to-end methodology for collecting datasets to recognize handwritten English alphabets by util",
        "x1": 4.064700603485107,
        "x2": 1.6998577117919922,
        "y1": 1.8413368463516235,
        "y2": 7.0118327140808105
      },
      {
        "r": 0,
        "text": "Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
        "trunc_text": "Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
        "x1": 7.660422325134277,
        "x2": 8.461999893188477,
        "y1": 7.362253189086914,
        "y2": 2.1383607387542725
      },
      {
        "r": 0,
        "text": "In this paper, we study linear regression applied to data structured on a manifold.",
        "trunc_text": "In this paper, we study linear regression applied to data structured on a manifold.",
        "x1": -1.2453911304473877,
        "x2": 0.22104856371879578,
        "y1": 7.3071393966674805,
        "y2": 1.0250425338745117
      },
      {
        "r": 0,
        "text": "We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold's extrinsic geometry on the regression.",
        "trunc_text": "We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impa",
        "x1": -1.2422212362289429,
        "x2": 0.22012834250926971,
        "y1": 7.309776306152344,
        "y2": 1.0051149129867554
      },
      {
        "r": 0,
        "text": "This research can be extended and contributes to the field of pattern recognition and offers valuable insights for developing improved systems for handwriting recognition, particularly in diverse linguistic and cultural contexts.",
        "trunc_text": "This research can be extended and contributes to the field of pattern recognition and offers valuable insights for devel",
        "x1": 4.006786823272705,
        "x2": 1.5954170227050781,
        "y1": 1.7828214168548584,
        "y2": 7.0211310386657715
      },
      {
        "r": 0,
        "text": "To prove these theorems, we revisit William Thurston's results on the calisson tilability of a region $R$.",
        "trunc_text": "To prove these theorems, we revisit William Thurston's results on the calisson tilability of a region $R$.",
        "x1": 1.664230465888977,
        "x2": 3.331554651260376,
        "y1": 8.523314476013184,
        "y2": 0.761079728603363
      },
      {
        "r": 0,
        "text": "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on early releases of ChatGPT that elicit undesired behavior.",
        "trunc_text": "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the ",
        "x1": 7.20945930480957,
        "x2": 6.234758377075195,
        "y1": 5.122007846832275,
        "y2": 6.213191986083984
      },
      {
        "r": 0,
        "text": "We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks.",
        "trunc_text": "We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4",
        "x1": 7.303920269012451,
        "x2": 6.3354010581970215,
        "y1": 5.1544575691223145,
        "y2": 6.215064525604248
      },
      {
        "r": 0,
        "text": "Given a triangular grid in a hexagon and some given edges of the grid, the problem is to find a calisson tiling such that no input edge is overlapped and calissons adjacent to an input edge have different orientations.",
        "trunc_text": "Given a triangular grid in a hexagon and some given edges of the grid, the problem is to find a calisson tiling such tha",
        "x1": 1.5511935949325562,
        "x2": 3.3496832847595215,
        "y1": 8.592409133911133,
        "y2": 0.7636210918426514
      },
      {
        "r": 0,
        "text": "Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.",
        "trunc_text": "Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire I",
        "x1": 4.728065013885498,
        "x2": 3.688777208328247,
        "y1": 4.222161769866943,
        "y2": 5.76224422454834
      },
      {
        "r": 0,
        "text": "We extend the puzzle to regions $R$ that are not necessarily hexagonal.",
        "trunc_text": "We extend the puzzle to regions $R$ that are not necessarily hexagonal.",
        "x1": 1.6432465314865112,
        "x2": 3.343890905380249,
        "y1": 8.539424896240234,
        "y2": 0.7610692381858826
      },
      {
        "r": 0,
        "text": "By using a natural language generation model to abductively infer a premise given another premise and a conclusion, we can impute missing pieces of evidence needed for the conclusion to be true.",
        "trunc_text": "By using a natural language generation model to abductively infer a premise given another premise and a conclusion, we c",
        "x1": 3.3284101486206055,
        "x2": 2.451021909713745,
        "y1": 3.5719757080078125,
        "y2": 5.7242865562438965
      },
      {
        "r": 0,
        "text": "We sample multiple possible outputs for each step to achieve coverage of the search space, at the same time ensuring correctness by filtering low-quality generations with a round-trip validation procedure.",
        "trunc_text": "We sample multiple possible outputs for each step to achieve coverage of the search space, at the same time ensuring cor",
        "x1": 3.789207935333252,
        "x2": 4.059833526611328,
        "y1": 5.997704029083252,
        "y2": 3.7696330547332764
      },
      {
        "r": 0,
        "text": "Results on a modified version of the EntailmentBank dataset and a new dataset called Everyday Norms: Why Not? show that abductive generation with validation can recover premises across in- and out-of-domain settings",
        "trunc_text": "Results on a modified version of the EntailmentBank dataset and a new dataset called Everyday Norms: Why Not? show that ",
        "x1": 3.2450294494628906,
        "x2": 2.5066840648651123,
        "y1": 3.6074185371398926,
        "y2": 5.599977970123291
      },
      {
        "r": 0,
        "text": "To address this issue, this paper presents a systematic and comprehensive study, quantitatively and qualitatively, on training such models.",
        "trunc_text": "To address this issue, this paper presents a systematic and comprehensive study, quantitatively and qualitatively, on tr",
        "x1": 4.0188164710998535,
        "x2": 4.363293647766113,
        "y1": 5.839928150177002,
        "y2": 4.283493995666504
      },
      {
        "r": 0,
        "text": "We implement over 20 variants with controlled settings.",
        "trunc_text": "We implement over 20 variants with controlled settings.",
        "x1": 4.0841965675354,
        "x2": 5.269783973693848,
        "y1": 8.682509422302246,
        "y2": 0.8903244733810425
      },
      {
        "r": 0,
        "text": "For training data, we investigate the impact of data and sampling strategies.",
        "trunc_text": "For training data, we investigate the impact of data and sampling strategies.",
        "x1": 3.6102819442749023,
        "x2": 4.069159030914307,
        "y1": 5.954117774963379,
        "y2": 3.820009231567383
      },
      {
        "r": 0,
        "text": "For benchmarks, we contribute the first, to our best knowledge, comprehensive evaluation set including both image and video tasks through crowd-sourcing.",
        "trunc_text": "For benchmarks, we contribute the first, to our best knowledge, comprehensive evaluation set including both image and vi",
        "x1": 3.479525327682495,
        "x2": 4.185588836669922,
        "y1": 6.926126956939697,
        "y2": 3.1463170051574707
      },
      {
        "r": 0,
        "text": "Here, remote sensing can provide reliable estimates of plastic pollution by regularly monitoring and detecting marine debris in coastal areas.",
        "trunc_text": "Here, remote sensing can provide reliable estimates of plastic pollution by regularly monitoring and detecting marine de",
        "x1": -0.1877659559249878,
        "x2": 1.8562051057815552,
        "y1": 9.141942024230957,
        "y2": -0.2711606025695801
      },
      {
        "r": 0,
        "text": "Medium-resolution satellite data of coastal areas is readily available and can be leveraged to detect aggregations of marine debris containing plastic litter.",
        "trunc_text": "Medium-resolution satellite data of coastal areas is readily available and can be leveraged to detect aggregations of ma",
        "x1": -0.1818222850561142,
        "x2": 1.8458096981048584,
        "y1": 9.135957717895508,
        "y2": -0.2781158685684204
      },
      {
        "r": 0,
        "text": "In this work, we present a detector for marine debris built on a deep segmentation model that outputs a probability for marine debris at the pixel level.",
        "trunc_text": "In this work, we present a detector for marine debris built on a deep segmentation model that outputs a probability for ",
        "x1": -0.1025933027267456,
        "x2": 1.9435770511627197,
        "y1": 9.085953712463379,
        "y2": -0.18292847275733948
      },
      {
        "r": 0,
        "text": "We train this detector with a combination of annotated datasets of marine debris and evaluate it on specifically selected test sites where it is highly probable that plastic pollution is present in the detected marine debris.",
        "trunc_text": "We train this detector with a combination of annotated datasets of marine debris and evaluate it on specifically selecte",
        "x1": -0.15207165479660034,
        "x2": 1.872793436050415,
        "y1": 9.109007835388184,
        "y2": -0.23690053820610046
      },
      {
        "r": 0,
        "text": "We demonstrate quantitatively and qualitatively that a deep learning model trained on this dataset issued from multiple sources outperforms existing detection models trained on previous datasets by a large margin.",
        "trunc_text": "We demonstrate quantitatively and qualitatively that a deep learning model trained on this dataset issued from multiple ",
        "x1": 1.9158716201782227,
        "x2": 2.366628646850586,
        "y1": 5.869019031524658,
        "y2": 2.8044512271881104
      },
      {
        "r": 0,
        "text": "We hope to accelerate advances in the large-scale automated detection of marine debris, which is a step towards quantifying and monitoring marine litter with remote sensing at global scales, and release the model weights and training source code under https://github.com/marccoru/marinedebrisdetector",
        "trunc_text": "We hope to accelerate advances in the large-scale automated detection of marine debris, which is a step towards quantify",
        "x1": -0.15087132155895233,
        "x2": 1.859169840812683,
        "y1": 9.11560344696045,
        "y2": -0.2500424385070801
      },
      {
        "r": 0,
        "text": "This paper proposes a framework called <projektor>, which predicts model performance and supports data selection decisions based on partial samples of prospective data sources.",
        "trunc_text": "This paper proposes a framework called <projektor>, which predicts model performance and supports data selection decisio",
        "x1": 3.7181148529052734,
        "x2": 4.183309078216553,
        "y1": 5.947376251220703,
        "y2": 3.928771734237671
      },
      {
        "r": 0,
        "text": "In the first stage, we leverage the Optimal Transport distance to predict the model's performance for any data mixture ratio within the range of disclosed data sizes.",
        "trunc_text": "In the first stage, we leverage the Optimal Transport distance to predict the model's performance for any data mixture r",
        "x1": 3.9066476821899414,
        "x2": 4.730515003204346,
        "y1": 6.514272689819336,
        "y2": 3.592782735824585
      },
      {
        "r": 0,
        "text": "To facilitate the development of comprehensive scene understanding solutions using multiple camera views, a new dataset called Road Genome (OpenLane-V2) has been released.",
        "trunc_text": "To facilitate the development of comprehensive scene understanding solutions using multiple camera views, a new dataset ",
        "x1": 3.459712028503418,
        "x2": 5.042383193969727,
        "y1": 10.140730857849121,
        "y2": -0.07908211648464203
      },
      {
        "r": 0,
        "text": "We achieve new state-of-the-art performance on the large-scale Waymo Open Dataset.",
        "trunc_text": "We achieve new state-of-the-art performance on the large-scale Waymo Open Dataset.",
        "x1": 4.5019707679748535,
        "x2": 4.706008434295654,
        "y1": 6.9752678871154785,
        "y2": 2.882864475250244
      },
      {
        "r": 0,
        "text": "Finally, we train a detector that generalizes to a wide range of part segmentation datasets while achieving better performance than dataset-specific training.",
        "trunc_text": "Finally, we train a detector that generalizes to a wide range of part segmentation datasets while achieving better perfo",
        "x1": 0.7765271663665771,
        "x2": 2.333181381225586,
        "y1": 7.183330059051514,
        "y2": 1.4623323678970337
      },
      {
        "r": 0,
        "text": "Finally, we release a large-scale synthetic dataset with 1.4M examples generated using TrueTeacher.",
        "trunc_text": "Finally, we release a large-scale synthetic dataset with 1.4M examples generated using TrueTeacher.",
        "x1": 5.165215492248535,
        "x2": 5.353381633758545,
        "y1": 6.930710792541504,
        "y2": 2.2575759887695312
      },
      {
        "r": 0,
        "text": "To fully unlock model capabilities for high-quality video generation, we curate a large-scale video dataset called HD-VG-130M. This dataset comprises 130 million text-video pairs from the open-domain, ensuring high-definition, widescreen and watermark-free characters.",
        "trunc_text": "To fully unlock model capabilities for high-quality video generation, we curate a large-scale video dataset called HD-VG",
        "x1": 1.7643085718154907,
        "x2": 3.1433603763580322,
        "y1": 8.363118171691895,
        "y2": 0.9747419357299805
      },
      {
        "r": 0,
        "text": "Code and datasets are available in https://github.com/zjunlp/DeepKE/tree/main/example/llm.",
        "trunc_text": "Code and datasets are available in https://github.com/zjunlp/DeepKE/tree/main/example/llm.",
        "x1": 7.37099552154541,
        "x2": 7.869103908538818,
        "y1": 7.772899150848389,
        "y2": 1.9731043577194214
      },
      {
        "r": 0,
        "text": "We evaluate TagGPT on publicly available datasets, i.e., Kuaishou and Food.com, and demonstrate the effectiveness of TagGPT compared to existing hashtags and off-the-shelf taggers.",
        "trunc_text": "We evaluate TagGPT on publicly available datasets, i.e., Kuaishou and Food.com, and demonstrate the effectiveness of Tag",
        "x1": 2.543215751647949,
        "x2": 1.5834358930587769,
        "y1": 3.5382509231567383,
        "y2": 5.518374919891357
      },
      {
        "r": 0,
        "text": "Additionally, to track new and creative applications for bioinformatics tools such as ChatGPT, we have established a GitHub repository at https://github.com/csbl-br/awesome-compbio-chatgpt.",
        "trunc_text": "Additionally, to track new and creative applications for bioinformatics tools such as ChatGPT, we have established a Git",
        "x1": 6.586867809295654,
        "x2": 4.753059387207031,
        "y1": 3.766104221343994,
        "y2": 6.922379016876221
      },
      {
        "r": 0,
        "text": "All data and trained models are publicly available.",
        "trunc_text": "All data and trained models are publicly available.",
        "x1": 7.713701248168945,
        "x2": 8.106456756591797,
        "y1": 7.130088806152344,
        "y2": 2.7768216133117676
      },
      {
        "r": 0,
        "text": "We have conducted extensive experiments on 16 public log datasets.",
        "trunc_text": "We have conducted extensive experiments on 16 public log datasets.",
        "x1": 4.614532947540283,
        "x2": 5.373019695281982,
        "y1": 7.40217924118042,
        "y2": 2.9046952724456787
      },
      {
        "r": 0,
        "text": "We also release the code and the annotated dataset for replication and future research.",
        "trunc_text": "We also release the code and the annotated dataset for replication and future research.",
        "x1": 6.544251918792725,
        "x2": 7.242033004760742,
        "y1": 7.352070331573486,
        "y2": 2.4807631969451904
      },
      {
        "r": 0,
        "text": "The training data for these models is usually collected from open-source repositories (e.g., GitHub) that contain software faults and security vulnerabilities.",
        "trunc_text": "The training data for these models is usually collected from open-source repositories (e.g., GitHub) that contain softwa",
        "x1": 7.701037883758545,
        "x2": 6.7190632820129395,
        "y1": 5.260862827301025,
        "y2": 6.4094133377075195
      },
      {
        "r": 0,
        "text": "CCUB dataset is curated and our approach is evaluated by people who have a personal relationship with that particular culture.",
        "trunc_text": "CCUB dataset is curated and our approach is evaluated by people who have a personal relationship with that particular cu",
        "x1": 5.52631139755249,
        "x2": 6.064211845397949,
        "y1": 7.281611919403076,
        "y2": 2.5632822513580322
      },
      {
        "r": 0,
        "text": "Project page: https://europe.naverlabs.com/imagenet-sd/",
        "trunc_text": "Project page: https://europe.naverlabs.com/imagenet-sd/",
        "x1": 7.871039390563965,
        "x2": 8.500521659851074,
        "y1": 8.007705688476562,
        "y2": 1.871315598487854
      },
      {
        "r": 0,
        "text": "GitHub repository: https://github.com/ymcui/Chinese-LLaMA-Alpaca",
        "trunc_text": "GitHub repository: https://github.com/ymcui/Chinese-LLaMA-Alpaca",
        "x1": 7.51558256149292,
        "x2": 8.199109077453613,
        "y1": 7.55122184753418,
        "y2": 1.871117115020752
      },
      {
        "r": 0,
        "text": "The resulting \\textbf{C}hinese \\textbf{O}pen \\textbf{I}nstruction \\textbf{G}eneralist (\\textbf{COIG}) corpora are available in Huggingface\\footnote{\\url{https://huggingface.co/datasets/BAAI/COIG}} and Github\\footnote{\\url{https://github.com/FlagOpen/FlagInstruct}}, and will be continuously updated.",
        "trunc_text": "The resulting \\textbf{C}hinese \\textbf{O}pen \\textbf{I}nstruction \\textbf{G}eneralist (\\textbf{COIG}) corpora are availa",
        "x1": 7.888484001159668,
        "x2": 8.460869789123535,
        "y1": 7.768563270568848,
        "y2": 2.14509916305542
      },
      {
        "r": 0,
        "text": "We make our model, data, as well as code publicly available.",
        "trunc_text": "We make our model, data, as well as code publicly available.",
        "x1": 7.745261192321777,
        "x2": 8.192710876464844,
        "y1": 7.015684604644775,
        "y2": 2.8643319606781006
      },
      {
        "r": 0,
        "text": "The data, code, and all model outputs are released in https://github.com/microsoft/AGIEval.",
        "trunc_text": "The data, code, and all model outputs are released in https://github.com/microsoft/AGIEval.",
        "x1": 7.444866180419922,
        "x2": 7.853267669677734,
        "y1": 7.341190814971924,
        "y2": 2.369678020477295
      },
      {
        "r": 0,
        "text": "We created a comprehensive dataset including 492.5K samples comprising code-related content produced by ChatGPT, encompassing popular software activities like Q&A (115K), code summarization (126K), and code generation (226.5K).",
        "trunc_text": "We created a comprehensive dataset including 492.5K samples comprising code-related content produced by ChatGPT, encompa",
        "x1": 6.515661239624023,
        "x2": 4.708551406860352,
        "y1": 3.802717685699463,
        "y2": 6.86530876159668
      },
      {
        "r": 0,
        "text": "The code is publicly available at https://github.com/Vision-CAIR/ChatCaptioner",
        "trunc_text": "The code is publicly available at https://github.com/Vision-CAIR/ChatCaptioner",
        "x1": 6.765621185302734,
        "x2": 8.81489086151123,
        "y1": 3.8102428913116455,
        "y2": 2.295109272003174
      },
      {
        "r": 0,
        "text": "Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor.",
        "trunc_text": "Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor.",
        "x1": 6.521927833557129,
        "x2": 4.673552989959717,
        "y1": 3.664614677429199,
        "y2": 6.917045593261719
      },
      {
        "r": 0,
        "text": "Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.",
        "trunc_text": "Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.",
        "x1": 7.338911056518555,
        "x2": 7.936141490936279,
        "y1": 7.764354228973389,
        "y2": 2.00883412361145
      },
      {
        "r": 0,
        "text": "Our source code and datasets are available at https://github.com/xinleihe/MGTBench.",
        "trunc_text": "Our source code and datasets are available at https://github.com/xinleihe/MGTBench.",
        "x1": 7.427187442779541,
        "x2": 7.849940299987793,
        "y1": 7.786895751953125,
        "y2": 2.0380539894104004
      },
      {
        "r": 0,
        "text": "The training data, codes, and weights of this project are available at: The training data, codes, and weights of this project are available at: https://github.com/Kent0n-Li/ChatDoctor.",
        "trunc_text": "The training data, codes, and weights of this project are available at: The training data, codes, and weights of this pr",
        "x1": 6.699934959411621,
        "x2": 7.908433437347412,
        "y1": 3.793433427810669,
        "y2": 2.0483996868133545
      },
      {
        "r": 0,
        "text": "Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API.",
        "trunc_text": "Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API.",
        "x1": 6.610718727111816,
        "x2": 4.73818826675415,
        "y1": 3.6750948429107666,
        "y2": 6.944683074951172
      },
      {
        "r": 0,
        "text": "The dataset and code are available at https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.",
        "trunc_text": "The dataset and code are available at https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.",
        "x1": 6.454324722290039,
        "x2": 4.631923675537109,
        "y1": 3.543701410293579,
        "y2": 6.959385871887207
      },
      {
        "r": 0,
        "text": "To support further research in related fields, we have made the data generated by ChatGPT publicly available at https://github.com/THU-BPM/chatgpt-sql.",
        "trunc_text": "To support further research in related fields, we have made the data generated by ChatGPT publicly available at https://",
        "x1": 6.6229424476623535,
        "x2": 4.711580753326416,
        "y1": 3.661174774169922,
        "y2": 6.969269275665283
      },
      {
        "r": 0,
        "text": "In the second stage, we extrapolate the performance to larger undisclosed data sizes based on a novel parameter-free mapping technique inspired by neural scaling laws.",
        "trunc_text": "In the second stage, we extrapolate the performance to larger undisclosed data sizes based on a novel parameter-free map",
        "x1": 3.924264669418335,
        "x2": 4.89371919631958,
        "y1": 6.581475734710693,
        "y2": 3.418999195098877
      },
      {
        "r": 0,
        "text": "Also, <projektor> outperforms by a wide margin in data selection effectiveness compared to a range of other off-the-shelf solutions.",
        "trunc_text": "Also, <projektor> outperforms by a wide margin in data selection effectiveness compared to a range of other off-the-shel",
        "x1": 3.68363881111145,
        "x2": 4.230438709259033,
        "y1": 5.961257457733154,
        "y2": 3.895357847213745
      },
      {
        "r": 0,
        "text": "Database alignment is a variant of the graph alignment problem: Given a pair of anonymized databases containing separate yet correlated features for a set of users, the problem is to identify the correspondence between the features and align the anonymized user sets based on correlation alone.",
        "trunc_text": "Database alignment is a variant of the graph alignment problem: Given a pair of anonymized databases containing separate",
        "x1": 3.393300771713257,
        "x2": 3.4938459396362305,
        "y1": 6.104717254638672,
        "y2": 3.8768773078918457
      },
      {
        "r": 0,
        "text": "We study an instance of the database alignment problem with multivariate Gaussian features and derive results that apply both for database alignment and for planted matching, demonstrating the connection between them.",
        "trunc_text": "We study an instance of the database alignment problem with multivariate Gaussian features and derive results that apply",
        "x1": 3.3751380443573,
        "x2": 3.522271156311035,
        "y1": 6.1157307624816895,
        "y2": 3.9256973266601562
      },
      {
        "r": 0,
        "text": "The maximum likelihood algorithms for both planted matching and database alignment take the form of a linear program and we study relaxations to better understand the significance of various constraints under various conditions and present achievability and converse bounds.",
        "trunc_text": "The maximum likelihood algorithms for both planted matching and database alignment take the form of a linear program and",
        "x1": 3.431891679763794,
        "x2": 3.512551784515381,
        "y1": 6.1279730796813965,
        "y2": 3.969264030456543
      },
      {
        "r": 0,
        "text": "Our analysis and results extend to the unbalanced case where one user set is not fully covered by the alignment.",
        "trunc_text": "Our analysis and results extend to the unbalanced case where one user set is not fully covered by the alignment.",
        "x1": 3.484279155731201,
        "x2": 3.5072455406188965,
        "y1": 6.1451945304870605,
        "y2": 3.9035708904266357
      },
      {
        "r": 0,
        "text": "They only work for in-distribution artifact types generated during training.",
        "trunc_text": "They only work for in-distribution artifact types generated during training.",
        "x1": 2.2276883125305176,
        "x2": 2.648911714553833,
        "y1": 6.520859718322754,
        "y2": 2.429060459136963
      },
      {
        "r": 0,
        "text": "In this paper, we analyze the cause and characteristics of the GAN artifacts produced in unseen test data without ground-truths.",
        "trunc_text": "In this paper, we analyze the cause and characteristics of the GAN artifacts produced in unseen test data without ground",
        "x1": 2.2062582969665527,
        "x2": 2.969060182571411,
        "y1": 6.8910627365112305,
        "y2": 2.420844554901123
      },
      {
        "r": 0,
        "text": "We then develop a novel method, namely, DeSRA, to Detect and then Delete those SR Artifacts in practice.",
        "trunc_text": "We then develop a novel method, namely, DeSRA, to Detect and then Delete those SR Artifacts in practice.",
        "x1": 2.247860908508301,
        "x2": 2.85089373588562,
        "y1": 6.717894077301025,
        "y2": 2.3493168354034424
      },
      {
        "r": 0,
        "text": "After detecting the artifact regions, we develop a finetune procedure to improve GAN-based SR models with a few samples, so that they can deal with similar types of artifacts in more unseen real data.",
        "trunc_text": "After detecting the artifact regions, we develop a finetune procedure to improve GAN-based SR models with a few samples,",
        "x1": 2.2197561264038086,
        "x2": 3.017927646636963,
        "y1": 6.871010780334473,
        "y2": 2.3627490997314453
      },
      {
        "r": 0,
        "text": "The code will be available at https://github.com/TencentARC/DeSRA.",
        "trunc_text": "The code will be available at https://github.com/TencentARC/DeSRA.",
        "x1": 8.20026969909668,
        "x2": 8.805447578430176,
        "y1": 7.63245964050293,
        "y2": 2.3035337924957275
      },
      {
        "r": 0,
        "text": "In this work, we review robustness issues of DL and particularly bridge concerns and attempts from approximation theory to statistical learning theory.",
        "trunc_text": "In this work, we review robustness issues of DL and particularly bridge concerns and attempts from approximation theory ",
        "x1": 1.8903639316558838,
        "x2": 1.38369882106781,
        "y1": 4.8727922439575195,
        "y2": 3.614302158355713
      },
      {
        "r": 0,
        "text": "Further, we review Bayesian Deep Learning as a means for uncertainty quantification and rigorous explainability.",
        "trunc_text": "Further, we review Bayesian Deep Learning as a means for uncertainty quantification and rigorous explainability.",
        "x1": 1.797800898551941,
        "x2": 2.1416330337524414,
        "y1": 5.477327346801758,
        "y2": 3.2054030895233154
      },
      {
        "r": 0,
        "text": "A new control paradigm using angular momentum and foot placement as state variables in the linear inverted pendulum model has expanded the realm of possibilities for the control of bipedal robots.",
        "trunc_text": "A new control paradigm using angular momentum and foot placement as state variables in the linear inverted pendulum mode",
        "x1": 2.985020160675049,
        "x2": 4.34895658493042,
        "y1": 9.3469877243042,
        "y2": -0.5431102514266968
      },
      {
        "r": 0,
        "text": "This new paradigm, known as the ALIP model, has shown effectiveness in cases where a robot's center of mass height can be assumed to be constant or near constant as well as in cases where there are no non-kinematic restrictions on foot placement.",
        "trunc_text": "This new paradigm, known as the ALIP model, has shown effectiveness in cases where a robot's center of mass height can b",
        "x1": 2.935182571411133,
        "x2": 4.28031587600708,
        "y1": 9.322392463684082,
        "y2": -0.5610411763191223
      },
      {
        "r": 0,
        "text": "Our code is available at https://github.com/amazon-science/codetaskcl-pptf",
        "trunc_text": "Our code is available at https://github.com/amazon-science/codetaskcl-pptf",
        "x1": 8.279430389404297,
        "x2": 8.800518035888672,
        "y1": 7.569182395935059,
        "y2": 2.3060519695281982
      },
      {
        "r": 0,
        "text": "We also show that it is possible to generate fully-synthetic image-annotation pairs to automatically augment any annotated dataset.",
        "trunc_text": "We also show that it is possible to generate fully-synthetic image-annotation pairs to automatically augment any annotat",
        "x1": 2.0475504398345947,
        "x2": 3.513127088546753,
        "y1": 7.410754203796387,
        "y2": 1.8515557050704956
      },
      {
        "r": 0,
        "text": "We present and release a new dataset of 50 manually annotated research articles.",
        "trunc_text": "We present and release a new dataset of 50 manually annotated research articles.",
        "x1": 2.7760684490203857,
        "x2": 1.6791046857833862,
        "y1": 3.3792037963867188,
        "y2": 5.624662399291992
      },
      {
        "r": 0,
        "text": "The code and trained weights are available at https://github.com/mordecaimalignatius/GAFAR/.",
        "trunc_text": "The code and trained weights are available at https://github.com/mordecaimalignatius/GAFAR/.",
        "x1": 7.565427780151367,
        "x2": 7.9547600746154785,
        "y1": 7.682176113128662,
        "y2": 2.020965814590454
      },
      {
        "r": 0,
        "text": "We build on existing tools to computationally analyze data retrieved from publicly available databases.",
        "trunc_text": "We build on existing tools to computationally analyze data retrieved from publicly available databases.",
        "x1": 5.924257755279541,
        "x2": 6.569126129150391,
        "y1": 7.170037269592285,
        "y2": 2.8331801891326904
      },
      {
        "r": 0,
        "text": "Among multiple benchmarks on the KITTI dataset, we achieve new state-of-the-art performance.",
        "trunc_text": "Among multiple benchmarks on the KITTI dataset, we achieve new state-of-the-art performance.",
        "x1": 4.36521577835083,
        "x2": 4.410686492919922,
        "y1": 6.66440486907959,
        "y2": 3.138329029083252
      },
      {
        "r": 0,
        "text": "Codes and models are publicly available at https://github.com/sunlicai/MAE-DFER.",
        "trunc_text": "Codes and models are publicly available at https://github.com/sunlicai/MAE-DFER.",
        "x1": 7.6505961418151855,
        "x2": 8.200770378112793,
        "y1": 7.405324935913086,
        "y2": 2.4591891765594482
      },
      {
        "r": 0,
        "text": "We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabilities detected in each individual program including location and function name, which makes the dataset ideal to train LLMs and machine learning algorithms.",
        "trunc_text": "We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabil",
        "x1": 7.664309978485107,
        "x2": 6.733201503753662,
        "y1": 5.094263076782227,
        "y2": 6.441110610961914
      },
      {
        "r": 0,
        "text": "Our implementation will be publicly available at \\url{https://github.com/ETHRuiGong/PTDiffSeg}.",
        "trunc_text": "Our implementation will be publicly available at \\url{https://github.com/ETHRuiGong/PTDiffSeg}.",
        "x1": 7.971231937408447,
        "x2": 8.509114265441895,
        "y1": 7.75109338760376,
        "y2": 2.2774484157562256
      },
      {
        "r": 0,
        "text": "Our dataset covers 520 images of mathematical graphics collected from 450 documents from different disciplines.",
        "trunc_text": "Our dataset covers 520 images of mathematical graphics collected from 450 documents from different disciplines.",
        "x1": 2.23433780670166,
        "x2": 4.315952777862549,
        "y1": 7.541132926940918,
        "y2": 1.6704771518707275
      },
      {
        "r": 0,
        "text": "The classification accuracy of the models in the absence and presence of the two attacks are computed on images from the publicly accessible ImageNet dataset.",
        "trunc_text": "The classification accuracy of the models in the absence and presence of the two attacks are computed on images from the",
        "x1": 7.268772125244141,
        "x2": 6.553473472595215,
        "y1": 5.578468322753906,
        "y2": 5.771376132965088
      },
      {
        "r": 0,
        "text": "Furthermore, it facilitates the creation of de-identified datasets for broader 2D image research at major research institutions.",
        "trunc_text": "Furthermore, it facilitates the creation of de-identified datasets for broader 2D image research at major research insti",
        "x1": 1.9622859954833984,
        "x2": 3.898833751678467,
        "y1": 7.860218048095703,
        "y2": 1.5288902521133423
      },
      {
        "r": 0,
        "text": "State-of-the-art results are achieved even on more detailed part-segmentation, Pascal-Animals, by only training on coarse-grained datasets.",
        "trunc_text": "State-of-the-art results are achieved even on more detailed part-segmentation, Pascal-Animals, by only training on coars",
        "x1": 0.8196502327919006,
        "x2": 2.4803531169891357,
        "y1": 7.1442179679870605,
        "y2": 1.3640694618225098
      },
      {
        "r": 0,
        "text": "Our code will be available at the URL: https://github.com/cofly2014/tsa-mlt.git",
        "trunc_text": "Our code will be available at the URL: https://github.com/cofly2014/tsa-mlt.git",
        "x1": 7.938840389251709,
        "x2": 8.692097663879395,
        "y1": 7.672255039215088,
        "y2": 2.2230560779571533
      },
      {
        "r": 0,
        "text": "KiTS21 is a sequel to its first edition in 2019, and it features a variety of innovations in how the challenge was designed, in addition to a larger dataset.",
        "trunc_text": "KiTS21 is a sequel to its first edition in 2019, and it features a variety of innovations in how the challenge was desig",
        "x1": 4.886349678039551,
        "x2": 5.066837310791016,
        "y1": 7.566723346710205,
        "y2": 2.2868409156799316
      },
      {
        "r": 0,
        "text": "Additionally, we introduce Tomatopia, a new, large and challenging dataset of greenhouse tomatoes.",
        "trunc_text": "Additionally, we introduce Tomatopia, a new, large and challenging dataset of greenhouse tomatoes.",
        "x1": 1.3397475481033325,
        "x2": 5.146060466766357,
        "y1": 7.601905345916748,
        "y2": 1.6567740440368652
      },
      {
        "r": 0,
        "text": "TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks.",
        "trunc_text": "TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and c",
        "x1": 5.282235145568848,
        "x2": 6.60955810546875,
        "y1": 7.603796005249023,
        "y2": 1.9765727519989014
      },
      {
        "r": 0,
        "text": "The code for this algorithm will be publicly available.",
        "trunc_text": "The code for this algorithm will be publicly available.",
        "x1": 8.27694320678711,
        "x2": 8.831311225891113,
        "y1": 7.324273586273193,
        "y2": 2.69166898727417
      },
      {
        "r": 0,
        "text": "The code and models will be made publicly at \\href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}.",
        "trunc_text": "The code and models will be made publicly at \\href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}",
        "x1": 8.009774208068848,
        "x2": 8.484233856201172,
        "y1": 7.614422798156738,
        "y2": 2.435215473175049
      },
      {
        "r": 0,
        "text": "In the fields of Experimental and Computational Aesthetics, numerous image datasets have been created over the last two decades.",
        "trunc_text": "In the fields of Experimental and Computational Aesthetics, numerous image datasets have been created over the last two ",
        "x1": 2.186614990234375,
        "x2": 3.946108102798462,
        "y1": 7.960386753082275,
        "y2": 1.4248641729354858
      },
      {
        "r": 0,
        "text": "Our code is publicly available at https://github.com/withchencheng/ECML_PKDD_23_Real.",
        "trunc_text": "Our code is publicly available at https://github.com/withchencheng/ECML_PKDD_23_Real.",
        "x1": 8.312982559204102,
        "x2": 8.853503227233887,
        "y1": 7.525223255157471,
        "y2": 2.4283511638641357
      },
      {
        "r": 0,
        "text": "We share this visualization and the dataset in the spirit of open science.",
        "trunc_text": "We share this visualization and the dataset in the spirit of open science.",
        "x1": 4.336770534515381,
        "x2": 5.910817623138428,
        "y1": 8.135936737060547,
        "y2": 2.262561559677124
      },
      {
        "r": 0,
        "text": "covLLM was trained with LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca and synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real abstract datasets.",
        "trunc_text": "covLLM was trained with LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca and synCovid data",
        "x1": 5.7513885498046875,
        "x2": 4.721402645111084,
        "y1": 4.751810073852539,
        "y2": 5.886908531188965
      },
      {
        "r": 0,
        "text": "Results demonstrate that training covLLM on the synCovid and abstract pairs datasets performs competitively with ChatGPT and outperforms covLLM trained primarily using the Alpaca dataset.",
        "trunc_text": "Results demonstrate that training covLLM on the synCovid and abstract pairs datasets performs competitively with ChatGPT",
        "x1": 6.433396339416504,
        "x2": 4.677735805511475,
        "y1": 3.798288583755493,
        "y2": 6.603915691375732
      },
      {
        "r": 0,
        "text": "Using the MIMIC-IT dataset, we train a large VLM named Otter.",
        "trunc_text": "Using the MIMIC-IT dataset, we train a large VLM named Otter.",
        "x1": 4.88604211807251,
        "x2": 5.53687047958374,
        "y1": 6.691786289215088,
        "y2": 2.756213903427124
      },
      {
        "r": 0,
        "text": "The code will be made available.",
        "trunc_text": "The code will be made available.",
        "x1": 8.313570022583008,
        "x2": 8.954340934753418,
        "y1": 7.443434715270996,
        "y2": 2.645479440689087
      },
      {
        "r": 0,
        "text": "Project page: https://ba2det.site .",
        "trunc_text": "Project page: https://ba2det.site .",
        "x1": 7.92707633972168,
        "x2": 8.520386695861816,
        "y1": 8.001031875610352,
        "y2": 1.871074914932251
      },
      {
        "r": 0,
        "text": "This dataset allows for the exploration of complex road connections and situations where lane markings may be absent.",
        "trunc_text": "This dataset allows for the exploration of complex road connections and situations where lane markings may be absent.",
        "x1": 3.508694887161255,
        "x2": 5.204649925231934,
        "y1": 10.156853675842285,
        "y2": -0.09114466607570648
      },
      {
        "r": 0,
        "text": "Specifically, we benchmark the recognition drop on common detection datasets, where we evaluate both traditional and realistic anonymization for faces and full bodies.",
        "trunc_text": "Specifically, we benchmark the recognition drop on common detection datasets, where we evaluate both traditional and rea",
        "x1": 2.92991304397583,
        "x2": 4.2742085456848145,
        "y1": 7.497035980224609,
        "y2": 2.541944742202759
      },
      {
        "r": 0,
        "text": "With this CNN, we derived homogeneous atmospheric parameters and abundances for 841300 stars, that remarkably compared to external data-sets.",
        "trunc_text": "With this CNN, we derived homogeneous atmospheric parameters and abundances for 841300 stars, that remarkably compared t",
        "x1": 1.9455757141113281,
        "x2": 4.933706283569336,
        "y1": 8.33696174621582,
        "y2": 1.9891917705535889
      },
      {
        "r": 0,
        "text": "The final trained model is publicly available at https://github.com/Jesper-Karsten/MBASC",
        "trunc_text": "The final trained model is publicly available at https://github.com/Jesper-Karsten/MBASC",
        "x1": 7.726943016052246,
        "x2": 8.232854843139648,
        "y1": 7.327322006225586,
        "y2": 2.5612375736236572
      },
      {
        "r": 0,
        "text": "All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.",
        "trunc_text": "All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.",
        "x1": 7.0525689125061035,
        "x2": 7.564583778381348,
        "y1": 7.6806321144104,
        "y2": 2.2103726863861084
      },
      {
        "r": 0,
        "text": "In this paper, we propose a detector with the ability to predict both open-vocabulary objects and their part segmentation.",
        "trunc_text": "In this paper, we propose a detector with the ability to predict both open-vocabulary objects and their part segmentatio",
        "x1": 0.7552657723426819,
        "x2": 2.368152141571045,
        "y1": 7.3201003074646,
        "y2": 1.4246271848678589
      },
      {
        "r": 0,
        "text": "First, we train the detector on the joint of part-level, object-level and image-level data to build the multi-granularity alignment between language and image.",
        "trunc_text": "First, we train the detector on the joint of part-level, object-level and image-level data to build the multi-granularit",
        "x1": 4.060746192932129,
        "x2": 2.5416903495788574,
        "y1": 3.883962392807007,
        "y2": 4.4973649978637695
      },
      {
        "r": 0,
        "text": "In light of this, the paper aims to analyze the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face, which is the most popular repository for pretrained ML models.",
        "trunc_text": "In light of this, the paper aims to analyze the measurement of the carbon footprint of 1,417 ML models and associated da",
        "x1": 2.7782559394836426,
        "x2": 4.028292655944824,
        "y1": 7.296450614929199,
        "y2": 2.4199070930480957
      },
      {
        "r": 0,
        "text": "Current methods rely on datasets with expensive annotations; multi-view images and their camera parameters.",
        "trunc_text": "Current methods rely on datasets with expensive annotations; multi-view images and their camera parameters.",
        "x1": 1.8581194877624512,
        "x2": 3.696429967880249,
        "y1": 7.914462566375732,
        "y2": 1.3410791158676147
      },
      {
        "r": 0,
        "text": "The WHOW-KG consists of a network of five ontologies and related linked open data, modelled according to those ontologies.",
        "trunc_text": "The WHOW-KG consists of a network of five ontologies and related linked open data, modelled according to those ontologie",
        "x1": 5.960415840148926,
        "x2": 6.59242057800293,
        "y1": 7.4783034324646,
        "y2": 2.4700675010681152
      },
      {
        "r": 0,
        "text": "We propose Multi-CrossRE, the broadest multi-lingual dataset for RE, including 26 languages in addition to English, and covering six text domains.",
        "trunc_text": "We propose Multi-CrossRE, the broadest multi-lingual dataset for RE, including 26 languages in addition to English, and ",
        "x1": 4.062301158905029,
        "x2": 2.4465363025665283,
        "y1": 2.8606858253479004,
        "y2": 6.2501325607299805
      },
      {
        "r": 0,
        "text": "We run a baseline model over the 26 new datasets and--as sanity check--over the 26 back-translations to English.",
        "trunc_text": "We run a baseline model over the 26 new datasets and--as sanity check--over the 26 back-translations to English.",
        "x1": 4.1032819747924805,
        "x2": 2.658557176589966,
        "y1": 2.962451219558716,
        "y2": 6.16895055770874
      },
      {
        "r": 0,
        "text": "We also evaluate performance on the MultiMedQA suite of benchmark datasets.",
        "trunc_text": "We also evaluate performance on the MultiMedQA suite of benchmark datasets.",
        "x1": 4.232223987579346,
        "x2": 4.399497985839844,
        "y1": 6.563326358795166,
        "y2": 3.1920366287231445
      },
      {
        "r": 0,
        "text": "Our model and code are available at https://github.com/microsoft/LMOps.",
        "trunc_text": "Our model and code are available at https://github.com/microsoft/LMOps.",
        "x1": 7.856240749359131,
        "x2": 8.44996166229248,
        "y1": 7.545709133148193,
        "y2": 2.354617118835449
      },
      {
        "r": 0,
        "text": "To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories.",
        "trunc_text": "To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executi",
        "x1": 7.648072719573975,
        "x2": 6.703122615814209,
        "y1": 5.123567581176758,
        "y2": 6.427911758422852
      },
      {
        "r": 0,
        "text": "We also release codebase for evaluation set extraction.",
        "trunc_text": "We also release codebase for evaluation set extraction.",
        "x1": 6.986395359039307,
        "x2": 7.533995151519775,
        "y1": 7.272111415863037,
        "y2": 2.4976251125335693
      },
      {
        "r": 0,
        "text": "Dataset, to fight the bias prevalent in giant datasets.",
        "trunc_text": "Dataset, to fight the bias prevalent in giant datasets.",
        "x1": 5.063125133514404,
        "x2": 5.809182643890381,
        "y1": 7.317767143249512,
        "y2": 2.4297852516174316
      },
      {
        "r": 0,
        "text": "We will make our code and pre-trained models publicly available.",
        "trunc_text": "We will make our code and pre-trained models publicly available.",
        "x1": 7.818745136260986,
        "x2": 8.258063316345215,
        "y1": 7.00644063949585,
        "y2": 2.8638322353363037
      },
      {
        "r": 0,
        "text": "We perform an extensive study across six datasets with eight models from three model families.",
        "trunc_text": "We perform an extensive study across six datasets with eight models from three model families.",
        "x1": 4.3300652503967285,
        "x2": 4.8915696144104,
        "y1": 6.9203290939331055,
        "y2": 2.926140308380127
      },
      {
        "r": 0,
        "text": "For this, we augment standard bug-fixing datasets with bug report discussions.",
        "trunc_text": "For this, we augment standard bug-fixing datasets with bug report discussions.",
        "x1": 5.268304347991943,
        "x2": 5.57620906829834,
        "y1": 6.185518741607666,
        "y2": 3.800663471221924
      },
      {
        "r": 0,
        "text": "Resources are made publicly available through GitHub, fostering open research in the Chinese NLP community and beyond.",
        "trunc_text": "Resources are made publicly available through GitHub, fostering open research in the Chinese NLP community and beyond.",
        "x1": 6.406356334686279,
        "x2": 7.35470724105835,
        "y1": 7.481893062591553,
        "y2": 2.85286021232605
      },
      {
        "r": 0,
        "text": "We release our code and data under fully permissive licenses.",
        "trunc_text": "We release our code and data under fully permissive licenses.",
        "x1": 7.904172897338867,
        "x2": 8.323077201843262,
        "y1": 7.061903476715088,
        "y2": 2.8190317153930664
      },
      {
        "r": 0,
        "text": "For enabling the combination of ChatGPT and RDF KGs, we present a research prototype, called GPToLODS, which is able to enrich any ChatGPT response with more information from hundreds of RDF KGs.",
        "trunc_text": "For enabling the combination of ChatGPT and RDF KGs, we present a research prototype, called GPToLODS, which is able to ",
        "x1": 6.6059441566467285,
        "x2": 4.72647762298584,
        "y1": 3.695723295211792,
        "y2": 6.9613800048828125
      },
      {
        "r": 0,
        "text": "In particular, it identifies and annotates each entity of the response with statistics and hyperlinks to LODsyndesis KG (which contains integrated data from 400 RDF KGs and over 412 million entities).",
        "trunc_text": "In particular, it identifies and annotates each entity of the response with statistics and hyperlinks to LODsyndesis KG ",
        "x1": 6.240899562835693,
        "x2": 6.757870674133301,
        "y1": 7.1434736251831055,
        "y2": 2.68277645111084
      },
      {
        "r": 0,
        "text": "Numerous AIGC detectors have been developed and evaluated on natural language data.",
        "trunc_text": "Numerous AIGC detectors have been developed and evaluated on natural language data.",
        "x1": 3.2212705612182617,
        "x2": 2.9000895023345947,
        "y1": 3.4146504402160645,
        "y2": 5.803774833679199
      },
      {
        "r": 0,
        "text": "We evaluated six AIGC detectors, including three commercial and three open-source solutions, assessing their performance on this dataset.",
        "trunc_text": "We evaluated six AIGC detectors, including three commercial and three open-source solutions, assessing their performance",
        "x1": 0.9374668002128601,
        "x2": 2.348841667175293,
        "y1": 6.933907985687256,
        "y2": 1.9055795669555664
      },
      {
        "r": 0,
        "text": "We have released a dataset comprised of ChatGPT's responses to support further research in this area.",
        "trunc_text": "We have released a dataset comprised of ChatGPT's responses to support further research in this area.",
        "x1": 6.498224258422852,
        "x2": 4.656016826629639,
        "y1": 3.5597403049468994,
        "y2": 6.957169055938721
      },
      {
        "r": 0,
        "text": "Both datasets involve scraping through known data sources (through platforms like stack overflow, crowdsourcing, etc.)",
        "trunc_text": "Both datasets involve scraping through known data sources (through platforms like stack overflow, crowdsourcing, etc.)",
        "x1": 6.881875038146973,
        "x2": 7.3137006759643555,
        "y1": 7.652604103088379,
        "y2": 2.1879842281341553
      },
      {
        "r": 0,
        "text": "We call the collected dataset the Human ChatGPT Comparison Corpus (HC3).",
        "trunc_text": "We call the collected dataset the Human ChatGPT Comparison Corpus (HC3).",
        "x1": 6.407115936279297,
        "x2": 4.454054832458496,
        "y1": 3.509561777114868,
        "y2": 6.9763360023498535
      },
      {
        "r": 0,
        "text": "The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection.",
        "trunc_text": "The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detecti",
        "x1": 6.595596790313721,
        "x2": 4.713092803955078,
        "y1": 3.6744561195373535,
        "y2": 7.01194429397583
      },
      {
        "r": 0,
        "text": "For datasets originally without shared tokens in label names, we also offer an automated method, using OpenAI's ChatGPT, to generate shared actions and objects.",
        "trunc_text": "For datasets originally without shared tokens in label names, we also offer an automated method, using OpenAI's ChatGPT,",
        "x1": 5.512440204620361,
        "x2": 6.743276596069336,
        "y1": 6.734413146972656,
        "y2": 2.530703067779541
      },
      {
        "r": 0,
        "text": "We make our code, models, and datasets publicly available.",
        "trunc_text": "We make our code, models, and datasets publicly available.",
        "x1": 7.669373035430908,
        "x2": 8.124272346496582,
        "y1": 7.029134273529053,
        "y2": 2.8138792514801025
      },
      {
        "r": 0,
        "text": "The source code will be released publicly.",
        "trunc_text": "The source code will be released publicly.",
        "x1": 8.367443084716797,
        "x2": 8.92794418334961,
        "y1": 7.298826217651367,
        "y2": 2.7621653079986572
      },
      {
        "r": 0,
        "text": "We evaluate PaTeCon on two large-scale datasets based on Wikidata and Freebase respectively.",
        "trunc_text": "We evaluate PaTeCon on two large-scale datasets based on Wikidata and Freebase respectively.",
        "x1": 5.848649501800537,
        "x2": 6.46644401550293,
        "y1": 7.241344928741455,
        "y2": 2.619783639907837
      },
      {
        "r": 0,
        "text": "We present a novel synthetic dataset, named Parcel3D, that is based on the Google Scanned Objects (GSO) dataset and consists of more than 13,000 images of parcels with full 3D annotations.",
        "trunc_text": "We present a novel synthetic dataset, named Parcel3D, that is based on the Google Scanned Objects (GSO) dataset and cons",
        "x1": 2.245861530303955,
        "x2": 3.9055657386779785,
        "y1": 8.186800003051758,
        "y2": 1.2961301803588867
      },
      {
        "r": 0,
        "text": "The dataset contains intact, i.e. cuboid-shaped, parcels and damaged parcels, which were generated in simulations.",
        "trunc_text": "The dataset contains intact, i.e. cuboid-shaped, parcels and damaged parcels, which were generated in simulations.",
        "x1": 2.394994020462036,
        "x2": 4.5165019035339355,
        "y1": 8.06102180480957,
        "y2": 1.5259897708892822
      },
      {
        "r": 0,
        "text": "Dataset and code are available at https://a-nau.github.io/parcel3d.",
        "trunc_text": "Dataset and code are available at https://a-nau.github.io/parcel3d.",
        "x1": 6.980631351470947,
        "x2": 7.716945648193359,
        "y1": 7.78065299987793,
        "y2": 1.9627163410186768
      },
      {
        "r": 0,
        "text": "Datasets, code and results are made publicly available and will be continuously updated at https://github.com/ZhaomingKong/Denoising-Comparison.",
        "trunc_text": "Datasets, code and results are made publicly available and will be continuously updated at https://github.com/ZhaomingKo",
        "x1": 6.994238376617432,
        "x2": 7.539292812347412,
        "y1": 7.662888050079346,
        "y2": 1.97026526927948
      },
      {
        "r": 0,
        "text": "Specifically, we collect a dataset of e-textbooks from one of the largest free online publishers in the world.",
        "trunc_text": "Specifically, we collect a dataset of e-textbooks from one of the largest free online publishers in the world.",
        "x1": 6.19596529006958,
        "x2": 6.939181804656982,
        "y1": 7.26078462600708,
        "y2": 2.4893481731414795
      },
      {
        "r": 0,
        "text": "The dataset and data analysis scripts are available on our open-source repository.",
        "trunc_text": "The dataset and data analysis scripts are available on our open-source repository.",
        "x1": 6.942111968994141,
        "x2": 7.3752288818359375,
        "y1": 7.57203483581543,
        "y2": 2.325784683227539
      },
      {
        "r": 0,
        "text": "To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset.",
        "trunc_text": "To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins In",
        "x1": 3.5349652767181396,
        "x2": 5.2652435302734375,
        "y1": 10.16253662109375,
        "y2": -0.07611680030822754
      },
      {
        "r": 0,
        "text": "Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks.",
        "trunc_text": "Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks.",
        "x1": 0.9480541348457336,
        "x2": 2.358071804046631,
        "y1": 7.388647079467773,
        "y2": 1.444571852684021
      },
      {
        "r": 0,
        "text": "The source code and the proposed dataset are publicly available at https://github.com/fh2019ustc/DocTr-Plus.",
        "trunc_text": "The source code and the proposed dataset are publicly available at https://github.com/fh2019ustc/DocTr-Plus.",
        "x1": 7.285224437713623,
        "x2": 7.595624923706055,
        "y1": 7.864743232727051,
        "y2": 1.9377703666687012
      },
      {
        "r": 0,
        "text": "Hence, we present a dataset that exclusively consists of healthy and diseased cashew leaves and fruits.",
        "trunc_text": "Hence, we present a dataset that exclusively consists of healthy and diseased cashew leaves and fruits.",
        "x1": 1.3949475288391113,
        "x2": 5.2494425773620605,
        "y1": 7.607626438140869,
        "y2": 1.6971136331558228
      },
      {
        "r": 0,
        "text": "The entire code and models will be open-sourced.",
        "trunc_text": "The entire code and models will be open-sourced.",
        "x1": 7.706039905548096,
        "x2": 8.172182083129883,
        "y1": 6.902459144592285,
        "y2": 2.8987462520599365
      },
      {
        "r": 0,
        "text": "With nationwide coverage, real-world network topology, and rich geospatial features, this data repository can be used for a variety of traffic-related tasks.",
        "trunc_text": "With nationwide coverage, real-world network topology, and rich geospatial features, this data repository can be used fo",
        "x1": 5.842466831207275,
        "x2": 6.971916198730469,
        "y1": 7.938621520996094,
        "y2": 1.9523282051086426
      },
      {
        "r": 0,
        "text": "The data and code are available on GitHub (https://github.com/baixianghuang/travel).",
        "trunc_text": "The data and code are available on GitHub (https://github.com/baixianghuang/travel).",
        "x1": 7.3571248054504395,
        "x2": 7.9946794509887695,
        "y1": 7.873905181884766,
        "y2": 2.0771684646606445
      },
      {
        "r": 0,
        "text": "Code and models will be accessed at https://github.com/Liuxinyv/SAZS.",
        "trunc_text": "Code and models will be accessed at https://github.com/Liuxinyv/SAZS.",
        "x1": 7.773935794830322,
        "x2": 8.249344825744629,
        "y1": 7.53121280670166,
        "y2": 2.3934760093688965
      },
      {
        "r": 0,
        "text": "BenchMD combines 19 publicly available datasets for 7 medical modalities, including 1D sensor data, 2D images, and 3D volumetric scans.",
        "trunc_text": "BenchMD combines 19 publicly available datasets for 7 medical modalities, including 1D sensor data, 2D images, and 3D vo",
        "x1": 3.0788512229919434,
        "x2": 4.0527238845825195,
        "y1": 7.26239013671875,
        "y2": 2.2344777584075928
      },
      {
        "r": 0,
        "text": "We introduce the LongForm dataset, which is created by leveraging English corpus examples with augmented instructions.",
        "trunc_text": "We introduce the LongForm dataset, which is created by leveraging English corpus examples with augmented instructions.",
        "x1": 3.524252414703369,
        "x2": 2.6721794605255127,
        "y1": 3.247579336166382,
        "y2": 6.019062042236328
      },
      {
        "r": 0,
        "text": "We publicly release our data and models: https://github.com/akoksal/LongForm.",
        "trunc_text": "We publicly release our data and models: https://github.com/akoksal/LongForm.",
        "x1": 7.379469871520996,
        "x2": 7.855170249938965,
        "y1": 7.137021064758301,
        "y2": 2.697843551635742
      },
      {
        "r": 0,
        "text": "Code is generated using a commercial tool, SonarCloud.",
        "trunc_text": "Code is generated using a commercial tool, SonarCloud.",
        "x1": 7.854780197143555,
        "x2": 8.697132110595703,
        "y1": 7.563479423522949,
        "y2": 2.4688947200775146
      },
      {
        "r": 0,
        "text": "Our source code will be available at https://github.com/MC-E/DragonDiffusion.",
        "trunc_text": "Our source code will be available at https://github.com/MC-E/DragonDiffusion.",
        "x1": 8.161700248718262,
        "x2": 8.760469436645508,
        "y1": 7.6712212562561035,
        "y2": 2.1973280906677246
      },
      {
        "r": 0,
        "text": "An open-source implementation is available online.",
        "trunc_text": "An open-source implementation is available online.",
        "x1": 7.676819801330566,
        "x2": 8.014619827270508,
        "y1": 6.651162624359131,
        "y2": 2.861807107925415
      },
      {
        "r": 0,
        "text": "Deep-learning-based object detection and semantic segmentation models have proven to be suitable for this purpose.",
        "trunc_text": "Deep-learning-based object detection and semantic segmentation models have proven to be suitable for this purpose.",
        "x1": 0.7762247323989868,
        "x2": 2.245452404022217,
        "y1": 7.493436813354492,
        "y2": 1.3655861616134644
      },
      {
        "r": 0,
        "text": "We evaluate the effectiveness of this approach by training a semantic segmentation model on a real dataset augmented in two ways: 1) using synthetic images obtained from real masks, and 2) generating images from synthetic semantic masks.",
        "trunc_text": "We evaluate the effectiveness of this approach by training a semantic segmentation model on a real dataset augmented in ",
        "x1": 1.3227283954620361,
        "x2": 2.7328288555145264,
        "y1": 7.368712425231934,
        "y2": 1.4119127988815308
      },
      {
        "r": 0,
        "text": "The dataset spans seven sub-topics and is annotated with a materials-science focused multi-label annotation scheme for AZ.",
        "trunc_text": "The dataset spans seven sub-topics and is annotated with a materials-science focused multi-label annotation scheme for A",
        "x1": 2.5350987911224365,
        "x2": 1.5510751008987427,
        "y1": 3.4850144386291504,
        "y2": 5.475593090057373
      },
      {
        "r": 0,
        "text": "The code is available at \\url{https://github.com/cjw2021/SOV-STG}.",
        "trunc_text": "The code is available at \\url{https://github.com/cjw2021/SOV-STG}.",
        "x1": 8.127809524536133,
        "x2": 8.712641716003418,
        "y1": 7.739593029022217,
        "y2": 2.2260329723358154
      },
      {
        "r": 0,
        "text": "The code and pretrained models will be released under https://github.com/lik1996/iCMFormer.",
        "trunc_text": "The code and pretrained models will be released under https://github.com/lik1996/iCMFormer.",
        "x1": 7.662170886993408,
        "x2": 8.250557899475098,
        "y1": 7.32932710647583,
        "y2": 2.6137073040008545
      },
      {
        "r": 0,
        "text": "The data samples are automatically generated from a curated set of reasoning patterns, where the patterns are annotated with inference labels by experts.",
        "trunc_text": "The data samples are automatically generated from a curated set of reasoning patterns, where the patterns are annotated ",
        "x1": 5.4612531661987305,
        "x2": 6.081462383270264,
        "y1": 7.132914066314697,
        "y2": 2.4654269218444824
      },
      {
        "r": 0,
        "text": "The data is obtained from a fleet of gas sensors that measure and track quantities such as oxygen and sound.",
        "trunc_text": "The data is obtained from a fleet of gas sensors that measure and track quantities such as oxygen and sound.",
        "x1": 5.883201599121094,
        "x2": 6.4901041984558105,
        "y1": 7.894881725311279,
        "y2": 1.8464062213897705
      },
      {
        "r": 0,
        "text": "With the help of this data, we can detect events such as occupancy in a specific environment.",
        "trunc_text": "With the help of this data, we can detect events such as occupancy in a specific environment.",
        "x1": 3.5567069053649902,
        "x2": 4.901793003082275,
        "y1": 9.49185848236084,
        "y2": 0.25082460045814514
      },
      {
        "r": 0,
        "text": "Extensive experiments and visualizations on four datasets demonstrate the powerful performance of our method.",
        "trunc_text": "Extensive experiments and visualizations on four datasets demonstrate the powerful performance of our method.",
        "x1": 4.113504409790039,
        "x2": 5.201318740844727,
        "y1": 7.498814105987549,
        "y2": 2.618760347366333
      },
      {
        "r": 0,
        "text": "Codes will be available.",
        "trunc_text": "Codes will be available.",
        "x1": 8.34410572052002,
        "x2": 8.930562973022461,
        "y1": 7.3851423263549805,
        "y2": 2.69278883934021
      },
      {
        "r": 0,
        "text": "Extensive evaluation on three benchmark datasets using multiple evaluation metrics show the effectiveness of the proposed framework.",
        "trunc_text": "Extensive evaluation on three benchmark datasets using multiple evaluation metrics show the effectiveness of the propose",
        "x1": 4.279353618621826,
        "x2": 4.560567378997803,
        "y1": 6.634278297424316,
        "y2": 3.2858388423919678
      },
      {
        "r": 0,
        "text": "Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name.",
        "trunc_text": "Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vu",
        "x1": 7.762088298797607,
        "x2": 6.758622169494629,
        "y1": 5.167495250701904,
        "y2": 6.4468302726745605
      },
      {
        "r": 0,
        "text": "This property of the dataset makes it suitable for evaluating the effectiveness of various static and dynamic analysis tools.",
        "trunc_text": "This property of the dataset makes it suitable for evaluating the effectiveness of various static and dynamic analysis t",
        "x1": 5.1429123878479,
        "x2": 5.87061071395874,
        "y1": 7.0550384521484375,
        "y2": 2.799837589263916
      },
      {
        "r": 0,
        "text": "While prior research demonstrated the high performance of ChatGPT across numerous NLP tasks, open-source LLMs like HugginChat and FLAN are gaining attention for their cost-effectiveness, transparency, reproducibility, and superior data protection.",
        "trunc_text": "While prior research demonstrated the high performance of ChatGPT across numerous NLP tasks, open-source LLMs like Huggi",
        "x1": 6.472334384918213,
        "x2": 4.5928802490234375,
        "y1": 3.667755603790283,
        "y2": 6.837728977203369
      },
      {
        "r": 0,
        "text": "With extensive experiments on a large-scale real-world dataset, we demonstrate the substantial effectiveness of our approach.",
        "trunc_text": "With extensive experiments on a large-scale real-world dataset, we demonstrate the substantial effectiveness of our appr",
        "x1": 4.302548885345459,
        "x2": 5.105864524841309,
        "y1": 7.374427318572998,
        "y2": 2.6051337718963623
      },
      {
        "r": 0,
        "text": "In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-date model repositories for model querying, legal compliance analysis between different model licenses, and copyright issues and intellectual property protection in model reusing.",
        "trunc_text": "In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model",
        "x1": 7.728676795959473,
        "x2": 8.082642555236816,
        "y1": 6.892423152923584,
        "y2": 2.95416259765625
      },
      {
        "r": 0,
        "text": "However, little is known about how much training data they require, and how this number depends on the data structure.",
        "trunc_text": "However, little is known about how much training data they require, and how this number depends on the data structure.",
        "x1": 4.140908241271973,
        "x2": 5.5334672927856445,
        "y1": 6.0788750648498535,
        "y2": 3.2675607204437256
      },
      {
        "r": 0,
        "text": "Our code is available at https://github.com/siyi-wind/MDViT.",
        "trunc_text": "Our code is available at https://github.com/siyi-wind/MDViT.",
        "x1": 8.121155738830566,
        "x2": 8.71181869506836,
        "y1": 7.703168869018555,
        "y2": 2.2041921615600586
      },
      {
        "r": 0,
        "text": "Along with the datasets, we also propose corresponding baseline solutions to the three aforementioned tasks.",
        "trunc_text": "Along with the datasets, we also propose corresponding baseline solutions to the three aforementioned tasks.",
        "x1": 4.046358108520508,
        "x2": 5.196341037750244,
        "y1": 6.843434810638428,
        "y2": 3.143982410430908
      },
      {
        "r": 0,
        "text": "These attacks are launched on three powerful pre-trained image classifier architectures, ResNet-34, GoogleNet, and DenseNet-161.",
        "trunc_text": "These attacks are launched on three powerful pre-trained image classifier architectures, ResNet-34, GoogleNet, and Dense",
        "x1": 7.324338912963867,
        "x2": 6.609982967376709,
        "y1": 5.519863605499268,
        "y2": 5.879425048828125
      },
      {
        "r": 0,
        "text": "This dataset comprises a large number of tasks that demand problem-solving skills.",
        "trunc_text": "This dataset comprises a large number of tasks that demand problem-solving skills.",
        "x1": 5.166937351226807,
        "x2": 5.581298828125,
        "y1": 6.7238874435424805,
        "y2": 2.9688844680786133
      },
      {
        "r": 0,
        "text": "The implementation of CAME is publicly available.",
        "trunc_text": "The implementation of CAME is publicly available.",
        "x1": 8.372580528259277,
        "x2": 8.887188911437988,
        "y1": 7.282079696655273,
        "y2": 2.7429261207580566
      },
      {
        "r": 0,
        "text": "However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions.",
        "trunc_text": "However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these repre",
        "x1": 4.212916851043701,
        "x2": 4.859275817871094,
        "y1": 7.38393497467041,
        "y2": 2.6178581714630127
      },
      {
        "r": 0,
        "text": "Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios.",
        "trunc_text": "Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios.",
        "x1": 2.22796630859375,
        "x2": 3.745718479156494,
        "y1": 7.379139423370361,
        "y2": 1.8238861560821533
      },
      {
        "r": 0,
        "text": "In this paper, we introduce ScalOTA, an end-to-end scalable OTA software update architecture and secure protocol for modern vehicles.",
        "trunc_text": "In this paper, we introduce ScalOTA, an end-to-end scalable OTA software update architecture and secure protocol for mod",
        "x1": 6.700509071350098,
        "x2": 7.616200923919678,
        "y1": 5.553323745727539,
        "y2": 2.2730090618133545
      },
      {
        "r": 0,
        "text": "Our code is available at https://github.com/yuping-wu/PULSAR.",
        "trunc_text": "Our code is available at https://github.com/yuping-wu/PULSAR.",
        "x1": 8.238435745239258,
        "x2": 8.82202434539795,
        "y1": 7.622326850891113,
        "y2": 2.283738374710083
      },
      {
        "r": 0,
        "text": "On both \\pascal and \\coco datasets, we conduct extensive experiments to evaluate the framework effectiveness.",
        "trunc_text": "On both \\pascal and \\coco datasets, we conduct extensive experiments to evaluate the framework effectiveness.",
        "x1": 4.167985439300537,
        "x2": 4.648274898529053,
        "y1": 6.888266563415527,
        "y2": 3.048438549041748
      },
      {
        "r": 0,
        "text": "Our source code and the related paper list are available on https://github.com/SLDGroup/survey-zero-shot-nas.",
        "trunc_text": "Our source code and the related paper list are available on https://github.com/SLDGroup/survey-zero-shot-nas.",
        "x1": 7.356242656707764,
        "x2": 7.801877021789551,
        "y1": 7.808840274810791,
        "y2": 1.9318852424621582
      },
      {
        "r": 0,
        "text": "Extensive experiments show our method achieves state-of-the-art results on the HMDB51 and UCF101 datasets and a competitive result on the benchmark of Kinetics and something-2-something V2 datasets.",
        "trunc_text": "Extensive experiments show our method achieves state-of-the-art results on the HMDB51 and UCF101 datasets and a competit",
        "x1": 4.355910301208496,
        "x2": 4.345394611358643,
        "y1": 6.660106658935547,
        "y2": 3.206418752670288
      },
      {
        "r": 0,
        "text": "Experimental results on real-world datasets demonstrate that our method achieves better performance compared with several state-of-the-art social bot detection methods.",
        "trunc_text": "Experimental results on real-world datasets demonstrate that our method achieves better performance compared with severa",
        "x1": 7.5918450355529785,
        "x2": 6.256152153015137,
        "y1": 3.462631940841675,
        "y2": 7.673814296722412
      },
      {
        "r": 0,
        "text": "However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to achieve real-time full-body capture while being accurate in world space.",
        "trunc_text": "However, due to the challenges in data collection and network designs, it remains challenging for existing solutions to ",
        "x1": 2.5229225158691406,
        "x2": 3.9061598777770996,
        "y1": 9.479275703430176,
        "y2": -0.3906395435333252
      },
      {
        "r": 0,
        "text": "In this work, we contribute a sequential proxy-to-motion learning scheme together with a proxy dataset of 2D skeleton sequences and 3D rotational motions in world space.",
        "trunc_text": "In this work, we contribute a sequential proxy-to-motion learning scheme together with a proxy dataset of 2D skeleton se",
        "x1": 2.594743251800537,
        "x2": 4.1132588386535645,
        "y1": 9.435053825378418,
        "y2": -0.5641077160835266
      },
      {
        "r": 0,
        "text": "More video results can be found at our project page: https://liuyebin.com/proxycap.",
        "trunc_text": "More video results can be found at our project page: https://liuyebin.com/proxycap.",
        "x1": 7.567230701446533,
        "x2": 8.44962215423584,
        "y1": 7.412967205047607,
        "y2": 2.08808970451355
      },
      {
        "r": 0,
        "text": "In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world.",
        "trunc_text": "In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player",
        "x1": 5.712010860443115,
        "x2": 3.7602779865264893,
        "y1": 3.2319629192352295,
        "y2": 7.000922679901123
      },
      {
        "r": 0,
        "text": "We address this issue by introducing a new dataset: GHOSTS.",
        "trunc_text": "We address this issue by introducing a new dataset: GHOSTS.",
        "x1": 5.396972179412842,
        "x2": 5.9950852394104,
        "y1": 7.535031795501709,
        "y2": 2.4088120460510254
      },
      {
        "r": 0,
        "text": "This paper also contributes a new surveillance dataset called NightSuR.",
        "trunc_text": "This paper also contributes a new surveillance dataset called NightSuR.",
        "x1": 1.7978509664535522,
        "x2": 4.475894451141357,
        "y1": 10.180980682373047,
        "y2": 0.607839047908783
      },
      {
        "r": 0,
        "text": "We present in this work a new Universal Morphology dataset for Korean.",
        "trunc_text": "We present in this work a new Universal Morphology dataset for Korean.",
        "x1": 4.2062602043151855,
        "x2": 2.3530337810516357,
        "y1": 2.7019290924072266,
        "y2": 6.420634746551514
      },
      {
        "r": 0,
        "text": "The PIKS technique is scalable and can readily ingest new datasets.",
        "trunc_text": "The PIKS technique is scalable and can readily ingest new datasets.",
        "x1": 5.370797157287598,
        "x2": 6.046935558319092,
        "y1": 7.164518356323242,
        "y2": 2.76698637008667
      },
      {
        "r": 0,
        "text": "Datasets that reflect the diverse array of multimodal, multilingual news sources available online could be used to teach models to benefit from this shift, but existing news video datasets focus on traditional news broadcasts produced for English-speaking audiences.",
        "trunc_text": "Datasets that reflect the diverse array of multimodal, multilingual news sources available online could be used to teach",
        "x1": 3.8390042781829834,
        "x2": 2.512192487716675,
        "y1": 2.938807487487793,
        "y2": 5.3504438400268555
      },
      {
        "r": 0,
        "text": "The MIND dataset is at the moment of writing the most extensive dataset available for the research and development of news recommender systems.",
        "trunc_text": "The MIND dataset is at the moment of writing the most extensive dataset available for the research and development of ne",
        "x1": 5.259698867797852,
        "x2": 5.963101863861084,
        "y1": 7.321413040161133,
        "y2": 2.445171594619751
      },
      {
        "r": 0,
        "text": "We also introduce a new dataset generated by our classifier that tracks the dynamics of fake news in the Chinese language during the early pandemic.",
        "trunc_text": "We also introduce a new dataset generated by our classifier that tracks the dynamics of fake news in the Chinese languag",
        "x1": 3.7420928478240967,
        "x2": 2.9462907314300537,
        "y1": 3.255387306213379,
        "y2": 5.7447638511657715
      },
      {
        "r": 0,
        "text": "Our code and data will be released shortly.",
        "trunc_text": "Our code and data will be released shortly.",
        "x1": 6.947945594787598,
        "x2": 7.564903736114502,
        "y1": 7.311914443969727,
        "y2": 2.6029183864593506
      },
      {
        "r": 0,
        "text": "This paper introduces a novel dataset",
        "trunc_text": "This paper introduces a novel dataset",
        "x1": 5.045401573181152,
        "x2": 5.7494306564331055,
        "y1": 7.401646614074707,
        "y2": 2.4133524894714355
      },
      {
        "r": 0,
        "text": "We present a unique collection of data",
        "trunc_text": "We present a unique collection of data",
        "x1": 5.507995128631592,
        "x2": 6.087723731994629,
        "y1": 7.377171039581299,
        "y2": 2.4898531436920166
      },
      {
        "r": 0,
        "text": "In this study, we unveil our newly created dataset",
        "trunc_text": "In this study, we unveil our newly created dataset",
        "x1": 5.478003978729248,
        "x2": 6.119111061096191,
        "y1": 7.454682350158691,
        "y2": 2.3860785961151123
      },
      {
        "r": 0,
        "text": "This article brings to light a freshly curated dataset",
        "trunc_text": "This article brings to light a freshly curated dataset",
        "x1": 5.191808223724365,
        "x2": 5.741751670837402,
        "y1": 7.237392425537109,
        "y2": 2.3538568019866943
      },
      {
        "r": 0,
        "text": "We reveal an unprecedented dataset in the field of",
        "trunc_text": "We reveal an unprecedented dataset in the field of",
        "x1": 5.214910507202148,
        "x2": 5.925105571746826,
        "y1": 7.46040153503418,
        "y2": 2.4024875164031982
      },
      {
        "r": 0,
        "text": "In this work, we propose a pioneering dataset",
        "trunc_text": "In this work, we propose a pioneering dataset",
        "x1": 5.1040358543396,
        "x2": 5.832625389099121,
        "y1": 7.472607612609863,
        "y2": 2.282783269882202
      },
      {
        "r": 0,
        "text": "This research details the compilation of a new dataset",
        "trunc_text": "This research details the compilation of a new dataset",
        "x1": 5.426214694976807,
        "x2": 6.024391174316406,
        "y1": 7.3255133628845215,
        "y2": 2.395869255065918
      },
      {
        "r": 0,
        "text": "Our article describes a groundbreaking data collection",
        "trunc_text": "Our article describes a groundbreaking data collection",
        "x1": 5.730013370513916,
        "x2": 6.410392761230469,
        "y1": 7.299600601196289,
        "y2": 2.651210069656372
      },
      {
        "r": 0,
        "text": "We are pleased to introduce an innovative dataset",
        "trunc_text": "We are pleased to introduce an innovative dataset",
        "x1": 5.336604118347168,
        "x2": 6.101102828979492,
        "y1": 7.432061672210693,
        "y2": 2.53885817527771
      },
      {
        "r": 0,
        "text": "This publication centers around our newly assembled dataset",
        "trunc_text": "This publication centers around our newly assembled dataset",
        "x1": 5.76644229888916,
        "x2": 6.497964382171631,
        "y1": 7.376084804534912,
        "y2": 2.537358283996582
      },
      {
        "r": 0,
        "text": "Herein, we unfold a freshly minted dataset",
        "trunc_text": "Herein, we unfold a freshly minted dataset",
        "x1": 5.7389607429504395,
        "x2": 6.645648002624512,
        "y1": 7.3719587326049805,
        "y2": 2.1875264644622803
      },
      {
        "r": 0,
        "text": "Our research entails the design and creation of a new dataset",
        "trunc_text": "Our research entails the design and creation of a new dataset",
        "x1": 5.417314529418945,
        "x2": 6.097527980804443,
        "y1": 7.401745319366455,
        "y2": 2.436795473098755
      },
      {
        "r": 0,
        "text": "The crux of this paper is the disclosure of a new dataset",
        "trunc_text": "The crux of this paper is the disclosure of a new dataset",
        "x1": 5.281342506408691,
        "x2": 6.036227226257324,
        "y1": 7.562497615814209,
        "y2": 2.3086934089660645
      },
      {
        "r": 0,
        "text": "This manuscript details the formulation of a novel data assembly",
        "trunc_text": "This manuscript details the formulation of a novel data assembly",
        "x1": 6.2071332931518555,
        "x2": 6.828094005584717,
        "y1": 6.825711250305176,
        "y2": 3.1064586639404297
      },
      {
        "r": 0,
        "text": "We exhibit an inventive dataset for the first time in this paper",
        "trunc_text": "We exhibit an inventive dataset for the first time in this paper",
        "x1": 5.377734184265137,
        "x2": 5.963382244110107,
        "y1": 7.407097339630127,
        "y2": 2.4163355827331543
      },
      {
        "r": 0,
        "text": "We present an exploratory dataset in this research",
        "trunc_text": "We present an exploratory dataset in this research",
        "x1": 5.168159484863281,
        "x2": 5.833019256591797,
        "y1": 7.419042587280273,
        "y2": 2.3945536613464355
      },
      {
        "r": 0,
        "text": "This paper features the unveiling of a hitherto unseen dataset",
        "trunc_text": "This paper features the unveiling of a hitherto unseen dataset",
        "x1": 5.214608669281006,
        "x2": 5.813722133636475,
        "y1": 7.480546474456787,
        "y2": 2.480104446411133
      },
      {
        "r": 0,
        "text": "In the present study, we showcase an original data compilation",
        "trunc_text": "In the present study, we showcase an original data compilation",
        "x1": 5.890713214874268,
        "x2": 6.558412551879883,
        "y1": 7.086965084075928,
        "y2": 2.8604319095611572
      },
      {
        "r": 0,
        "text": "Our work represents the first public exhibition of a unique dataset",
        "trunc_text": "Our work represents the first public exhibition of a unique dataset",
        "x1": 5.428534984588623,
        "x2": 6.041887283325195,
        "y1": 7.489114284515381,
        "y2": 2.4535927772521973
      },
      {
        "r": 0,
        "text": "We draw back the curtains on a state-of-the-art dataset in this research",
        "trunc_text": "We draw back the curtains on a state-of-the-art dataset in this research",
        "x1": 5.126133918762207,
        "x2": 5.790637969970703,
        "y1": 7.391844749450684,
        "y2": 2.4538886547088623
      },
      {
        "r": 0,
        "text": "We will release our dataset and code for future research.",
        "trunc_text": "We will release our dataset and code for future research.",
        "x1": 6.681474208831787,
        "x2": 7.273600101470947,
        "y1": 7.374755859375,
        "y2": 2.488785982131958
      },
      {
        "r": 0,
        "text": "The real-world datasets will be released.",
        "trunc_text": "The real-world datasets will be released.",
        "x1": 6.514266014099121,
        "x2": 7.0958333015441895,
        "y1": 7.554195880889893,
        "y2": 2.17322039604187
      },
      {
        "r": 0,
        "text": "In an effort to accelerate this research and assist others, we are releasing our dataset and model to the research community.",
        "trunc_text": "In an effort to accelerate this research and assist others, we are releasing our dataset and model to the research commu",
        "x1": 5.951524257659912,
        "x2": 6.8302693367004395,
        "y1": 7.443131923675537,
        "y2": 2.5043673515319824
      },
      {
        "r": 0,
        "text": "To promote progress towards this goal, we release OBJECT: a dataset consisting of 400K editing examples created from procedurally generated 3D scenes.",
        "trunc_text": "To promote progress towards this goal, we release OBJECT: a dataset consisting of 400K editing examples created from pro",
        "x1": 2.1367316246032715,
        "x2": 3.7139954566955566,
        "y1": 8.514019966125488,
        "y2": 1.012569546699524
      },
      {
        "r": 0,
        "text": "This article presents DataXploreFines, an innovative Shiny application that revolutionizes data exploration, analysis, and visualization.",
        "trunc_text": "This article presents DataXploreFines, an innovative Shiny application that revolutionizes data exploration, analysis, a",
        "x1": 4.391742706298828,
        "x2": 6.0085930824279785,
        "y1": 8.201557159423828,
        "y2": 1.8850367069244385
      },
      {
        "r": 0,
        "text": "Users can upload their datasets in popular formats like CSV or Excel, explore the data structure, perform manipulations, and obtain statistical summaries.",
        "trunc_text": "Users can upload their datasets in popular formats like CSV or Excel, explore the data structure, perform manipulations,",
        "x1": 6.04326057434082,
        "x2": 6.747027397155762,
        "y1": 7.133281230926514,
        "y2": 2.7088561058044434
      },
      {
        "r": 0,
        "text": "DataXploreFines provides a wide range of interactive visualizations, including histograms, scatter plots, bar charts, and line graphs, enabling users to identify patterns and trends.",
        "trunc_text": "DataXploreFines provides a wide range of interactive visualizations, including histograms, scatter plots, bar charts, an",
        "x1": 4.187408924102783,
        "x2": 5.920959949493408,
        "y1": 8.266349792480469,
        "y2": 1.6569832563400269
      },
      {
        "r": 0,
        "text": "Experimental results demonstrate that our proposed approach outperforms the state-of-the-art methods on fine-grained datasets under real-world scenarios.",
        "trunc_text": "Experimental results demonstrate that our proposed approach outperforms the state-of-the-art methods on fine-grained dat",
        "x1": 4.203674793243408,
        "x2": 4.924807071685791,
        "y1": 7.427290916442871,
        "y2": 2.636113405227661
      },
      {
        "r": 0,
        "text": "We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs.",
        "trunc_text": "We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequen",
        "x1": 3.4999494552612305,
        "x2": 4.486645221710205,
        "y1": 7.2675299644470215,
        "y2": 2.4087321758270264
      },
      {
        "r": 0,
        "text": "Substantial progress has been made recently in motion data collection technologies and generation methods, laying the foundation for increasing interest in human motion generation.",
        "trunc_text": "Substantial progress has been made recently in motion data collection technologies and generation methods, laying the fo",
        "x1": 2.66831374168396,
        "x2": 4.145431995391846,
        "y1": 9.53309440612793,
        "y2": -0.561042070388794
      },
      {
        "r": 0,
        "text": "Additionally, we provide an overview of common datasets and evaluation metrics.",
        "trunc_text": "Additionally, we provide an overview of common datasets and evaluation metrics.",
        "x1": 4.860942840576172,
        "x2": 5.503543853759766,
        "y1": 6.481785774230957,
        "y2": 3.290534257888794
      },
      {
        "r": 0,
        "text": "For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED).",
        "trunc_text": "For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED).",
        "x1": 4.865118980407715,
        "x2": 2.976797342300415,
        "y1": 2.5886449813842773,
        "y2": 6.953747749328613
      },
      {
        "r": 0,
        "text": "For English, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets.",
        "trunc_text": "For English, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets.",
        "x1": 4.17156982421875,
        "x2": 2.2604222297668457,
        "y1": 2.5190327167510986,
        "y2": 6.5877580642700195
      },
      {
        "r": 0,
        "text": "Our dataset is available at https://github.com/iamazxl/OAVQA.",
        "trunc_text": "Our dataset is available at https://github.com/iamazxl/OAVQA.",
        "x1": 7.147575855255127,
        "x2": 7.653591156005859,
        "y1": 7.926761150360107,
        "y2": 1.8419182300567627
      },
      {
        "r": 0,
        "text": "Concretely, we first summarize the widely-used ST ocean datasets and identify their unique characteristics.",
        "trunc_text": "Concretely, we first summarize the widely-used ST ocean datasets and identify their unique characteristics.",
        "x1": -0.273375928401947,
        "x2": 1.7404465675354004,
        "y1": 9.244539260864258,
        "y2": -0.39305028319358826
      },
      {
        "r": 0,
        "text": "In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples.",
        "trunc_text": "In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different sample",
        "x1": 4.990431785583496,
        "x2": 5.180532932281494,
        "y1": 7.00774621963501,
        "y2": 2.358400344848633
      },
      {
        "r": 0,
        "text": "To address this, we propose a large-scale SlowTV dataset curated from YouTube, containing an order of magnitude more data than existing automotive datasets.",
        "trunc_text": "To address this, we propose a large-scale SlowTV dataset curated from YouTube, containing an order of magnitude more dat",
        "x1": 2.623481273651123,
        "x2": 4.229953765869141,
        "y1": 8.454837799072266,
        "y2": 0.7658231854438782
      },
      {
        "r": 0,
        "text": "13k sentence pairs) and a web-domain corpus (approx.",
        "trunc_text": "13k sentence pairs) and a web-domain corpus (approx.",
        "x1": 3.498023271560669,
        "x2": 2.599245071411133,
        "y1": 3.302443265914917,
        "y2": 5.968135833740234
      },
      {
        "r": 0,
        "text": "We release the resulting corpus and our analysis pipeline for future research.",
        "trunc_text": "We release the resulting corpus and our analysis pipeline for future research.",
        "x1": 2.9407670497894287,
        "x2": 1.7772372961044312,
        "y1": 3.3524980545043945,
        "y2": 5.670838832855225
      },
      {
        "r": 0,
        "text": "The corpora have been annotated with dependency trees, lemmas, and part-of-speech tags.",
        "trunc_text": "The corpora have been annotated with dependency trees, lemmas, and part-of-speech tags.",
        "x1": 2.9329726696014404,
        "x2": 1.7351022958755493,
        "y1": 3.36122989654541,
        "y2": 5.675169944763184
      },
      {
        "r": 0,
        "text": "Therefore, we propose that LLM-assisted annotation is a promising automated approach for corpus studies.",
        "trunc_text": "Therefore, we propose that LLM-assisted annotation is a promising automated approach for corpus studies.",
        "x1": 2.8182809352874756,
        "x2": 1.6328015327453613,
        "y1": 3.3455164432525635,
        "y2": 5.606717586517334
      },
      {
        "r": 0,
        "text": "In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information.",
        "trunc_text": "In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotatio",
        "x1": 2.965529441833496,
        "x2": 1.7635200023651123,
        "y1": 3.289217710494995,
        "y2": 5.7573161125183105
      },
      {
        "r": 0,
        "text": "This article proposes a new training model to solve this problem through NLP processing methods.",
        "trunc_text": "This article proposes a new training model to solve this problem through NLP processing methods.",
        "x1": 3.425737142562866,
        "x2": 3.0321617126464844,
        "y1": 3.6894702911376953,
        "y2": 5.621866703033447
      },
      {
        "r": 0,
        "text": "We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus.",
        "trunc_text": "We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus.",
        "x1": 3.5273499488830566,
        "x2": 2.5858612060546875,
        "y1": 3.277770519256592,
        "y2": 6.009217262268066
      },
      {
        "r": 0,
        "text": "Starting with an initial text corpus, our framework employs a large language model to select multiple sentences that describe the same scene from various perspectives.",
        "trunc_text": "Starting with an initial text corpus, our framework employs a large language model to select multiple sentences that des",
        "x1": 3.661470413208008,
        "x2": 2.6102676391601562,
        "y1": 3.2367377281188965,
        "y2": 5.887128829956055
      },
      {
        "r": 0,
        "text": "Experimental results on multiple benchmark datasets demonstrate the effectiveness of our method.",
        "trunc_text": "Experimental results on multiple benchmark datasets demonstrate the effectiveness of our method.",
        "x1": 4.163438320159912,
        "x2": 4.516870975494385,
        "y1": 6.708120346069336,
        "y2": 3.236948013305664
      },
      {
        "r": 0,
        "text": "Our proposed methods achieve state-of-the-art results on three popular benchmark datasets, and the source code will be made publicly available shortly.",
        "trunc_text": "Our proposed methods achieve state-of-the-art results on three popular benchmark datasets, and the source code will be m",
        "x1": 4.381272792816162,
        "x2": 4.415514945983887,
        "y1": 6.747063159942627,
        "y2": 3.084904193878174
      },
      {
        "r": 0,
        "text": "We perform an exhaustive evaluation in two benchmark datasets.",
        "trunc_text": "We perform an exhaustive evaluation in two benchmark datasets.",
        "x1": 4.241884231567383,
        "x2": 4.4041900634765625,
        "y1": 6.570939540863037,
        "y2": 3.2348194122314453
      },
      {
        "r": 0,
        "text": "We conduct experiments on two benchmark datasets.",
        "trunc_text": "We conduct experiments on two benchmark datasets.",
        "x1": 4.330323219299316,
        "x2": 4.548501968383789,
        "y1": 6.560351371765137,
        "y2": 3.308119535446167
      },
      {
        "r": 0,
        "text": "Extensive experiments conducted on two benchmark datasets show that our approach achieves excellent performance compared to its competitors.",
        "trunc_text": "Extensive experiments conducted on two benchmark datasets show that our approach achieves excellent performance compared",
        "x1": 4.253758907318115,
        "x2": 4.533927917480469,
        "y1": 6.559473037719727,
        "y2": 3.3435378074645996
      },
      {
        "r": 0,
        "text": "The proposed model is validated through extensive experiments on two benchmark datasets, showcasing superior performance compared to existing methods.",
        "trunc_text": "The proposed model is validated through extensive experiments on two benchmark datasets, showcasing superior performance",
        "x1": 4.180073261260986,
        "x2": 4.572267055511475,
        "y1": 6.6405487060546875,
        "y2": 3.255183219909668
      },
      {
        "r": 0,
        "text": "With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice.",
        "trunc_text": "With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further resea",
        "x1": 4.002213478088379,
        "x2": 4.522052764892578,
        "y1": 6.897326946258545,
        "y2": 3.125199556350708
      },
      {
        "r": 0,
        "text": "Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared with baselines.",
        "trunc_text": "Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared wi",
        "x1": 3.9832875728607178,
        "x2": 4.681384563446045,
        "y1": 6.536732196807861,
        "y2": 3.4725635051727295
      },
      {
        "r": 0,
        "text": "We validate our scheme with some of the most popular benchmarking datasets.",
        "trunc_text": "We validate our scheme with some of the most popular benchmarking datasets.",
        "x1": 4.563152313232422,
        "x2": 4.540058612823486,
        "y1": 6.5097432136535645,
        "y2": 3.289883613586426
      },
      {
        "r": 0,
        "text": "Method: We present an evaluation of five open-source and four proprietary tools against a benchmark dataset.",
        "trunc_text": "Method: We present an evaluation of five open-source and four proprietary tools against a benchmark dataset.",
        "x1": 4.444023609161377,
        "x2": 4.454594135284424,
        "y1": 6.795546531677246,
        "y2": 3.223928213119507
      },
      {
        "r": 0,
        "text": "To testify the effectiveness and superiority of the proposed approach, we conduct extensive experiments on benchmark datasets.",
        "trunc_text": "To testify the effectiveness and superiority of the proposed approach, we conduct extensive experiments on benchmark dat",
        "x1": 4.213659763336182,
        "x2": 4.516419887542725,
        "y1": 6.6767754554748535,
        "y2": 3.370434522628784
      },
      {
        "r": 0,
        "text": "We also introduce a new dataset for benchmarking, and the evaluations are performed from four different perspectives including quantitative metrics, visual effects, human ratings and computational cost.",
        "trunc_text": "We also introduce a new dataset for benchmarking, and the evaluations are performed from four different perspectives inc",
        "x1": 3.9960720539093018,
        "x2": 4.2675042152404785,
        "y1": 6.539825439453125,
        "y2": 3.231468677520752
      },
      {
        "r": 0,
        "text": "Our method is simple yet it outperforms several state-of-the-art methods on six popular dataset benchmarks.",
        "trunc_text": "Our method is simple yet it outperforms several state-of-the-art methods on six popular dataset benchmarks.",
        "x1": 4.2633562088012695,
        "x2": 4.521676063537598,
        "y1": 6.845212936401367,
        "y2": 3.093514919281006
      },
      {
        "r": 0,
        "text": "Finally, we conduct extensive experiments on widely-used benchmark datasets to validate the superiority of our method by comparing it with existing state-of-the-art methods.",
        "trunc_text": "Finally, we conduct extensive experiments on widely-used benchmark datasets to validate the superiority of our method by",
        "x1": 4.151961326599121,
        "x2": 4.567933559417725,
        "y1": 6.651869773864746,
        "y2": 3.1676599979400635
      },
      {
        "r": 0,
        "text": "We validate the model both qualitatively and quantitatively on four benchmark datasets.",
        "trunc_text": "We validate the model both qualitatively and quantitatively on four benchmark datasets.",
        "x1": 4.127562999725342,
        "x2": 4.6883111000061035,
        "y1": 6.451813697814941,
        "y2": 3.3922665119171143
      },
      {
        "r": 0,
        "text": "Finally, I provide the first high-quality benchmark dataset in order to fairly compare existing models and aid future model developers.",
        "trunc_text": "Finally, I provide the first high-quality benchmark dataset in order to fairly compare existing models and aid future mo",
        "x1": 4.620686054229736,
        "x2": 4.428889751434326,
        "y1": 6.600205898284912,
        "y2": 3.2546122074127197
      },
      {
        "r": 0,
        "text": "Finally, we applied our method on two benchmark datasets, STACOM2018, and M\\&Ms 2020 challenges, to show the potency of the proposed model.",
        "trunc_text": "Finally, we applied our method on two benchmark datasets, STACOM2018, and M\\&Ms 2020 challenges, to show the potency of ",
        "x1": 4.363931179046631,
        "x2": 4.444789886474609,
        "y1": 6.521004676818848,
        "y2": 3.3993659019470215
      },
      {
        "r": 0,
        "text": "Extensive experiments on several benchmark datasets show that our method outperforms existing methods across all datasets while maintaining low computational complexity.",
        "trunc_text": "Extensive experiments on several benchmark datasets show that our method outperforms existing methods across all dataset",
        "x1": 4.227239608764648,
        "x2": 4.440036296844482,
        "y1": 6.802992820739746,
        "y2": 3.077209711074829
      },
      {
        "r": 0,
        "text": "Our data and benchmarking results are available at: https://lmexam.com.",
        "trunc_text": "Our data and benchmarking results are available at: https://lmexam.com.",
        "x1": 4.628540515899658,
        "x2": 4.406355381011963,
        "y1": 6.476406574249268,
        "y2": 3.5050690174102783
      },
      {
        "r": 0,
        "text": "Empirically, we conduct extensive experiments on several benchmark datasets to support our theory.",
        "trunc_text": "Empirically, we conduct extensive experiments on several benchmark datasets to support our theory.",
        "x1": 4.217855930328369,
        "x2": 4.463338851928711,
        "y1": 6.488468647003174,
        "y2": 3.309889316558838
      },
      {
        "r": 0,
        "text": "Extensive experiments are conducted on multiple benchmark data sets and our method establishes a state-of-the-art performance in terms of both performance and trustworthiness.",
        "trunc_text": "Extensive experiments are conducted on multiple benchmark data sets and our method establishes a state-of-the-art perfor",
        "x1": 4.253890037536621,
        "x2": 4.4765625,
        "y1": 6.618688106536865,
        "y2": 3.333577871322632
      },
      {
        "r": 0,
        "text": "We evaluate our model on four datasets and achieve state-of-the-art performances.",
        "trunc_text": "We evaluate our model on four datasets and achieve state-of-the-art performances.",
        "x1": 4.510300159454346,
        "x2": 4.911253452301025,
        "y1": 6.842242240905762,
        "y2": 3.015155792236328
      },
      {
        "r": 0,
        "text": "Experimental results demonstrate the performance and limitations of existing algorithms, and the dataset benchmark has good versatility and effectiveness.",
        "trunc_text": "Experimental results demonstrate the performance and limitations of existing algorithms, and the dataset benchmark has g",
        "x1": 4.063607692718506,
        "x2": 4.722297191619873,
        "y1": 7.058938026428223,
        "y2": 3.0203795433044434
      },
      {
        "r": 0,
        "text": "Our extensive experiments on three benchmarks, Lucchi, MitoEM-R and MitoEM-H, reveal the benefits of the proposed contributions achieving state-of-the-art results on all three datasets.",
        "trunc_text": "Our extensive experiments on three benchmarks, Lucchi, MitoEM-R and MitoEM-H, reveal the benefits of the proposed contri",
        "x1": 4.333975791931152,
        "x2": 4.436925888061523,
        "y1": 6.596635818481445,
        "y2": 3.2810845375061035
      },
      {
        "r": 0,
        "text": "We test our method on two public datasets, our method achieves the best performances on these two datasets.",
        "trunc_text": "We test our method on two public datasets, our method achieves the best performances on these two datasets.",
        "x1": 4.3928303718566895,
        "x2": 4.927853107452393,
        "y1": 7.029267311096191,
        "y2": 3.1493287086486816
      },
      {
        "r": 0,
        "text": "Finally, the experimental results on several benchmark datasets verify the effectiveness of the proposed method.",
        "trunc_text": "Finally, the experimental results on several benchmark datasets verify the effectiveness of the proposed method.",
        "x1": 4.078729629516602,
        "x2": 4.784992218017578,
        "y1": 6.952004909515381,
        "y2": 3.259092092514038
      },
      {
        "r": 0,
        "text": "Extensive experiments on two real-world datasets show the superior performance of our method.",
        "trunc_text": "Extensive experiments on two real-world datasets show the superior performance of our method.",
        "x1": 4.083861827850342,
        "x2": 4.958065509796143,
        "y1": 7.315695762634277,
        "y2": 2.8322997093200684
      },
      {
        "r": 0,
        "text": "We have evaluated various baselines on this dataset and benchmarked it with a new neural model, SPOT, which we introduce in this paper.",
        "trunc_text": "We have evaluated various baselines on this dataset and benchmarked it with a new neural model, SPOT, which we introduce",
        "x1": 3.9606683254241943,
        "x2": 4.7168169021606445,
        "y1": 6.522767066955566,
        "y2": 3.1768641471862793
      },
      {
        "r": 0,
        "text": "The benchmark dataset, leaderboard, and baseline methods are released in https://github.com/THU-KEG/KoRC.",
        "trunc_text": "The benchmark dataset, leaderboard, and baseline methods are released in https://github.com/THU-KEG/KoRC.",
        "x1": 4.6429924964904785,
        "x2": 4.560405731201172,
        "y1": 6.446413993835449,
        "y2": 3.811269760131836
      },
      {
        "r": 0,
        "text": "Experimental results on real-world and benchmark datasets validate the effectiveness of the proposed method.",
        "trunc_text": "Experimental results on real-world and benchmark datasets validate the effectiveness of the proposed method.",
        "x1": 4.1977972984313965,
        "x2": 4.714193820953369,
        "y1": 7.177697658538818,
        "y2": 3.1466243267059326
      },
      {
        "r": 0,
        "text": "Our results improve the state-of-the-art on standard benchmarks.",
        "trunc_text": "Our results improve the state-of-the-art on standard benchmarks.",
        "x1": 4.541001319885254,
        "x2": 4.429318428039551,
        "y1": 6.4496169090271,
        "y2": 3.6659493446350098
      },
      {
        "r": 0,
        "text": "Experimental evaluations both on our assembled dataset and public benchmark datasets demonstrate the effectiveness of our proposed network.",
        "trunc_text": "Experimental evaluations both on our assembled dataset and public benchmark datasets demonstrate the effectiveness of ou",
        "x1": 4.202322959899902,
        "x2": 4.46072244644165,
        "y1": 6.667694568634033,
        "y2": 3.263742685317993
      },
      {
        "r": 0,
        "text": "Our results show that our simple synthetic datasets are sufficient to challenge most of the benchmarked methods.",
        "trunc_text": "Our results show that our simple synthetic datasets are sufficient to challenge most of the benchmarked methods.",
        "x1": 4.715518951416016,
        "x2": 4.886785507202148,
        "y1": 7.07000207901001,
        "y2": 2.371466875076294
      },
      {
        "r": 0,
        "text": "Additionally, we introduce a novel benchmark based on images from the Open Images Dataset.",
        "trunc_text": "Additionally, we introduce a novel benchmark based on images from the Open Images Dataset.",
        "x1": 2.7583978176116943,
        "x2": 3.854515552520752,
        "y1": 6.985647201538086,
        "y2": 2.8776118755340576
      },
      {
        "r": 0,
        "text": "As a result, our method TOLD achieves a DER of 10.14% on the CALLHOME dataset, which is a new state-of-the-art result on this benchmark to the best of our knowledge.",
        "trunc_text": "As a result, our method TOLD achieves a DER of 10.14% on the CALLHOME dataset, which is a new state-of-the-art result on",
        "x1": 4.353747367858887,
        "x2": 4.41734504699707,
        "y1": 6.579430103302002,
        "y2": 3.2120494842529297
      },
      {
        "r": 0,
        "text": "We conducted comprehensive experiments on multiple benchmark datasets, demonstrating the superior performance of our proposed SPIFFNet in terms of both quantitative metrics and visual quality when compared to state-of-the-art methods.",
        "trunc_text": "We conducted comprehensive experiments on multiple benchmark datasets, demonstrating the superior performance of our pro",
        "x1": 4.049069881439209,
        "x2": 4.333093166351318,
        "y1": 6.5769853591918945,
        "y2": 3.1375529766082764
      },
      {
        "r": 0,
        "text": "We provide a detailed analysis of the dataset.",
        "trunc_text": "We provide a detailed analysis of the dataset.",
        "x1": 5.228243350982666,
        "x2": 5.935349941253662,
        "y1": 7.355108261108398,
        "y2": 2.482198715209961
      },
      {
        "r": 0,
        "text": "We evaluate our method on three challenging datasets.",
        "trunc_text": "We evaluate our method on three challenging datasets.",
        "x1": 4.43217658996582,
        "x2": 5.11936092376709,
        "y1": 7.004464149475098,
        "y2": 3.0886147022247314
      },
      {
        "r": 0,
        "text": "The benchmark results produced by three different deep learning methods are provided.",
        "trunc_text": "The benchmark results produced by three different deep learning methods are provided.",
        "x1": 3.850928544998169,
        "x2": 3.9604480266571045,
        "y1": 6.3938469886779785,
        "y2": 3.293464422225952
      },
      {
        "r": 0,
        "text": "In addition, we provide extra annotations for used datasets and introduce our new benchmark.",
        "trunc_text": "In addition, we provide extra annotations for used datasets and introduce our new benchmark.",
        "x1": 4.669625282287598,
        "x2": 4.169815540313721,
        "y1": 6.697597026824951,
        "y2": 3.1188158988952637
      },
      {
        "r": 0,
        "text": "Experimental results on two real-world datasets demonstrate that our method outperforms some state-of-the-art approaches.",
        "trunc_text": "Experimental results on two real-world datasets demonstrate that our method outperforms some state-of-the-art approaches",
        "x1": 4.13054895401001,
        "x2": 4.965887546539307,
        "y1": 7.317593574523926,
        "y2": 2.70129656791687
      },
      {
        "r": 0,
        "text": "Experiments on real-world datasets demonstrate the effectiveness of our approach.",
        "trunc_text": "Experiments on real-world datasets demonstrate the effectiveness of our approach.",
        "x1": 4.304828643798828,
        "x2": 5.063546180725098,
        "y1": 7.36398983001709,
        "y2": 2.6620688438415527
      },
      {
        "r": 0,
        "text": "The extensive experiments demonstrated that our proposed method achieves state-of-the-art performance on the newly collected dataset.",
        "trunc_text": "The extensive experiments demonstrated that our proposed method achieves state-of-the-art performance on the newly colle",
        "x1": 4.110841274261475,
        "x2": 5.078198432922363,
        "y1": 7.346693992614746,
        "y2": 2.8898348808288574
      },
      {
        "r": 0,
        "text": "We evaluate our method on several datasets and demonstrate its superior performance under heavily occluded scenarios compared to other methods.",
        "trunc_text": "We evaluate our method on several datasets and demonstrate its superior performance under heavily occluded scenarios com",
        "x1": 4.124643325805664,
        "x2": 4.657450199127197,
        "y1": 6.9787421226501465,
        "y2": 2.9433250427246094
      },
      {
        "r": 0,
        "text": "The dataset and source code will be released on GitHub soon.",
        "trunc_text": "The dataset and source code will be released on GitHub soon.",
        "x1": 7.172675609588623,
        "x2": 7.664538860321045,
        "y1": 7.747176170349121,
        "y2": 2.056694746017456
      },
      {
        "r": 0,
        "text": "Then this model is trained and evaluated on the new, more extensive dataset to obtain a representative result.",
        "trunc_text": "Then this model is trained and evaluated on the new, more extensive dataset to obtain a representative result.",
        "x1": 5.0704216957092285,
        "x2": 5.629870414733887,
        "y1": 7.194782733917236,
        "y2": 2.7196247577667236
      },
      {
        "r": 0,
        "text": "Results show that the performance significantly increases with the dataset size.",
        "trunc_text": "Results show that the performance significantly increases with the dataset size.",
        "x1": 4.1265363693237305,
        "x2": 4.956724643707275,
        "y1": 6.709175109863281,
        "y2": 3.3644275665283203
      },
      {
        "r": 0,
        "text": "Details are available in CSV files provided with the datasets.   ",
        "trunc_text": "Details are available in CSV files provided with the datasets.   ",
        "x1": 6.859856128692627,
        "x2": 7.216777324676514,
        "y1": 7.764217853546143,
        "y2": 2.1556637287139893
      },
      {
        "r": 0,
        "text": "Since the manual creation of such datasets is a laborious task, obtaining data from online resources can be a cheap solution to create large-scale datasets.",
        "trunc_text": "Since the manual creation of such datasets is a laborious task, obtaining data from online resources can be a cheap solu",
        "x1": 6.250639915466309,
        "x2": 7.1436076164245605,
        "y1": 7.248350620269775,
        "y2": 2.3227124214172363
      },
      {
        "r": 0,
        "text": "Data format and usage notes:",
        "trunc_text": "Data format and usage notes:",
        "x1": 6.208852767944336,
        "x2": 6.9231343269348145,
        "y1": 6.811885356903076,
        "y2": 3.203596591949463
      },
      {
        "r": 0,
        "text": "However, online companies also gather user data for more principled purposes, such as improving the user experience and aggregating statistics.",
        "trunc_text": "However, online companies also gather user data for more principled purposes, such as improving the user experience and ",
        "x1": 6.047260761260986,
        "x2": 6.588304042816162,
        "y1": 7.0112738609313965,
        "y2": 2.926734209060669
      },
      {
        "r": 0,
        "text": "Finally, we provide publicly an open dataset, and online resources with the results.",
        "trunc_text": "Finally, we provide publicly an open dataset, and online resources with the results.",
        "x1": 6.1093220710754395,
        "x2": 6.797881126403809,
        "y1": 7.3959269523620605,
        "y2": 2.540583372116089
      },
      {
        "r": 0,
        "text": "Besides the traditional data, various types of data, including video, have become available.",
        "trunc_text": "Besides the traditional data, various types of data, including video, have become available.",
        "x1": 6.375129222869873,
        "x2": 6.858292102813721,
        "y1": 7.554406642913818,
        "y2": 2.2431106567382812
      },
      {
        "r": 0,
        "text": "Besides, with such an instruction, we can also easily carry out quantitative statistics.",
        "trunc_text": "Besides, with such an instruction, we can also easily carry out quantitative statistics.",
        "x1": 5.775051116943359,
        "x2": 6.375618934631348,
        "y1": 6.807976245880127,
        "y2": 3.129753589630127
      },
      {
        "r": 0,
        "text": "Moreover, limitations related to data sources that change over time (e.g., code bases) and the lack of documentation of extraction processes make it difficult to reproduce datasets over time.",
        "trunc_text": "Moreover, limitations related to data sources that change over time (e.g., code bases) and the lack of documentation of ",
        "x1": 5.933951377868652,
        "x2": 6.475543975830078,
        "y1": 6.970551013946533,
        "y2": 3.016394853591919
      },
      {
        "r": 0,
        "text": "Evaluation datasets and frameworks like the one we present support this line of research.",
        "trunc_text": "Evaluation datasets and frameworks like the one we present support this line of research.",
        "x1": 4.788393020629883,
        "x2": 5.427919864654541,
        "y1": 6.870222568511963,
        "y2": 3.214012622833252
      },
      {
        "r": 0,
        "text": "We then review available datasets, recent approaches and evaluation metrics of the task.",
        "trunc_text": "We then review available datasets, recent approaches and evaluation metrics of the task.",
        "x1": 4.301070690155029,
        "x2": 4.946410179138184,
        "y1": 6.4526872634887695,
        "y2": 3.2796387672424316
      },
      {
        "r": 0,
        "text": "Besides, our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation metrics.",
        "trunc_text": "Besides, our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation metrics.",
        "x1": 4.9144287109375,
        "x2": 5.456299304962158,
        "y1": 5.971158504486084,
        "y2": 3.7430291175842285
      },
      {
        "r": 0,
        "text": "However, we identify issues with the dataset quality and evaluation metric.",
        "trunc_text": "However, we identify issues with the dataset quality and evaluation metric.",
        "x1": 5.05505895614624,
        "x2": 5.553980827331543,
        "y1": 5.958939552307129,
        "y2": 3.7456395626068115
      },
      {
        "r": 0,
        "text": "In order to obtain meaningful results, careful partitioning of data into training, validation, and test sets, as well as the selection of suitable evaluation metrics are crucial.",
        "trunc_text": "In order to obtain meaningful results, careful partitioning of data into training, validation, and test sets, as well as",
        "x1": 4.342698097229004,
        "x2": 5.037935733795166,
        "y1": 6.1745734214782715,
        "y2": 3.6459410190582275
      },
      {
        "r": 0,
        "text": "To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics).",
        "trunc_text": "To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metri",
        "x1": 4.2066521644592285,
        "x2": 4.664191246032715,
        "y1": 6.1166863441467285,
        "y2": 3.8072962760925293
      },
      {
        "r": 0,
        "text": "These metrics have been successful on datasets that leverage the average human perception in limited settings.",
        "trunc_text": "These metrics have been successful on datasets that leverage the average human perception in limited settings.",
        "x1": 4.623767852783203,
        "x2": 5.279956340789795,
        "y1": 6.383060455322266,
        "y2": 3.4574687480926514
      },
      {
        "r": 0,
        "text": "We use standard metrics to evaluate the performances of the different training scenarios.",
        "trunc_text": "We use standard metrics to evaluate the performances of the different training scenarios.",
        "x1": 4.0801215171813965,
        "x2": 4.64384651184082,
        "y1": 6.032738208770752,
        "y2": 3.729933738708496
      },
      {
        "r": 0,
        "text": "Finally, we report extensive experiments on two real-world datasets to offer insight into the efficiency, scalability, and effectiveness of the proposed framework.",
        "trunc_text": "Finally, we report extensive experiments on two real-world datasets to offer insight into the efficiency, scalability, a",
        "x1": 4.37677526473999,
        "x2": 5.027751445770264,
        "y1": 7.3560566902160645,
        "y2": 2.737928628921509
      },
      {
        "r": 0,
        "text": "Here we describe Athena 2.0, UCSC's conversational agent for Amazon's Socialbot Grand Challenge 4.",
        "trunc_text": "Here we describe Athena 2.0, UCSC's conversational agent for Amazon's Socialbot Grand Challenge 4.",
        "x1": 5.907057285308838,
        "x2": 3.911832094192505,
        "y1": 3.242648124694824,
        "y2": 7.129494667053223
      },
      {
        "r": 0,
        "text": "The Alexa Prize Challenge aims to create a socialbot, which allows the user to engage in coherent conversations, on a range of popular topics that will interest the user.",
        "trunc_text": "The Alexa Prize Challenge aims to create a socialbot, which allows the user to engage in coherent conversations, on a ra",
        "x1": 6.045308589935303,
        "x2": 3.999488353729248,
        "y1": 3.2803452014923096,
        "y2": 7.214863300323486
      },
      {
        "r": 0,
        "text": "Athena 2.0 also relies on a user model to personalize topic selection and other aspects of the conversation to individual users.",
        "trunc_text": "Athena 2.0 also relies on a user model to personalize topic selection and other aspects of the conversation to individua",
        "x1": 5.730723857879639,
        "x2": 3.7090585231781006,
        "y1": 3.114739418029785,
        "y2": 6.985008239746094
      },
      {
        "r": 0,
        "text": "Athena 2.0 utilizes a novel knowledge-grounded discourse model that tracks the entity links that Athena introduces into the dialogue, and uses them to constrain named-entity recognition and linking, and coreference resolution.",
        "trunc_text": "Athena 2.0 utilizes a novel knowledge-grounded discourse model that tracks the entity links that Athena introduces into ",
        "x1": 5.715214729309082,
        "x2": 3.656595230102539,
        "y1": 3.120173692703247,
        "y2": 6.916886806488037
      },
      {
        "r": 0,
        "text": "Code and data are publicly available on GitHub.",
        "trunc_text": "Code and data are publicly available on GitHub.",
        "x1": 7.8491973876953125,
        "x2": 8.349669456481934,
        "y1": 7.267940521240234,
        "y2": 2.664797306060791
      },
      {
        "r": 0,
        "text": "The code is publicly available.",
        "trunc_text": "The code is publicly available.",
        "x1": 8.323941230773926,
        "x2": 8.88492202758789,
        "y1": 7.34203577041626,
        "y2": 2.6857526302337646
      },
      {
        "r": 0,
        "text": "The code and models will be publicly available.",
        "trunc_text": "The code and models will be publicly available.",
        "x1": 7.924649238586426,
        "x2": 8.415225982666016,
        "y1": 7.1795172691345215,
        "y2": 2.704040288925171
      },
      {
        "r": 0,
        "text": "We make the source code and models publicly available.",
        "trunc_text": "We make the source code and models publicly available.",
        "x1": 7.802188396453857,
        "x2": 8.219514846801758,
        "y1": 6.9648003578186035,
        "y2": 2.946725845336914
      },
      {
        "r": 0,
        "text": "Our code is publicly available.",
        "trunc_text": "Our code is publicly available.",
        "x1": 8.28121280670166,
        "x2": 8.899640083312988,
        "y1": 7.257635116577148,
        "y2": 2.735405683517456
      },
      {
        "r": 0,
        "text": "Code will be publicly available.",
        "trunc_text": "Code will be publicly available.",
        "x1": 8.345808029174805,
        "x2": 8.911955833435059,
        "y1": 7.289069175720215,
        "y2": 2.723463535308838
      },
      {
        "r": 0,
        "text": "Our data and code are publicly available.",
        "trunc_text": "Our data and code are publicly available.",
        "x1": 7.878459453582764,
        "x2": 8.347210884094238,
        "y1": 7.127894401550293,
        "y2": 2.797344923019409
      },
      {
        "r": 0,
        "text": "Code is publicly released.",
        "trunc_text": "Code is publicly released.",
        "x1": 8.432243347167969,
        "x2": 8.938619613647461,
        "y1": 7.278984069824219,
        "y2": 2.726651906967163
      },
      {
        "r": 0,
        "text": "The code will be accessible to the public.",
        "trunc_text": "The code will be accessible to the public.",
        "x1": 8.3607177734375,
        "x2": 8.962010383605957,
        "y1": 7.319457530975342,
        "y2": 2.7482504844665527
      },
      {
        "r": 0,
        "text": "Code and data will be made publicly available.",
        "trunc_text": "Code and data will be made publicly available.",
        "x1": 7.9589972496032715,
        "x2": 8.372403144836426,
        "y1": 7.144597053527832,
        "y2": 2.754638433456421
      },
      {
        "r": 0,
        "text": "Code will be made publicly available.",
        "trunc_text": "Code will be made publicly available.",
        "x1": 8.379373550415039,
        "x2": 8.88884162902832,
        "y1": 7.293862819671631,
        "y2": 2.727247476577759
      },
      {
        "r": 0,
        "text": "Our data and code are publicly available at: https://pointodyssey.com",
        "trunc_text": "Our data and code are publicly available at: https://pointodyssey.com",
        "x1": 7.943702697753906,
        "x2": 8.385929107666016,
        "y1": 7.191291809082031,
        "y2": 2.6814956665039062
      },
      {
        "r": 0,
        "text": "The code will be made publicly available.",
        "trunc_text": "The code will be made publicly available.",
        "x1": 8.371479988098145,
        "x2": 8.943696022033691,
        "y1": 7.319443702697754,
        "y2": 2.757932662963867
      },
      {
        "r": 0,
        "text": "The source code and dataset are available at project page.",
        "trunc_text": "The source code and dataset are available at project page.",
        "x1": 7.570284843444824,
        "x2": 7.968913555145264,
        "y1": 7.843276023864746,
        "y2": 2.213925361633301
      },
      {
        "r": 0,
        "text": "Project page is https://damo-vilab.github.io/AnyDoor-Page/.",
        "trunc_text": "Project page is https://damo-vilab.github.io/AnyDoor-Page/.",
        "x1": 7.98986291885376,
        "x2": 8.58081340789795,
        "y1": 8.025794982910156,
        "y2": 1.855350136756897
      },
      {
        "r": 0,
        "text": "Source code is public, and can be accessed at: https://github.com/ApplicationTechnologyOfMedicalBigData/pFedNet-code.",
        "trunc_text": "Source code is public, and can be accessed at: https://github.com/ApplicationTechnologyOfMedicalBigData/pFedNet-code.",
        "x1": 8.282601356506348,
        "x2": 8.841666221618652,
        "y1": 7.342260837554932,
        "y2": 2.6485214233398438
      },
      {
        "r": 0,
        "text": "Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.",
        "trunc_text": "Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.",
        "x1": 7.964924335479736,
        "x2": 8.546394348144531,
        "y1": 7.994287490844727,
        "y2": 1.8829282522201538
      },
      {
        "r": 0,
        "text": "Our code will be publicly available.",
        "trunc_text": "Our code will be publicly available.",
        "x1": 8.355680465698242,
        "x2": 8.905734062194824,
        "y1": 7.309675693511963,
        "y2": 2.727531671524048
      },
      {
        "r": 0,
        "text": "In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds.",
        "trunc_text": "In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes ",
        "x1": 2.399498462677002,
        "x2": 3.8867483139038086,
        "y1": 9.439738273620605,
        "y2": -0.3689592182636261
      },
      {
        "r": 0,
        "text": "Furthermore, we propose a 3D human pose estimation algorithm based on multi-modal data input, which demonstrates the advantages of multi-modal data input for 3D human pose estimation.",
        "trunc_text": "Furthermore, we propose a 3D human pose estimation algorithm based on multi-modal data input, which demonstrates the adv",
        "x1": 2.4027371406555176,
        "x2": 3.8246068954467773,
        "y1": 9.431697845458984,
        "y2": -0.3875506818294525
      },
      {
        "r": 0,
        "text": "3D human pose estimation in outdoor environments has garnered increasing attention recently.",
        "trunc_text": "3D human pose estimation in outdoor environments has garnered increasing attention recently.",
        "x1": 2.4202284812927246,
        "x2": 3.8659818172454834,
        "y1": 9.458600997924805,
        "y2": -0.3636600077152252
      },
      {
        "r": 0,
        "text": "We introduce a novel one-stage end-to-end multi-person 2D pose estimation algorithm, known as Joint Coordinate Regression and Association (JCRA), that produces human pose joints and associations without requiring any post-processing.",
        "trunc_text": "We introduce a novel one-stage end-to-end multi-person 2D pose estimation algorithm, known as Joint Coordinate Regressio",
        "x1": 2.444689989089966,
        "x2": 3.8899405002593994,
        "y1": 9.4527006149292,
        "y2": -0.3981028199195862
      },
      {
        "r": 0,
        "text": "However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene.",
        "trunc_text": "However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize onl",
        "x1": 2.3433377742767334,
        "x2": 3.8334450721740723,
        "y1": 9.352964401245117,
        "y2": -0.2825424075126648
      },
      {
        "r": 0,
        "text": "iEDA is publicly available from the project home page http://ieda.oscc.cc.",
        "trunc_text": "iEDA is publicly available from the project home page http://ieda.oscc.cc.",
        "x1": 7.39267110824585,
        "x2": 7.901576995849609,
        "y1": 6.965636253356934,
        "y2": 2.931705951690674
      },
      {
        "r": 0,
        "text": "The codes and data are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "trunc_text": "The codes and data are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "x1": 7.771755218505859,
        "x2": 8.29936408996582,
        "y1": 7.224352836608887,
        "y2": 2.6549015045166016
      },
      {
        "r": 0,
        "text": "Our extensive set of experiments on a variety of standard and few-shot datasets show that our method produces substantially improved performance when compared to the current state of the art methods.",
        "trunc_text": "Our extensive set of experiments on a variety of standard and few-shot datasets show that our method produces substantia",
        "x1": 2.9963622093200684,
        "x2": 3.728583812713623,
        "y1": 6.726274490356445,
        "y2": 3.252408742904663
      },
      {
        "r": 0,
        "text": "Our benchmark reflects real-world data constraints by evaluating methods across a range of dataset sizes, including challenging few-shot settings that incentivize the use of pretraining.",
        "trunc_text": "Our benchmark reflects real-world data constraints by evaluating methods across a range of dataset sizes, including chal",
        "x1": 2.9545881748199463,
        "x2": 3.6985161304473877,
        "y1": 6.6140456199646,
        "y2": 3.1603713035583496
      },
      {
        "r": 0,
        "text": "Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur.",
        "trunc_text": "Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when tes",
        "x1": 3.2591440677642822,
        "x2": 3.94745135307312,
        "y1": 6.602460861206055,
        "y2": 3.039423942565918
      },
      {
        "r": 0,
        "text": "We demonstrate that our method achieves state-of-the-art results on several challenging benchmark datasets even in comparison to recent supervised methods, and that our method reaches previously unseen cross-dataset generalisation ability.",
        "trunc_text": "We demonstrate that our method achieves state-of-the-art results on several challenging benchmark datasets even in compa",
        "x1": 3.298649311065674,
        "x2": 3.95094895362854,
        "y1": 6.500587463378906,
        "y2": 3.0918495655059814
      },
      {
        "r": 0,
        "text": "Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions.",
        "trunc_text": "Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of simila",
        "x1": 2.9049153327941895,
        "x2": 3.616273880004883,
        "y1": 6.647900581359863,
        "y2": 3.3072025775909424
      },
      {
        "r": 0,
        "text": "Traditional zero-shot learning methods are constrained by the training dataset.",
        "trunc_text": "Traditional zero-shot learning methods are constrained by the training dataset.",
        "x1": 2.846968650817871,
        "x2": 3.5049593448638916,
        "y1": 6.635625839233398,
        "y2": 3.3670835494995117
      },
      {
        "r": 0,
        "text": "Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining.",
        "trunc_text": "Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yieldin",
        "x1": 2.864567995071411,
        "x2": 3.746032238006592,
        "y1": 6.4882426261901855,
        "y2": 2.8401095867156982
      },
      {
        "r": 0,
        "text": "Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.",
        "trunc_text": "Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evalua",
        "x1": 2.969491720199585,
        "x2": 3.7163684368133545,
        "y1": 6.687559604644775,
        "y2": 3.2509610652923584
      },
      {
        "r": 0,
        "text": "Combining both, our method (ProVP-Ref) is evaluated on 11 image benchmark datasets and achieves 7/11 state-of-theart results on both few-shot and base-to-novel settings.",
        "trunc_text": "Combining both, our method (ProVP-Ref) is evaluated on 11 image benchmark datasets and achieves 7/11 state-of-theart res",
        "x1": 2.8438446521759033,
        "x2": 3.785029411315918,
        "y1": 6.888392925262451,
        "y2": 3.013415813446045
      },
      {
        "r": 0,
        "text": "We perform experiments on simulated and retrospective in-vivo data to evaluate the performance of the proposed zero-shot learning method for temporal FSE reconstruction.",
        "trunc_text": "We perform experiments on simulated and retrospective in-vivo data to evaluate the performance of the proposed zero-shot",
        "x1": 2.9250900745391846,
        "x2": 3.63755464553833,
        "y1": 6.711464881896973,
        "y2": 3.1525092124938965
      },
      {
        "r": 0,
        "text": "Finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks.",
        "trunc_text": "Finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks.",
        "x1": 4.039669990539551,
        "x2": 4.5611772537231445,
        "y1": 6.7725324630737305,
        "y2": 3.339693546295166
      },
      {
        "r": 0,
        "text": "Extensive experiments are conducted on multiple benchmark datasets to validate the effectiveness of our proposed method.",
        "trunc_text": "Extensive experiments are conducted on multiple benchmark datasets to validate the effectiveness of our proposed method.",
        "x1": 4.147006988525391,
        "x2": 4.698464870452881,
        "y1": 6.773273468017578,
        "y2": 3.375046968460083
      },
      {
        "r": 0,
        "text": "We have quantitatively and qualitatively validated our method on three common benchmark datasets: Human3.6M, MPI-INF-3DHP, and HumanEva.",
        "trunc_text": "We have quantitatively and qualitatively validated our method on three common benchmark datasets: Human3.6M, MPI-INF-3DH",
        "x1": 4.090219497680664,
        "x2": 4.316250801086426,
        "y1": 6.729002475738525,
        "y2": 3.2627601623535156
      },
      {
        "r": 0,
        "text": "Extensive experiments on six datasets show substantial improvements to the baseline.",
        "trunc_text": "Extensive experiments on six datasets show substantial improvements to the baseline.",
        "x1": 3.9071719646453857,
        "x2": 4.992986679077148,
        "y1": 6.820870876312256,
        "y2": 3.395360231399536
      },
      {
        "r": 0,
        "text": "Besides, since medical OSR is still a nascent field, two publicly available benchmark datasets are proposed for comparison.",
        "trunc_text": "Besides, since medical OSR is still a nascent field, two publicly available benchmark datasets are proposed for comparis",
        "x1": 3.6323862075805664,
        "x2": 4.336943626403809,
        "y1": 7.106700420379639,
        "y2": 2.6030473709106445
      },
      {
        "r": 0,
        "text": "In this study, we conducted experiments using medical datasets comprising only 100 samples from three medical modalities.",
        "trunc_text": "In this study, we conducted experiments using medical datasets comprising only 100 samples from three medical modalities",
        "x1": 3.6939327716827393,
        "x2": 4.7363457679748535,
        "y1": 7.366372108459473,
        "y2": 2.409071207046509
      },
      {
        "r": 0,
        "text": "To validate our model design, we conduct extensive experiments on three benchmark datasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP.",
        "trunc_text": "To validate our model design, we conduct extensive experiments on three benchmark datasets: Human3.6M, HumanEva-I, and M",
        "x1": 4.153220176696777,
        "x2": 4.438553333282471,
        "y1": 6.616299152374268,
        "y2": 3.356677293777466
      },
      {
        "r": 0,
        "text": "We conduct extensive experiments on three public benchmarks (i.e., HumanEval, MBPP, and MBCPP).",
        "trunc_text": "We conduct extensive experiments on three public benchmarks (i.e., HumanEval, MBPP, and MBCPP).",
        "x1": 4.1334228515625,
        "x2": 4.450088977813721,
        "y1": 6.756925582885742,
        "y2": 3.4425346851348877
      },
      {
        "r": 0,
        "text": "Comprehensive experiments are conducted on public benchmarks and the corresponding experimental results demonstrate the effectiveness of our proposed method.",
        "trunc_text": "Comprehensive experiments are conducted on public benchmarks and the corresponding experimental results demonstrate the ",
        "x1": 4.086844444274902,
        "x2": 4.439395904541016,
        "y1": 6.758362770080566,
        "y2": 3.5045766830444336
      },
      {
        "r": 0,
        "text": "We have conducted experiments on two real datasets and compare our method with three baselines.",
        "trunc_text": "We have conducted experiments on two real datasets and compare our method with three baselines.",
        "x1": 3.898153305053711,
        "x2": 5.00280237197876,
        "y1": 6.85564661026001,
        "y2": 3.4525084495544434
      },
      {
        "r": 0,
        "text": "We have theoretically analyzed our method and empirically evaluated it on two real-world medical datasets.",
        "trunc_text": "We have theoretically analyzed our method and empirically evaluated it on two real-world medical datasets.",
        "x1": 3.720170497894287,
        "x2": 4.6774067878723145,
        "y1": 7.34415340423584,
        "y2": 2.5480833053588867
      },
      {
        "r": 0,
        "text": "We then describe the dataset and the results of benchmarking.",
        "trunc_text": "We then describe the dataset and the results of benchmarking.",
        "x1": 4.5124640464782715,
        "x2": 4.419365406036377,
        "y1": 6.62556266784668,
        "y2": 3.3562023639678955
      },
      {
        "r": 0,
        "text": "ModFed is evaluated on three in-vivo datasets.",
        "trunc_text": "ModFed is evaluated on three in-vivo datasets.",
        "x1": 3.5949223041534424,
        "x2": 4.511910438537598,
        "y1": 7.3852057456970215,
        "y2": 2.5149521827697754
      },
      {
        "r": 0,
        "text": "To this end, we introduce a new data curation pipeline to construct a culturally relevant parallel corpus, enriched with annotations of cultural-specific entities.",
        "trunc_text": "To this end, we introduce a new data curation pipeline to construct a culturally relevant parallel corpus, enriched with",
        "x1": 2.7326622009277344,
        "x2": 1.6474751234054565,
        "y1": 3.383812665939331,
        "y2": 5.625799179077148
      },
      {
        "r": 0,
        "text": "Furthermore, while engineers have developed cutting-edge technologies for image classification, there is still a gap in the application of these models in human heritage collections, where data sets usually consist of low-quality pictures of people with diverse ethnicity, gender, and age.",
        "trunc_text": "Furthermore, while engineers have developed cutting-edge technologies for image classification, there is still a gap in ",
        "x1": 2.472461223602295,
        "x2": 4.041152000427246,
        "y1": 8.102938652038574,
        "y2": 1.3101242780685425
      },
      {
        "r": 0,
        "text": "We evaluated the effectiveness of the bias mitigation pipeline on a cultural heritage collection of photographs from the 19th and 20th centuries, and we used the FairFace data set for the transfer learning experiments.",
        "trunc_text": "We evaluated the effectiveness of the bias mitigation pipeline on a cultural heritage collection of photographs from the",
        "x1": 2.4122726917266846,
        "x2": 3.9393422603607178,
        "y1": 8.05057144165039,
        "y2": 1.394871711730957
      },
      {
        "r": 0,
        "text": "This paper answers the need to support GLAM institutions in facilitating the transition into publishing their digital content and to introduce collections as data services; this will also help their future efficient contribution to data spaces and cultural heritage clouds.",
        "trunc_text": "This paper answers the need to support GLAM institutions in facilitating the transition into publishing their digital co",
        "x1": 5.954994201660156,
        "x2": 6.563216209411621,
        "y1": 7.526963710784912,
        "y2": 2.6238222122192383
      },
      {
        "r": 0,
        "text": "Within the current transition to the emerging data spaces, clouds for cultural heritage and open science, the need to identify practices which support more GLAM institutions to offer datasets becomes a priority, especially within the smaller and medium-sized institutions.   ",
        "trunc_text": "Within the current transition to the emerging data spaces, clouds for cultural heritage and open science, the need to id",
        "x1": 5.888839244842529,
        "x2": 6.543671607971191,
        "y1": 7.479493618011475,
        "y2": 2.540631055831909
      },
      {
        "r": 0,
        "text": "Creating an intelligent search and retrieval system for artwork images, particularly paintings, is crucial for documenting cultural heritage, fostering wider public engagement, and advancing artistic analysis and interpretation.",
        "trunc_text": "Creating an intelligent search and retrieval system for artwork images, particularly paintings, is crucial for documenti",
        "x1": 2.5417909622192383,
        "x2": 4.087177753448486,
        "y1": 8.095720291137695,
        "y2": 1.3883178234100342
      },
      {
        "r": 0,
        "text": "We will release our annotation scheme, the corpus, and codes to the research community to alleviate the scarcity of labeled data in this domain.",
        "trunc_text": "We will release our annotation scheme, the corpus, and codes to the research community to alleviate the scarcity of labe",
        "x1": 2.672450065612793,
        "x2": 1.5711698532104492,
        "y1": 3.4331233501434326,
        "y2": 5.565897464752197
      },
      {
        "r": 0,
        "text": "Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities.",
        "trunc_text": "Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or",
        "x1": 2.4128000736236572,
        "x2": 4.036803722381592,
        "y1": 8.34376335144043,
        "y2": 1.023980736732483
      },
      {
        "r": 0,
        "text": "Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains.",
        "trunc_text": "Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, herita",
        "x1": 2.313713788986206,
        "x2": 3.978455066680908,
        "y1": 8.428266525268555,
        "y2": 1.0470064878463745
      },
      {
        "r": 0,
        "text": "Our second contribution is to conduct a comprehensive analysis of the annotations, focusing on how different demographic groups are represented.",
        "trunc_text": "Our second contribution is to conduct a comprehensive analysis of the annotations, focusing on how different demographic",
        "x1": 2.572460889816284,
        "x2": 1.4795714616775513,
        "y1": 3.445209503173828,
        "y2": 5.5561089515686035
      },
      {
        "r": 0,
        "text": "Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment.",
        "trunc_text": "Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which o",
        "x1": 4.284651756286621,
        "x2": 4.678827285766602,
        "y1": 7.034337520599365,
        "y2": 2.701850414276123
      },
      {
        "r": 0,
        "text": "Moreover, our method demonstrates high generalization capability in cross-dataset experiments, even when the training and test sets have different characteristics.",
        "trunc_text": "Moreover, our method demonstrates high generalization capability in cross-dataset experiments, even when the training an",
        "x1": 3.3207619190216064,
        "x2": 4.035435676574707,
        "y1": 6.537312984466553,
        "y2": 3.176928997039795
      },
      {
        "r": 0,
        "text": "Our model shows strong performance improvements on the Winoground and VL-checklist datasets with only a mild degradation in zero-shot performance.",
        "trunc_text": "Our model shows strong performance improvements on the Winoground and VL-checklist datasets with only a mild degradation",
        "x1": 2.999752998352051,
        "x2": 3.7094814777374268,
        "y1": 6.696235179901123,
        "y2": 3.2913718223571777
      },
      {
        "r": 0,
        "text": "We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet.",
        "trunc_text": "We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet.",
        "x1": 2.534435510635376,
        "x2": 4.346351623535156,
        "y1": 7.13114595413208,
        "y2": 2.3212203979492188
      },
      {
        "r": 0,
        "text": "We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models.",
        "trunc_text": "We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distri",
        "x1": 3.581544876098633,
        "x2": 4.150781154632568,
        "y1": 6.4310712814331055,
        "y2": 3.515756368637085
      },
      {
        "r": 0,
        "text": "This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality.",
        "trunc_text": "This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is general",
        "x1": 2.861335039138794,
        "x2": 3.6081347465515137,
        "y1": 6.597591400146484,
        "y2": 3.251715898513794
      },
      {
        "r": 0,
        "text": "Experiments on two datasets demonstrate that our proposed method outperforms the baselines and achieves new state-of-the-art performance.",
        "trunc_text": "Experiments on two datasets demonstrate that our proposed method outperforms the baselines and achieves new state-of-the",
        "x1": 3.958441972732544,
        "x2": 4.8862128257751465,
        "y1": 6.793090343475342,
        "y2": 3.3606157302856445
      },
      {
        "r": 0,
        "text": "Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and a superior performance in zero-shot learning scenarios as compared to state-of-the-art baselines.",
        "trunc_text": "Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and ",
        "x1": 3.7891550064086914,
        "x2": 2.7227017879486084,
        "y1": 3.4274051189422607,
        "y2": 5.68959379196167
      },
      {
        "r": 0,
        "text": "Cross-dataset evaluation further shows the strong generalization ability of our approach.",
        "trunc_text": "Cross-dataset evaluation further shows the strong generalization ability of our approach.",
        "x1": 3.3331737518310547,
        "x2": 3.9389171600341797,
        "y1": 6.547784805297852,
        "y2": 3.125227451324463
      },
      {
        "r": 0,
        "text": "We also tested our model in a zero-shot setting on an independent test site without any additional fine-tuning.",
        "trunc_text": "We also tested our model in a zero-shot setting on an independent test site without any additional fine-tuning.",
        "x1": 2.9279820919036865,
        "x2": 3.6001169681549072,
        "y1": 6.652429103851318,
        "y2": 3.38508677482605
      },
      {
        "r": 0,
        "text": "Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets.",
        "trunc_text": "Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets.",
        "x1": 2.8905982971191406,
        "x2": 3.563983678817749,
        "y1": 6.627299785614014,
        "y2": 3.3349852561950684
      },
      {
        "r": 0,
        "text": "Extensive experiments on real-world degraded datasets demonstrate the effectiveness of our proposed method.",
        "trunc_text": "Extensive experiments on real-world degraded datasets demonstrate the effectiveness of our proposed method.",
        "x1": 4.246715545654297,
        "x2": 4.986626148223877,
        "y1": 7.340506553649902,
        "y2": 2.7637674808502197
      },
      {
        "r": 0,
        "text": "We propose a new approach, CLIPPR (CLIP with Priors), which adapts zero-shot models for regression and classification on unlabelled datasets.",
        "trunc_text": "We propose a new approach, CLIPPR (CLIP with Priors), which adapts zero-shot models for regression and classification on",
        "x1": 2.8838493824005127,
        "x2": 3.4795987606048584,
        "y1": 6.569454193115234,
        "y2": 3.3111348152160645
      },
      {
        "r": 0,
        "text": "Extensive experiments show that our model has great cross-dataset generalization.",
        "trunc_text": "Extensive experiments show that our model has great cross-dataset generalization.",
        "x1": 3.244340181350708,
        "x2": 3.9290850162506104,
        "y1": 6.4756388664245605,
        "y2": 3.1650521755218506
      },
      {
        "r": 0,
        "text": "Without resorting to any complex techniques, such as image translation, augmentation, or rare-class sampling, we set a new state-of-the-art on all benchmarks.",
        "trunc_text": "Without resorting to any complex techniques, such as image translation, augmentation, or rare-class sampling, we set a n",
        "x1": 2.6911799907684326,
        "x2": 3.8263678550720215,
        "y1": 7.161770820617676,
        "y2": 2.6745283603668213
      },
      {
        "r": 0,
        "text": "In addition, our method also surprisingly helps improve the generalization ability of the models under zero-shot settings.",
        "trunc_text": "In addition, our method also surprisingly helps improve the generalization ability of the models under zero-shot setting",
        "x1": 2.9324100017547607,
        "x2": 3.582733392715454,
        "y1": 6.614019393920898,
        "y2": 3.3966259956359863
      },
      {
        "r": 0,
        "text": "We demonstrate the effectiveness of our method by conducting experiments on benchmark datasets for few-shot learning as well as in the real world for robot perception.",
        "trunc_text": "We demonstrate the effectiveness of our method by conducting experiments on benchmark datasets for few-shot learning as ",
        "x1": 2.9656622409820557,
        "x2": 3.672494888305664,
        "y1": 6.676734924316406,
        "y2": 3.2203633785247803
      },
      {
        "r": 0,
        "text": "Existing methods show impressive results on individual datasets but lack the ability to generalize to unseen scenarios.",
        "trunc_text": "Existing methods show impressive results on individual datasets but lack the ability to generalize to unseen scenarios.",
        "x1": 3.891228437423706,
        "x2": 5.006125450134277,
        "y1": 7.097859859466553,
        "y2": 2.9580283164978027
      },
      {
        "r": 0,
        "text": "Empirically, our method achieves better performance than all baselines on multiple datasets.",
        "trunc_text": "Empirically, our method achieves better performance than all baselines on multiple datasets.",
        "x1": 3.7772538661956787,
        "x2": 4.8851752281188965,
        "y1": 6.596404075622559,
        "y2": 3.557196617126465
      },
      {
        "r": 0,
        "text": "Our technique functions is in a zero-shot fashion, as it only utilizes data from a single scan of highly undersampled time series images.",
        "trunc_text": "Our technique functions is in a zero-shot fashion, as it only utilizes data from a single scan of highly undersampled ti",
        "x1": 3.0508904457092285,
        "x2": 3.7085840702056885,
        "y1": 6.781460762023926,
        "y2": 2.9099907875061035
      },
      {
        "r": 0,
        "text": "As a result, current methods often resort to random sampling from supervised datasets to create \"few-data\" setups and employ inconsistent training strategies during evaluations, which poses a challenge in accurately comparing recent progress.",
        "trunc_text": "As a result, current methods often resort to random sampling from supervised datasets to create \"few-data\" setups and em",
        "x1": 3.587618827819824,
        "x2": 4.226232051849365,
        "y1": 6.375464916229248,
        "y2": 3.6481215953826904
      },
      {
        "r": 0,
        "text": "In comparison to existing zero-shot methods, our approach is universally applicable to any retriever without additional adaptation or indexing.",
        "trunc_text": "In comparison to existing zero-shot methods, our approach is universally applicable to any retriever without additional ",
        "x1": 2.9621267318725586,
        "x2": 3.565058708190918,
        "y1": 6.685324668884277,
        "y2": 3.2545723915100098
      },
      {
        "r": 0,
        "text": "However, due to the computational demands associated with training these models, their applications often rely on zero-shot settings.",
        "trunc_text": "However, due to the computational demands associated with training these models, their applications often rely on zero-s",
        "x1": 2.841183662414551,
        "x2": 3.5932788848876953,
        "y1": 6.617869853973389,
        "y2": 3.3668360710144043
      },
      {
        "r": 0,
        "text": "The model can capture more context information from multiple scales and better fuse the local and global information to achieve high-quality segmentation.",
        "trunc_text": "The model can capture more context information from multiple scales and better fuse the local and global information to ",
        "x1": 1.0321522951126099,
        "x2": 2.630805730819702,
        "y1": 7.592845916748047,
        "y2": 1.3574119806289673
      },
      {
        "r": 0,
        "text": "When benchmarking against popular existing segmentation models across three datasets, our proposed model demonstrates a substantial leap in performance.",
        "trunc_text": "When benchmarking against popular existing segmentation models across three datasets, our proposed model demonstrates a ",
        "x1": 0.8651097416877747,
        "x2": 2.5697600841522217,
        "y1": 7.144618511199951,
        "y2": 1.3793511390686035
      },
      {
        "r": 0,
        "text": "Extensive experiments on the PASCAL VOC 2012 and Cityscapes datasets demonstrate the superiority and scalability of the proposed method, outperforming the current state-of-the-art approaches.",
        "trunc_text": "Extensive experiments on the PASCAL VOC 2012 and Cityscapes datasets demonstrate the superiority and scalability of the ",
        "x1": 3.6878254413604736,
        "x2": 4.735204696655273,
        "y1": 7.6966233253479,
        "y2": 2.5102508068084717
      },
      {
        "r": 0,
        "text": "In this paper, we present a framework to enhance depth by leveraging semantic segmentation to guide the network to jump out of the local minimum.",
        "trunc_text": "In this paper, we present a framework to enhance depth by leveraging semantic segmentation to guide the network to jump ",
        "x1": 0.9664123058319092,
        "x2": 2.656972646713257,
        "y1": 8.295187950134277,
        "y2": 0.9194732904434204
      },
      {
        "r": 0,
        "text": "The DNMP provides a new paradigm for urban-level scene representation with appealing properties: $(1)$ High-quality rendering.",
        "trunc_text": "The DNMP provides a new paradigm for urban-level scene representation with appealing properties: $(1)$ High-quality rend",
        "x1": 1.878721833229065,
        "x2": 3.8243820667266846,
        "y1": 8.897326469421387,
        "y2": 0.6732800602912903
      },
      {
        "r": 0,
        "text": "Results on the Cityscapes and Mapillary datasets show the proposed approach achieves significantly more controllability and improved image quality than previous approaches on urban scenes and is on par with general-purpose non-controllable generative models (like StyleGAN2) in terms of quality.",
        "trunc_text": "Results on the Cityscapes and Mapillary datasets show the proposed approach achieves significantly more controllability ",
        "x1": 2.0567636489868164,
        "x2": 3.5190844535827637,
        "y1": 7.413866996765137,
        "y2": 1.9256037473678589
      },
      {
        "r": 0,
        "text": "To facilitate a unified comparison between novel segmentation algorithms, we propose a standardized evaluation strategy for our dataset.",
        "trunc_text": "To facilitate a unified comparison between novel segmentation algorithms, we propose a standardized evaluation strategy ",
        "x1": 0.8024983406066895,
        "x2": 2.5260860919952393,
        "y1": 7.059961795806885,
        "y2": 1.4236358404159546
      },
      {
        "r": 0,
        "text": "Experimental results on the nuScenes dataset demonstrate that our framework is highly compatible with various map segmentation and detection architectures and considerably strengthens map prediction performance, even under adverse weather conditions and across longer horizons.",
        "trunc_text": "Experimental results on the nuScenes dataset demonstrate that our framework is highly compatible with various map segmen",
        "x1": 1.229371428489685,
        "x2": 4.429455280303955,
        "y1": 7.880923271179199,
        "y2": 0.670321524143219
      },
      {
        "r": 0,
        "text": "For the high-resolution Cityscapes and Mapillary Vistas datasets, we achieve improvements of up to +2.5 on the Panoptic Quality for thing classes, and even more considerable gains of up to +5.8 on both the pixel accuracy and pixel precision, which we identify as better metrics to capture the confusion problem.",
        "trunc_text": "For the high-resolution Cityscapes and Mapillary Vistas datasets, we achieve improvements of up to +2.5 on the Panoptic ",
        "x1": 1.916409969329834,
        "x2": 3.6430037021636963,
        "y1": 7.557335376739502,
        "y2": 1.708125114440918
      },
      {
        "r": 0,
        "text": "Our experimental results demonstrate the effectiveness of our approach in overcoming the challenges posed by urban traffic scenes.",
        "trunc_text": "Our experimental results demonstrate the effectiveness of our approach in overcoming the challenges posed by urban traff",
        "x1": 3.384758949279785,
        "x2": 5.077572822570801,
        "y1": 10.004057884216309,
        "y2": 0.10487987846136093
      },
      {
        "r": 0,
        "text": "To develop and evaluate our approach, a large urban driving dataset dubbed AV Breadcrumbs is automatically labeled by leveraging vector map representations and projective geometry to annotate over 900,000 images.",
        "trunc_text": "To develop and evaluate our approach, a large urban driving dataset dubbed AV Breadcrumbs is automatically labeled by le",
        "x1": 3.307739734649658,
        "x2": 4.8638200759887695,
        "y1": 9.855775833129883,
        "y2": -0.04282315820455551
      },
      {
        "r": 0,
        "text": "Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years.",
        "trunc_text": "Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing ",
        "x1": 0.9819677472114563,
        "x2": 2.6258420944213867,
        "y1": 8.647748947143555,
        "y2": 0.4165792465209961
      },
      {
        "r": 0,
        "text": "Extensive experiments show the effectiveness of our method.",
        "trunc_text": "Extensive experiments show the effectiveness of our method.",
        "x1": 5.424071311950684,
        "x2": 6.478338718414307,
        "y1": 8.835214614868164,
        "y2": 4.813859939575195
      },
      {
        "r": 0,
        "text": "Extensive experiments demonstrate the effectiveness of our method.",
        "trunc_text": "Extensive experiments demonstrate the effectiveness of our method.",
        "x1": 5.418705940246582,
        "x2": 6.476978302001953,
        "y1": 8.878905296325684,
        "y2": 4.817477703094482
      },
      {
        "r": 0,
        "text": "We finally conduct extensive analyses to understand the effectiveness of our method.",
        "trunc_text": "We finally conduct extensive analyses to understand the effectiveness of our method.",
        "x1": 5.389307498931885,
        "x2": 6.476754188537598,
        "y1": 8.832921028137207,
        "y2": 4.791493892669678
      },
      {
        "r": 0,
        "text": "Extensive experiments show the effectiveness of our approach.",
        "trunc_text": "Extensive experiments show the effectiveness of our approach.",
        "x1": 5.426995277404785,
        "x2": 6.540741920471191,
        "y1": 8.876984596252441,
        "y2": 4.840846538543701
      },
      {
        "r": 0,
        "text": "A series of experiments demonstrate the effectiveness of our methods.",
        "trunc_text": "A series of experiments demonstrate the effectiveness of our methods.",
        "x1": 5.430721759796143,
        "x2": 6.540938377380371,
        "y1": 8.90621280670166,
        "y2": 4.866044044494629
      },
      {
        "r": 0,
        "text": "Our method is effective and presents a significant improvement over the original model.",
        "trunc_text": "Our method is effective and presents a significant improvement over the original model.",
        "x1": 5.2427496910095215,
        "x2": 6.192867279052734,
        "y1": 8.585819244384766,
        "y2": 4.62996244430542
      },
      {
        "r": 0,
        "text": "We show that our method produces highly accurate results.",
        "trunc_text": "We show that our method produces highly accurate results.",
        "x1": 5.311394214630127,
        "x2": 6.16495418548584,
        "y1": 8.383024215698242,
        "y2": 4.598179817199707
      },
      {
        "r": 0,
        "text": "Extensive experiments demonstrate that our method outperforms existing works significantly.",
        "trunc_text": "Extensive experiments demonstrate that our method outperforms existing works significantly.",
        "x1": 5.341052532196045,
        "x2": 6.349907398223877,
        "y1": 8.725980758666992,
        "y2": 4.741650104522705
      },
      {
        "r": 0,
        "text": "Our approach has several advantages over many previous methods.",
        "trunc_text": "Our approach has several advantages over many previous methods.",
        "x1": 5.251262664794922,
        "x2": 6.249590873718262,
        "y1": 8.58715534210205,
        "y2": 4.572030067443848
      },
      {
        "r": 0,
        "text": "Additionally, we employ the self-training strategy to improve the performance of our method further.",
        "trunc_text": "Additionally, we employ the self-training strategy to improve the performance of our method further.",
        "x1": 5.2130126953125,
        "x2": 6.099357604980469,
        "y1": 8.49622631072998,
        "y2": 4.6483659744262695
      },
      {
        "r": 0,
        "text": "our method has three advantages.",
        "trunc_text": "our method has three advantages.",
        "x1": 5.2930989265441895,
        "x2": 6.345295429229736,
        "y1": 8.668978691101074,
        "y2": 4.663156986236572
      },
      {
        "r": 0,
        "text": "Extensive experiments are conducted, and the results demonstrate the effectiveness and superiority of our method.",
        "trunc_text": "Extensive experiments are conducted, and the results demonstrate the effectiveness and superiority of our method.",
        "x1": 5.502634525299072,
        "x2": 6.549470901489258,
        "y1": 8.908892631530762,
        "y2": 4.90561056137085
      },
      {
        "r": 0,
        "text": "Comparison experiments demonstrate the effectiveness of our method.",
        "trunc_text": "Comparison experiments demonstrate the effectiveness of our method.",
        "x1": 5.482831001281738,
        "x2": 6.507266998291016,
        "y1": 8.880685806274414,
        "y2": 4.834045886993408
      },
      {
        "r": 0,
        "text": "Extensive experimental results confirm the effectiveness of our method.",
        "trunc_text": "Extensive experimental results confirm the effectiveness of our method.",
        "x1": 5.417093276977539,
        "x2": 6.4628400802612305,
        "y1": 8.8352632522583,
        "y2": 4.8144378662109375
      },
      {
        "r": 0,
        "text": "Our method consistently outperforms previous approaches across a range of tasks.",
        "trunc_text": "Our method consistently outperforms previous approaches across a range of tasks.",
        "x1": 5.2502570152282715,
        "x2": 6.1991753578186035,
        "y1": 8.599688529968262,
        "y2": 4.685171604156494
      },
      {
        "r": 0,
        "text": "Our method enjoys three key technical parts.",
        "trunc_text": "Our method enjoys three key technical parts.",
        "x1": 5.218067169189453,
        "x2": 6.258149147033691,
        "y1": 8.603974342346191,
        "y2": 4.537964820861816
      },
      {
        "r": 0,
        "text": "Our approach yields excellent results, with accuracies that are comparable to or better than those of existing state-of-the-art approaches.",
        "trunc_text": "Our approach yields excellent results, with accuracies that are comparable to or better than those of existing state-of-",
        "x1": 5.272773265838623,
        "x2": 6.237191200256348,
        "y1": 8.602726936340332,
        "y2": 4.643458843231201
      },
      {
        "r": 0,
        "text": "Empirical experiments demonstrate the efficacy of our approach.",
        "trunc_text": "Empirical experiments demonstrate the efficacy of our approach.",
        "x1": 5.435168266296387,
        "x2": 6.544554233551025,
        "y1": 8.889104843139648,
        "y2": 4.910224437713623
      },
      {
        "r": 0,
        "text": "Extensive experiments are conducted to demonstrate the effectiveness of our proposed method.",
        "trunc_text": "Extensive experiments are conducted to demonstrate the effectiveness of our proposed method.",
        "x1": 5.48548698425293,
        "x2": 6.524815559387207,
        "y1": 8.904767036437988,
        "y2": 4.885303497314453
      },
      {
        "r": 0,
        "text": "Our experimental results demonstrate the effectiveness of our proposed model, outperforming existing methods on three datasets.",
        "trunc_text": "Our experimental results demonstrate the effectiveness of our proposed model, outperforming existing methods on three da",
        "x1": 4.112587928771973,
        "x2": 5.041152000427246,
        "y1": 7.223169803619385,
        "y2": 2.966522455215454
      },
      {
        "r": 0,
        "text": "The method proposed here showed improvement from our precious results on the same dataset.",
        "trunc_text": "The method proposed here showed improvement from our precious results on the same dataset.",
        "x1": 4.038906097412109,
        "x2": 5.023953437805176,
        "y1": 7.210639476776123,
        "y2": 3.1487278938293457
      },
      {
        "r": 0,
        "text": "Experiments show that the proposed method achieves state-of-the-art results on the present dataset.",
        "trunc_text": "Experiments show that the proposed method achieves state-of-the-art results on the present dataset.",
        "x1": 4.110237121582031,
        "x2": 5.082458019256592,
        "y1": 7.317553520202637,
        "y2": 2.938161849975586
      },
      {
        "r": 0,
        "text": "Experiments conducted on two dedicated datasets demonstrate the superior performance of the proposed method over recent studies in the literature.",
        "trunc_text": "Experiments conducted on two dedicated datasets demonstrate the superior performance of the proposed method over recent ",
        "x1": 3.980912923812866,
        "x2": 5.175007343292236,
        "y1": 7.204545974731445,
        "y2": 3.1569578647613525
      },
      {
        "r": 0,
        "text": "Sufficient experiments demonstrate that the proposed method achieves results beyond the state-of-the-art methods on various datasets.",
        "trunc_text": "Sufficient experiments demonstrate that the proposed method achieves results beyond the state-of-the-art methods on vari",
        "x1": 4.1323981285095215,
        "x2": 5.044000625610352,
        "y1": 7.340811252593994,
        "y2": 2.839355945587158
      },
      {
        "r": 0,
        "text": "Extensive experiments demonstrate that benefiting from the proposed components, we achieve new state-of-the-art performance compared to previous methods on several public datasets.",
        "trunc_text": "Extensive experiments demonstrate that benefiting from the proposed components, we achieve new state-of-the-art performa",
        "x1": 4.3090691566467285,
        "x2": 5.002529621124268,
        "y1": 7.342731952667236,
        "y2": 2.8764803409576416
      },
      {
        "r": 0,
        "text": "Furthermore, on various datasets, we make competitive achievement results with other previous state-of-the-art methods.",
        "trunc_text": "Furthermore, on various datasets, we make competitive achievement results with other previous state-of-the-art methods.",
        "x1": 4.258006572723389,
        "x2": 4.630321979522705,
        "y1": 6.4041242599487305,
        "y2": 3.4626212120056152
      },
      {
        "r": 0,
        "text": "Overall, our work carefully studies the effectiveness of popular scoring functions in realistic settings and helps to better understand their limitations.",
        "trunc_text": "Overall, our work carefully studies the effectiveness of popular scoring functions in realistic settings and helps to be",
        "x1": 4.671756267547607,
        "x2": 5.2033305168151855,
        "y1": 6.146871089935303,
        "y2": 4.071773529052734
      },
      {
        "r": 0,
        "text": "We have evaluated our method on a set of supervised-learning datasets.",
        "trunc_text": "We have evaluated our method on a set of supervised-learning datasets.",
        "x1": 4.486971378326416,
        "x2": 5.243218898773193,
        "y1": 6.882030963897705,
        "y2": 3.1442370414733887
      },
      {
        "r": 0,
        "text": "Our method yields superior performance when dealing with scales outside of those covered by the training dataset.",
        "trunc_text": "Our method yields superior performance when dealing with scales outside of those covered by the training dataset.",
        "x1": 3.820554733276367,
        "x2": 4.833677291870117,
        "y1": 6.654434680938721,
        "y2": 3.4005370140075684
      },
      {
        "r": 0,
        "text": "The experimental results demonstrate that the proposed method can bring significant improvements in BLEU scores on two datasets.",
        "trunc_text": "The experimental results demonstrate that the proposed method can bring significant improvements in BLEU scores on two d",
        "x1": 3.9081859588623047,
        "x2": 4.899557590484619,
        "y1": 7.102908611297607,
        "y2": 3.2949748039245605
      },
      {
        "r": 0,
        "text": "Compared to a variety of baselines, our method achieves superior results.",
        "trunc_text": "Compared to a variety of baselines, our method achieves superior results.",
        "x1": 3.788222312927246,
        "x2": 4.869251728057861,
        "y1": 6.515990257263184,
        "y2": 3.9221460819244385
      },
      {
        "r": 0,
        "text": "Experiments on real-world datasets prove the significant effectiveness and generalization ability of the proposed method.",
        "trunc_text": "Experiments on real-world datasets prove the significant effectiveness and generalization ability of the proposed method",
        "x1": 4.162853240966797,
        "x2": 4.974782466888428,
        "y1": 7.355136394500732,
        "y2": 2.7049012184143066
      },
      {
        "r": 0,
        "text": "Extensive evaluations conducted on public datasets and comparison with other methods further confirm the effectiveness of the proposed framework.",
        "trunc_text": "Extensive evaluations conducted on public datasets and comparison with other methods further confirm the effectiveness o",
        "x1": 4.398134231567383,
        "x2": 5.2098517417907715,
        "y1": 7.282101631164551,
        "y2": 2.872638463973999
      },
      {
        "r": 0,
        "text": "Experiments on five challenging large-scale public datasets demonstrate that our proposed method is effective and outperforms the state-of-the-art methods.",
        "trunc_text": "Experiments on five challenging large-scale public datasets demonstrate that our proposed method is effective and outper",
        "x1": 4.521122455596924,
        "x2": 5.193345546722412,
        "y1": 7.432322025299072,
        "y2": 2.7949345111846924
      },
      {
        "r": 0,
        "text": "The experimental results on two datasets show the superiority of our methods.",
        "trunc_text": "The experimental results on two datasets show the superiority of our methods.",
        "x1": 3.8309597969055176,
        "x2": 5.050785541534424,
        "y1": 7.073171615600586,
        "y2": 3.2499499320983887
      },
      {
        "r": 0,
        "text": "Extensive experiments are conducted on seven widely used datasets to demonstrate the effectiveness of the proposed approach.",
        "trunc_text": "Extensive experiments are conducted on seven widely used datasets to demonstrate the effectiveness of the proposed appro",
        "x1": 4.251654624938965,
        "x2": 5.2490692138671875,
        "y1": 7.510896682739258,
        "y2": 2.955714225769043
      },
      {
        "r": 0,
        "text": "We validate our method on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a wider range of training budgets.",
        "trunc_text": "We validate our method on five datasets, empirically demonstrating that it outperforms the baseline methods in most case",
        "x1": 3.768892765045166,
        "x2": 4.897105693817139,
        "y1": 6.541119575500488,
        "y2": 3.5167949199676514
      },
      {
        "r": 0,
        "text": "Extensive experiments have been implemented on three real life datasets to demonstrate the effectiveness of our proposed algorithm.",
        "trunc_text": "Extensive experiments have been implemented on three real life datasets to demonstrate the effectiveness of our proposed",
        "x1": 4.162206172943115,
        "x2": 4.990808486938477,
        "y1": 7.339452266693115,
        "y2": 2.774472713470459
      },
      {
        "r": 0,
        "text": "Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show that our approach outperforms the state-of-the-art (SOTA) in four representative datasets, both qualitatively and quantitatively.",
        "trunc_text": "Extensive experiments are conducted to demonstrate the effectiveness of all the proposed components, and results show th",
        "x1": 4.293391227722168,
        "x2": 5.207531929016113,
        "y1": 7.472700119018555,
        "y2": 2.7605032920837402
      },
      {
        "r": 0,
        "text": "Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives.",
        "trunc_text": "Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives.",
        "x1": 3.8069820404052734,
        "x2": 4.9963555335998535,
        "y1": 6.965939044952393,
        "y2": 3.2794649600982666
      },
      {
        "r": 0,
        "text": "Extensive experiments with multiple datasets are conducted to demonstrate the effectiveness of our unified solution to all the three tasks and the generalized case.",
        "trunc_text": "Extensive experiments with multiple datasets are conducted to demonstrate the effectiveness of our unified solution to a",
        "x1": 3.9338204860687256,
        "x2": 5.1659770011901855,
        "y1": 7.093957424163818,
        "y2": 3.1097018718719482
      },
      {
        "r": 0,
        "text": "Extensive experiments conducted on three popular public datasets (CASIA-B, OU-MVLP, and GREW) demonstrate that our proposed method achieves state-of-the-art performance on multiple benchmark tests.",
        "trunc_text": "Extensive experiments conducted on three popular public datasets (CASIA-B, OU-MVLP, and GREW) demonstrate that our propo",
        "x1": 4.317679405212402,
        "x2": 4.681604862213135,
        "y1": 6.9570183753967285,
        "y2": 3.203829526901245
      },
      {
        "r": 0,
        "text": "We demonstrate that our method surpasses previous attempts in qualitative and quantitative results through extensive experiments conducted on benchmark datasets such as Set5, Set14, Urban100, BSD100, and Manga109.",
        "trunc_text": "We demonstrate that our method surpasses previous attempts in qualitative and quantitative results through extensive exp",
        "x1": 3.989809036254883,
        "x2": 4.619379043579102,
        "y1": 6.779125213623047,
        "y2": 3.293560743331909
      },
      {
        "r": 0,
        "text": "Through extensive experiments on three real datasets, we demonstrate the efficiency and effectiveness of our proposed algorithms.",
        "trunc_text": "Through extensive experiments on three real datasets, we demonstrate the efficiency and effectiveness of our proposed al",
        "x1": 4.011185169219971,
        "x2": 4.8665361404418945,
        "y1": 7.152401924133301,
        "y2": 2.89872407913208
      },
      {
        "r": 0,
        "text": "We show that our method performs superior to state of-the-art alternatives on various datasets.",
        "trunc_text": "We show that our method performs superior to state of-the-art alternatives on various datasets.",
        "x1": 4.318568706512451,
        "x2": 4.801798343658447,
        "y1": 7.0168657302856445,
        "y2": 2.95542573928833
      },
      {
        "r": 0,
        "text": "The statistical and experimental results indicate that our dataset provides the basis for the future improvements of existing methods.",
        "trunc_text": "The statistical and experimental results indicate that our dataset provides the basis for the future improvements of exi",
        "x1": 3.979877471923828,
        "x2": 5.112158298492432,
        "y1": 7.2310075759887695,
        "y2": 3.12343168258667
      },
      {
        "r": 0,
        "text": "Finally, extensive experimental results on three public datasets clearly demonstrate the effectiveness of the proposed model.",
        "trunc_text": "Finally, extensive experimental results on three public datasets clearly demonstrate the effectiveness of the proposed m",
        "x1": 4.365667343139648,
        "x2": 5.300597190856934,
        "y1": 7.41114616394043,
        "y2": 2.8867039680480957
      },
      {
        "r": 0,
        "text": "Experiment results on real-world datasets MLQA demonstrate that the proposed method can improve the performance by a large margin, outperforming the baseline method by 13.18%/12.00% F1/EM on average.",
        "trunc_text": "Experiment results on real-world datasets MLQA demonstrate that the proposed method can improve the performance by a lar",
        "x1": 3.8699791431427,
        "x2": 4.952946186065674,
        "y1": 6.450352191925049,
        "y2": 3.598702907562256
      },
      {
        "r": 0,
        "text": "Extensive experiments demonstrate that our method outperforms baselines on 16 out of 20 datasets, underlining its effectiveness and superiority in alleviating the heterophily problem.",
        "trunc_text": "Extensive experiments demonstrate that our method outperforms baselines on 16 out of 20 datasets, underlining its effect",
        "x1": 3.7266082763671875,
        "x2": 4.937880992889404,
        "y1": 6.735951900482178,
        "y2": 3.404794454574585
      },
      {
        "r": 0,
        "text": "Our methods are complementary to the existing pre-training or data mining approaches and can be used in a variety of settings.",
        "trunc_text": "Our methods are complementary to the existing pre-training or data mining approaches and can be used in a variety of set",
        "x1": 4.213247299194336,
        "x2": 5.135151386260986,
        "y1": 7.078679084777832,
        "y2": 3.2293379306793213
      },
      {
        "r": 0,
        "text": "Experimental results on two authoritative public datasets demonstrate that our proposed method boosts state-of-the-art performance by a large margin.",
        "trunc_text": "Experimental results on two authoritative public datasets demonstrate that our proposed method boosts state-of-the-art p",
        "x1": 4.48520040512085,
        "x2": 5.195577621459961,
        "y1": 7.367582321166992,
        "y2": 2.819915294647217
      },
      {
        "r": 0,
        "text": "Upon testing this framework on four distinct real datasets, we find that our synthetic training data are able to yield high-quality results also on real images-even more so if fine-tune on a few real images was done.",
        "trunc_text": "Upon testing this framework on four distinct real datasets, we find that our synthetic training data are able to yield h",
        "x1": 2.124607801437378,
        "x2": 3.4054114818573,
        "y1": 7.339916706085205,
        "y2": 1.7586380243301392
      },
      {
        "r": 0,
        "text": "We show that our framework significantly improves the quality of the resulting synthetic images and is adaptable to unseen data with fine-tuning.",
        "trunc_text": "We show that our framework significantly improves the quality of the resulting synthetic images and is adaptable to unse",
        "x1": 2.08431077003479,
        "x2": 3.4268112182617188,
        "y1": 7.445796966552734,
        "y2": 1.7463887929916382
      },
      {
        "r": 0,
        "text": "To this end, we generate two synthetic datasets and then develop an end-to-end pipeline and model that is tested on both benchmarks.",
        "trunc_text": "To this end, we generate two synthetic datasets and then develop an end-to-end pipeline and model that is tested on both",
        "x1": 5.115459442138672,
        "x2": 5.093119144439697,
        "y1": 6.924538612365723,
        "y2": 2.275073766708374
      },
      {
        "r": 0,
        "text": "The results on the kgbench dataset with three different embedding methods show promising results.",
        "trunc_text": "The results on the kgbench dataset with three different embedding methods show promising results.",
        "x1": 3.036440849304199,
        "x2": 2.612931966781616,
        "y1": 4.779689311981201,
        "y2": 4.065341472625732
      },
      {
        "r": 0,
        "text": "We present our findings that these embeddings are useful beyond the noise prediction task, as they contain discriminative information and can also be leveraged for classification.",
        "trunc_text": "We present our findings that these embeddings are useful beyond the noise prediction task, as they contain discriminativ",
        "x1": 2.9489543437957764,
        "x2": 2.5739238262176514,
        "y1": 4.900454998016357,
        "y2": 3.9123189449310303
      },
      {
        "r": 0,
        "text": "Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible.",
        "trunc_text": "Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-co",
        "x1": 2.972165107727051,
        "x2": 2.660337448120117,
        "y1": 4.939708709716797,
        "y2": 3.9229447841644287
      },
      {
        "r": 0,
        "text": "Our results show that the inherent data bias that persists in KG can be altered by specific algorithm bias as incorporated by KG embedding learning algorithms.",
        "trunc_text": "Our results show that the inherent data bias that persists in KG can be altered by specific algorithm bias as incorporat",
        "x1": 3.025143623352051,
        "x2": 2.68481183052063,
        "y1": 4.843538761138916,
        "y2": 3.9265685081481934
      },
      {
        "r": 0,
        "text": "To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA).",
        "trunc_text": "To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an Eng",
        "x1": 3.5447564125061035,
        "x2": 2.6853902339935303,
        "y1": 3.4962499141693115,
        "y2": 5.6867756843566895
      },
      {
        "r": 0,
        "text": "Most existing graph embedding methods fall short of reaching high data scalability.",
        "trunc_text": "Most existing graph embedding methods fall short of reaching high data scalability.",
        "x1": 3.0278501510620117,
        "x2": 2.7803986072540283,
        "y1": 5.024942398071289,
        "y2": 3.8891522884368896
      },
      {
        "r": 0,
        "text": "In this paper, we present a large-scale human-labeled dataset for Russian text recognition in-the-wild.",
        "trunc_text": "In this paper, we present a large-scale human-labeled dataset for Russian text recognition in-the-wild.",
        "x1": 3.8932554721832275,
        "x2": 2.386777400970459,
        "y1": 2.443570613861084,
        "y2": 6.227955341339111
      },
      {
        "r": 0,
        "text": "Experiments are conducted on two commonly used KGET datasets to show that the performance of KGE methods on the KGET task can be substantially improved by the proposed multiple auxiliary relations and asynchronous embedding learning.",
        "trunc_text": "Experiments are conducted on two commonly used KGET datasets to show that the performance of KGE methods on the KGET tas",
        "x1": 3.1180529594421387,
        "x2": 2.7854294776916504,
        "y1": 4.8220720291137695,
        "y2": 3.995760440826416
      },
      {
        "r": 0,
        "text": "Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time.",
        "trunc_text": "Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image emb",
        "x1": 3.152838945388794,
        "x2": 2.566274881362915,
        "y1": 4.606889247894287,
        "y2": 4.293819427490234
      },
      {
        "r": 0,
        "text": "However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution.",
        "trunc_text": "However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may l",
        "x1": 2.991805076599121,
        "x2": 2.724675178527832,
        "y1": 4.989029407501221,
        "y2": 3.8713152408599854
      },
      {
        "r": 0,
        "text": "External Knowledge Injection:} By fine-tuning special token embeddings for each category via Textual Inversion, performance improves across 17 datasets, except when dealing with low-resolution reference images.   ",
        "trunc_text": "External Knowledge Injection:} By fine-tuning special token embeddings for each category via Textual Inversion, performa",
        "x1": 3.0022876262664795,
        "x2": 2.651552438735962,
        "y1": 4.996921539306641,
        "y2": 3.93192195892334
      },
      {
        "r": 0,
        "text": "However, existing embedding architectures suffer from two limitations: (1) limited discriminability of synthetic features' embedding without considering fine-grained cluster structures; (2) inflexible optimization due to restricted scaling mechanisms on existing contrastive embedding networks, leading to overlapped representations in the embedding space.",
        "trunc_text": "However, existing embedding architectures suffer from two limitations: (1) limited discriminability of synthetic feature",
        "x1": 2.923417568206787,
        "x2": 2.6304495334625244,
        "y1": 4.958296775817871,
        "y2": 3.892054319381714
      },
      {
        "r": 0,
        "text": "Recent works in this domain have achieved significant improvements by the representation learning paradigm, e.g., embedding-based retrieval (EBR) and collaborative filtering (CF).",
        "trunc_text": "Recent works in this domain have achieved significant improvements by the representation learning paradigm, e.g., embedd",
        "x1": 3.029878854751587,
        "x2": 2.6079487800598145,
        "y1": 4.842162609100342,
        "y2": 4.061474800109863
      },
      {
        "r": 0,
        "text": "We compare these embeddings to those generated by competing architectures and pre-trainings for classification tasks.",
        "trunc_text": "We compare these embeddings to those generated by competing architectures and pre-trainings for classification tasks.",
        "x1": 2.8691163063049316,
        "x2": 2.551071882247925,
        "y1": 4.957101821899414,
        "y2": 3.910416841506958
      },
      {
        "r": 0,
        "text": "While there are many high-quality datasets for English text recognition; there are no available datasets for Russian language.",
        "trunc_text": "While there are many high-quality datasets for English text recognition; there are no available datasets for Russian lan",
        "x1": 3.943770408630371,
        "x2": 2.3082892894744873,
        "y1": 2.353123903274536,
        "y2": 6.272478103637695
      },
      {
        "r": 0,
        "text": "In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets.",
        "trunc_text": "In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and",
        "x1": 3.549633502960205,
        "x2": 2.610839605331421,
        "y1": 3.5623505115509033,
        "y2": 5.463571548461914
      },
      {
        "r": 0,
        "text": "Moreover, we compared the 3RL dataset with other famous state-of-the-art datasets (FER dataset, CK+ dataset), and we applied the most commonly used algorithms in previous works, SVM and CNN.",
        "trunc_text": "Moreover, we compared the 3RL dataset with other famous state-of-the-art datasets (FER dataset, CK+ dataset), and we app",
        "x1": 4.425362586975098,
        "x2": 4.841436862945557,
        "y1": 7.170141696929932,
        "y2": 2.8238372802734375
      },
      {
        "r": 0,
        "text": "We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-quality conversation data sets.",
        "trunc_text": "We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-q",
        "x1": 6.018871784210205,
        "x2": 4.372015953063965,
        "y1": 3.5761735439300537,
        "y2": 6.760919570922852
      },
      {
        "r": 0,
        "text": "This analysis also investigates the potential of utilizing cooperation LLMs to generate high-quality datasets.",
        "trunc_text": "This analysis also investigates the potential of utilizing cooperation LLMs to generate high-quality datasets.",
        "x1": 6.252728462219238,
        "x2": 5.038496017456055,
        "y1": 4.765538215637207,
        "y2": 6.168155670166016
      },
      {
        "r": 0,
        "text": "This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023 shared task for Task-A and Task-C.  ",
        "trunc_text": "This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023 shared task for Task-A and Task-C.  ",
        "x1": 6.250802516937256,
        "x2": 4.406583786010742,
        "y1": 3.6468863487243652,
        "y2": 6.829756259918213
      },
      {
        "r": 0,
        "text": "The experiment results demonstrate that our approaches yield reasonable performance as evaluated by automatic metrics such as ROUGE, medical concept recall, BLEU, and Self-BLEU.",
        "trunc_text": "The experiment results demonstrate that our approaches yield reasonable performance as evaluated by automatic metrics su",
        "x1": 3.3588898181915283,
        "x2": 4.29788064956665,
        "y1": 6.572851657867432,
        "y2": 3.0980865955352783
      },
      {
        "r": 0,
        "text": "Furthermore, we conducted a comparative analysis between our proposed method and ChatGPT and GPT-4.",
        "trunc_text": "Furthermore, we conducted a comparative analysis between our proposed method and ChatGPT and GPT-4.",
        "x1": 6.563868522644043,
        "x2": 4.664177417755127,
        "y1": 3.605611801147461,
        "y2": 6.957191467285156
      },
      {
        "r": 0,
        "text": "We present working notes on transfer learning with semi-supervised dataset annotation for the BirdCLEF 2023 competition, focused on identifying African bird species in recorded soundscapes",
        "trunc_text": "We present working notes on transfer learning with semi-supervised dataset annotation for the BirdCLEF 2023 competition,",
        "x1": 2.162933111190796,
        "x2": 2.105198860168457,
        "y1": 5.030372142791748,
        "y2": 3.640779972076416
      },
      {
        "r": 0,
        "text": "We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised learning",
        "trunc_text": "We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised le",
        "x1": 2.2260169982910156,
        "x2": 2.2544078826904297,
        "y1": 5.029632091522217,
        "y2": 3.657935857772827
      },
      {
        "r": 0,
        "text": "Our approach utilizes existing off-the-shelf models, BirdNET and MixIT, to address representation and labeling challenges in the competition.",
        "trunc_text": "Our approach utilizes existing off-the-shelf models, BirdNET and MixIT, to address representation and labeling challenge",
        "x1": 2.1674740314483643,
        "x2": 2.07669997215271,
        "y1": 4.93810510635376,
        "y2": 3.7012646198272705
      },
      {
        "r": 0,
        "text": "We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised learning.",
        "trunc_text": "We explore the embedding space learned by BirdNET and propose a process to derive an annotated dataset for supervised le",
        "x1": 2.2399661540985107,
        "x2": 2.2345213890075684,
        "y1": 5.088099002838135,
        "y2": 3.6601362228393555
      },
      {
        "r": 0,
        "text": "Our experiments involve various models and feature engineerih in classifying bird species and highlight the potential of transfer learning and semi-supervised dataset annotation in similar tasks.",
        "trunc_text": "Our experiments involve various models and feature engineerih in classifying bird species and highlight the potential of",
        "x1": 2.086001396179199,
        "x2": 2.132211685180664,
        "y1": 5.159111022949219,
        "y2": 3.5227150917053223
      },
      {
        "r": 0,
        "text": "Our data generator is capable of generating large-scale datasets of human activities",
        "trunc_text": "Our data generator is capable of generating large-scale datasets of human activities",
        "x1": 2.786487340927124,
        "x2": 4.117367744445801,
        "y1": 9.570771217346191,
        "y2": -0.4126298725605011
      },
      {
        "r": 0,
        "text": "We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with larger group sizes and higher complexity of inter-person interactions than previous multi-person datasets",
        "trunc_text": "We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with larger group sizes and higher complexity of ",
        "x1": 2.592447519302368,
        "x2": 4.00492000579834,
        "y1": 9.435103416442871,
        "y2": -0.4988120198249817
      },
      {
        "r": 0,
        "text": "The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision.",
        "trunc_text": "The understanding of complex human interactions and group activities has garnered attention in human-centric computer vi",
        "x1": 2.6663737297058105,
        "x2": 4.057621002197266,
        "y1": 9.287063598632812,
        "y2": -0.423807829618454
      },
      {
        "r": 0,
        "text": "However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets.",
        "trunc_text": "However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-wo",
        "x1": 5.496528625488281,
        "x2": 6.026885986328125,
        "y1": 6.82949161529541,
        "y2": 2.9691905975341797
      },
      {
        "r": 0,
        "text": "To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator.",
        "trunc_text": "To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity da",
        "x1": 2.64632511138916,
        "x2": 3.999894142150879,
        "y1": 9.430965423583984,
        "y2": -0.5631753206253052
      },
      {
        "r": 0,
        "text": "Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process.  ",
        "trunc_text": "Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camer",
        "x1": 2.469332456588745,
        "x2": 3.994091033935547,
        "y1": 9.46986198425293,
        "y2": -0.43904542922973633
      },
      {
        "r": 0,
        "text": "with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories).",
        "trunc_text": "with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual per",
        "x1": 2.325371026992798,
        "x2": 3.873729944229126,
        "y1": 9.31413745880127,
        "y2": -0.21416115760803223
      },
      {
        "r": 0,
        "text": "Using M3Act, we perform synthetic data pre-training for 2D skeleton-based group activity recognition and RGB-based multi-person pose tracking.",
        "trunc_text": "Using M3Act, we perform synthetic data pre-training for 2D skeleton-based group activity recognition and RGB-based multi",
        "x1": 2.5062901973724365,
        "x2": 3.9741828441619873,
        "y1": 9.409811973571777,
        "y2": -0.48880383372306824
      },
      {
        "r": 0,
        "text": "The results indicate that learning from our synthetic datasets largely improves the model performances on real-world datasets, with the highest gain of 5.59% and 7.32% respectively in group and person recognition accuracy on CAD2, as well as an improvement of 6.63 in MOTP on HiEve.",
        "trunc_text": "The results indicate that learning from our synthetic datasets largely improves the model performances on real-world dat",
        "x1": 4.225128650665283,
        "x2": 4.879679203033447,
        "y1": 7.390687942504883,
        "y2": 2.3207693099975586
      },
      {
        "r": 0,
        "text": "Pre-training with our synthetic data also leads to faster model convergence on downstream tasks (up to 6.8% faster).",
        "trunc_text": "Pre-training with our synthetic data also leads to faster model convergence on downstream tasks (up to 6.8% faster).",
        "x1": 4.150049686431885,
        "x2": 4.074187278747559,
        "y1": 5.446033000946045,
        "y2": 4.480593204498291
      },
      {
        "r": 0,
        "text": "Moreover, M3Act opens new research problems for 3D group activity generation.",
        "trunc_text": "Moreover, M3Act opens new research problems for 3D group activity generation.",
        "x1": 2.5635488033294678,
        "x2": 4.011380672454834,
        "y1": 9.468482971191406,
        "y2": -0.5403524041175842
      },
      {
        "r": 0,
        "text": "We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with larger g",
        "trunc_text": "We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with larger g",
        "x1": 2.5727145671844482,
        "x2": 3.984130859375,
        "y1": 9.47523307800293,
        "y2": -0.5581693649291992
      },
      {
        "r": 0,
        "text": "Large language models typically undergo two training stages, pretraining and finetuning.",
        "trunc_text": "Large language models typically undergo two training stages, pretraining and finetuning.",
        "x1": 4.917210102081299,
        "x2": 3.790039300918579,
        "y1": 3.958649158477783,
        "y2": 5.975822925567627
      },
      {
        "r": 0,
        "text": "Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times.",
        "trunc_text": "Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, t",
        "x1": 4.910480976104736,
        "x2": 3.784635543823242,
        "y1": 3.9670026302337646,
        "y2": 5.896275520324707
      },
      {
        "r": 0,
        "text": "To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area.",
        "trunc_text": "To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged a",
        "x1": 5.171701908111572,
        "x2": 4.016550540924072,
        "y1": 4.0487470626831055,
        "y2": 6.119839191436768
      },
      {
        "r": 0,
        "text": "Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data.",
        "trunc_text": "Recent studies found that large language models can be finetuned to perform well even with a small amount of high-qualit",
        "x1": 5.197983264923096,
        "x2": 3.870387554168701,
        "y1": 4.068391799926758,
        "y2": 6.117146015167236
      },
      {
        "r": 0,
        "text": "However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow.",
        "trunc_text": "However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow.",
        "x1": 4.652957916259766,
        "x2": 3.0287280082702637,
        "y1": 3.686108350753784,
        "y2": 6.052118301391602
      },
      {
        "r": 0,
        "text": "In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality.",
        "trunc_text": "In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality.",
        "x1": 5.431869029998779,
        "x2": 4.49833869934082,
        "y1": 4.397421360015869,
        "y2": 5.9687299728393555
      },
      {
        "r": 0,
        "text": "We formulate InstructMining using specific natural language indicators.",
        "trunc_text": "We formulate InstructMining using specific natural language indicators.",
        "x1": 3.0125606060028076,
        "x2": 1.7183345556259155,
        "y1": 3.330672264099121,
        "y2": 5.833315849304199
      },
      {
        "r": 0,
        "text": "To investigate the relationship between data quality and these indicators, we further conduct extensive finetuning experiments.",
        "trunc_text": "To investigate the relationship between data quality and these indicators, we further conduct extensive finetuning exper",
        "x1": 5.074489593505859,
        "x2": 5.544902801513672,
        "y1": 5.979365825653076,
        "y2": 3.767913818359375
      },
      {
        "r": 0,
        "text": "The experiment results are then applied to estimating parameters in InstructMining.",
        "trunc_text": "The experiment results are then applied to estimating parameters in InstructMining.",
        "x1": 5.435527801513672,
        "x2": 6.566359519958496,
        "y1": 8.92174243927002,
        "y2": 4.819018840789795
      },
      {
        "r": 0,
        "text": "To further investigate its performance, we use InstructMining to select high-quality data from unseen datasets.",
        "trunc_text": "To further investigate its performance, we use InstructMining to select high-quality data from unseen datasets.",
        "x1": 5.244197368621826,
        "x2": 5.368309020996094,
        "y1": 6.15904426574707,
        "y2": 3.541891098022461
      },
      {
        "r": 0,
        "text": "Results demonstrate that InstructMining can help select relatively high-quality samples from various instruction-following datasets.",
        "trunc_text": "Results demonstrate that InstructMining can help select relatively high-quality samples from various instruction-followi",
        "x1": 5.497766494750977,
        "x2": 4.629338264465332,
        "y1": 4.5428009033203125,
        "y2": 5.824027061462402
      },
      {
        "r": 0,
        "text": "Compared to models finetuned on unfiltered datasets, models finetuned on InstructMining selected datasets perform better on 42.5% cases.",
        "trunc_text": "Compared to models finetuned on unfiltered datasets, models finetuned on InstructMining selected datasets perform better",
        "x1": 4.918791770935059,
        "x2": 5.154904842376709,
        "y1": 6.029804706573486,
        "y2": 3.698531150817871
      },
      {
        "r": 0,
        "text": "Therefore, we introduce the Infrastructural Multi-Person Trajectory and Context Dataset (IMPTC).",
        "trunc_text": "Therefore, we introduce the Infrastructural Multi-Person Trajectory and Context Dataset (IMPTC).",
        "x1": 2.9865074157714844,
        "x2": 4.619389057159424,
        "y1": 9.677624702453613,
        "y2": -0.4754670262336731
      },
      {
        "r": 0,
        "text": "The resulting dataset consists of eight hours of measurement data",
        "trunc_text": "The resulting dataset consists of eight hours of measurement data",
        "x1": 5.605084419250488,
        "x2": 6.20415735244751,
        "y1": 7.690568923950195,
        "y2": 2.1184117794036865
      },
      {
        "r": 0,
        "text": "In addition, to enable the entire stack of research capabilities, the dataset includes all data, starting from the sensor-, calibration- and detection data until trajectory and context data.",
        "trunc_text": "In addition, to enable the entire stack of research capabilities, the dataset includes all data, starting from the senso",
        "x1": 5.904175758361816,
        "x2": 6.541123867034912,
        "y1": 7.785407066345215,
        "y2": 1.988051176071167
      },
      {
        "r": 0,
        "text": "The dataset is continuously expanded and is available online for non-commercial research at https://github.com/kav-institute/imptc-dataset.",
        "trunc_text": "The dataset is continuously expanded and is available online for non-commercial research at https://github.com/kav-insti",
        "x1": 6.834771633148193,
        "x2": 7.282679080963135,
        "y1": 7.79609489440918,
        "y2": 2.018629550933838
      },
      {
        "r": 0,
        "text": "Inner-city intersections are among the most critical traffic areas for injury and fatal accidents.",
        "trunc_text": "Inner-city intersections are among the most critical traffic areas for injury and fatal accidents.",
        "x1": 3.5665056705474854,
        "x2": 5.199611663818359,
        "y1": 9.979997634887695,
        "y2": 0.07223940640687943
      },
      {
        "r": 0,
        "text": "Automated vehicles struggle with the complex and hectic everyday life within those areas.",
        "trunc_text": "Automated vehicles struggle with the complex and hectic everyday life within those areas.",
        "x1": 3.716259717941284,
        "x2": 5.326321125030518,
        "y1": 9.838529586791992,
        "y2": 0.09008942544460297
      },
      {
        "r": 0,
        "text": "Sensor-equipped smart infrastructures, which can cooperate with vehicles, can benefit automated traffic by extending the perception capabilities of drivers and vehicle perception systems.",
        "trunc_text": "Sensor-equipped smart infrastructures, which can cooperate with vehicles, can benefit automated traffic by extending the",
        "x1": 3.572441339492798,
        "x2": 5.176900863647461,
        "y1": 9.982063293457031,
        "y2": -0.0304231196641922
      },
      {
        "r": 0,
        "text": "Additionally, they offer the opportunity to gather reproducible and precise data of a holistic scene understanding, including context information as a basis for training algorithms for various applications in automated traffic.  ",
        "trunc_text": "Additionally, they offer the opportunity to gather reproducible and precise data of a holistic scene understanding, incl",
        "x1": 1.557003140449524,
        "x2": 3.109656572341919,
        "y1": 8.040169715881348,
        "y2": 1.1857686042785645
      },
      {
        "r": 0,
        "text": "We use an intelligent public inner-city intersection in Germany with visual sensor technology.",
        "trunc_text": "We use an intelligent public inner-city intersection in Germany with visual sensor technology.",
        "x1": 3.4829747676849365,
        "x2": 5.0768561363220215,
        "y1": 10.018143653869629,
        "y2": 0.07635476440191269
      },
      {
        "r": 0,
        "text": "A multi-view camera and LiDAR system perceives traffic situations and road users' behavior.",
        "trunc_text": "A multi-view camera and LiDAR system perceives traffic situations and road users' behavior.",
        "x1": 3.4753305912017822,
        "x2": 5.072114944458008,
        "y1": 9.996795654296875,
        "y2": -0.03529783710837364
      },
      {
        "r": 0,
        "text": "Additional sensors monitor contextual information like weather, lighting, and traffic light signal status.",
        "trunc_text": "Additional sensors monitor contextual information like weather, lighting, and traffic light signal status.",
        "x1": 3.4513840675354004,
        "x2": 4.999377727508545,
        "y1": 9.711030960083008,
        "y2": 0.15808480978012085
      },
      {
        "r": 0,
        "text": "The data acquisition system focuses on Vulnerable Road Users (VRUs) and multi-agent interaction.",
        "trunc_text": "The data acquisition system focuses on Vulnerable Road Users (VRUs) and multi-agent interaction.",
        "x1": 3.6735761165618896,
        "x2": 5.314540386199951,
        "y1": 10.058323860168457,
        "y2": -0.07913412153720856
      },
      {
        "r": 0,
        "text": "The resulting dataset consists of eight hours of measurement data.",
        "trunc_text": "The resulting dataset consists of eight hours of measurement data.",
        "x1": 5.610365867614746,
        "x2": 6.183761119842529,
        "y1": 7.780542373657227,
        "y2": 2.1437478065490723
      },
      {
        "r": 0,
        "text": "It contains over 2,500 VRU trrollers, and wheelchair users, and over 20,000 vehicle trajectories at different day times, weather conditions, and seasons.",
        "trunc_text": "It contains over 2,500 VRU trrollers, and wheelchair users, and over 20,000 vehicle trajectories at different day times,",
        "x1": 3.5694777965545654,
        "x2": 5.036630630493164,
        "y1": 9.869826316833496,
        "y2": -0.309972882270813
      },
      {
        "r": 0,
        "text": "In addition, to enable the entire stack of research capabilities, the dataset includes all data, starting from the sensor-, calibration- and detection data until",
        "trunc_text": "In addition, to enable the entire stack of research capabilities, the dataset includes all data, starting from the senso",
        "x1": 6.141182899475098,
        "x2": 6.635809421539307,
        "y1": 7.67038631439209,
        "y2": 2.0069453716278076
      },
      {
        "r": 0,
        "text": "Noisy label problems are inevitably in existence within medical image segmentation causing severe performance degradation.",
        "trunc_text": "Noisy label problems are inevitably in existence within medical image segmentation causing severe performance degradatio",
        "x1": 0.9464175701141357,
        "x2": 1.0641160011291504,
        "y1": 5.456337928771973,
        "y2": 3.0373528003692627
      },
      {
        "r": 0,
        "text": "Previous segmentation methods for noisy label problems only utilize a single image while the potential of leveraging the correlation between images has been overlooked.",
        "trunc_text": "Previous segmentation methods for noisy label problems only utilize a single image while the potential of leveraging the",
        "x1": 0.866730272769928,
        "x2": 0.9991656541824341,
        "y1": 5.441632270812988,
        "y2": 2.9498956203460693
      },
      {
        "r": 0,
        "text": "Especially for video segmentation, adjacent frames contain rich contextual information beneficial in cognizing noisy labels.",
        "trunc_text": "Especially for video segmentation, adjacent frames contain rich contextual information beneficial in cognizing noisy lab",
        "x1": 0.895139753818512,
        "x2": 1.1007071733474731,
        "y1": 5.563348293304443,
        "y2": 2.8653016090393066
      },
      {
        "r": 0,
        "text": "Based on two insights, we propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to resolve noisy-labeled medical video segmentation issues.",
        "trunc_text": "Based on two insights, we propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to resolve noisy-",
        "x1": 0.9135085344314575,
        "x2": 1.2215046882629395,
        "y1": 5.716286659240723,
        "y2": 2.7263386249542236
      },
      {
        "r": 0,
        "text": "First, we argue the sequential prior of videos is an effective reference, i.e., pixel-level features from adjacent frames are close in distance for the same class and far in distance otherwise.",
        "trunc_text": "First, we argue the sequential prior of videos is an effective reference, i.e., pixel-level features from adjacent frame",
        "x1": 1.0724292993545532,
        "x2": 2.635568857192993,
        "y1": 8.508522987365723,
        "y2": 0.643443763256073
      },
      {
        "r": 0,
        "text": "Therefore, Temporal Feature Affinity Learning (TFAL) is devised to indicate possible noisy labels by evaluating the affinity between pixels in two adjacent frames.",
        "trunc_text": "Therefore, Temporal Feature Affinity Learning (TFAL) is devised to indicate possible noisy labels by evaluating the affi",
        "x1": 0.9560989141464233,
        "x2": 1.126346230506897,
        "y1": 5.542644500732422,
        "y2": 2.889822483062744
      },
      {
        "r": 0,
        "text": "We also notice that the noise distribution exhibits considerable variations across video, image, and pixel levels.",
        "trunc_text": "We also notice that the noise distribution exhibits considerable variations across video, image, and pixel levels.",
        "x1": 0.9772345423698425,
        "x2": 1.1663137674331665,
        "y1": 5.64101505279541,
        "y2": 2.867790699005127
      },
      {
        "r": 0,
        "text": "In this way, we introduce Multi-Scale Supervision (MSS) to supervise the network from three different perspectives by re-weighting and refining the samples.",
        "trunc_text": "In this way, we introduce Multi-Scale Supervision (MSS) to supervise the network from three different perspectives by re",
        "x1": 1.903286337852478,
        "x2": 2.088320016860962,
        "y1": 5.377200603485107,
        "y2": 3.4827675819396973
      },
      {
        "r": 0,
        "text": "This design enables the network to concentrate on clean samples in a coarse-to-fine manner.",
        "trunc_text": "This design enables the network to concentrate on clean samples in a coarse-to-fine manner.",
        "x1": 2.0071043968200684,
        "x2": 2.323378562927246,
        "y1": 5.818934440612793,
        "y2": 3.130730152130127
      },
      {
        "r": 0,
        "text": "Experiments with both synthetic and real-world label noise demonstrate that our method outperforms recent state-of-the-art robust segmentation approaches.",
        "trunc_text": "Experiments with both synthetic and real-world label noise demonstrate that our method outperforms recent state-of-the-a",
        "x1": 0.8799351453781128,
        "x2": 0.9753754138946533,
        "y1": 5.376552104949951,
        "y2": 3.078078031539917
      },
      {
        "r": 0,
        "text": "Code is available at https://github.com/BeileiCui/MS-TFAL.",
        "trunc_text": "Code is available at https://github.com/BeileiCui/MS-TFAL.",
        "x1": 7.98348331451416,
        "x2": 8.709623336791992,
        "y1": 7.633321762084961,
        "y2": 2.271672487258911
      },
      {
        "r": 0,
        "text": "This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario.",
        "trunc_text": "This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-bo",
        "x1": 7.266915798187256,
        "x2": 6.552902698516846,
        "y1": 5.55585241317749,
        "y2": 5.850077152252197
      },
      {
        "r": 0,
        "text": "The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining the backdoored network output than any other benign features.",
        "trunc_text": "The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in d",
        "x1": 7.317593097686768,
        "x2": 6.614346027374268,
        "y1": 5.567866325378418,
        "y2": 5.783670425415039
      },
      {
        "r": 0,
        "text": "To quantitatively measure the effects of triggers and benign features on determining the backdoored network output, we introduce five metrics.",
        "trunc_text": "To quantitatively measure the effects of triggers and benign features on determining the backdoored network output, we i",
        "x1": 7.3808441162109375,
        "x2": 6.598160743713379,
        "y1": 5.541706562042236,
        "y2": 5.857418537139893
      },
      {
        "r": 0,
        "text": "To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the input's partial contents into clean validation samples.",
        "trunc_text": "To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the inpu",
        "x1": 4.044143199920654,
        "x2": 4.9120097160339355,
        "y1": 6.00663423538208,
        "y2": 3.581163167953491
      },
      {
        "r": 0,
        "text": "Then, the five metrics are computed by using the output labels of the corresponding synthetic samples.",
        "trunc_text": "Then, the five metrics are computed by using the output labels of the corresponding synthetic samples.",
        "x1": 4.081056594848633,
        "x2": 4.9705376625061035,
        "y1": 6.050243377685547,
        "y2": 3.5865092277526855
      },
      {
        "r": 0,
        "text": "One contribution of this work is the use of a tiny clean validation dataset.",
        "trunc_text": "One contribution of this work is the use of a tiny clean validation dataset.",
        "x1": 4.97360897064209,
        "x2": 5.275290012359619,
        "y1": 6.900369167327881,
        "y2": 3.1481094360351562
      },
      {
        "r": 0,
        "text": "Having the computed five metrics, five novelty detectors are trained from the validation dataset.",
        "trunc_text": "Having the computed five metrics, five novelty detectors are trained from the validation dataset.",
        "x1": 1.8484017848968506,
        "x2": 2.515862464904785,
        "y1": 6.102745056152344,
        "y2": 2.337350368499756
      },
      {
        "r": 0,
        "text": "A meta novelty detector fuses the output of the five trained novelty detectors to generate a meta confidence score.",
        "trunc_text": "A meta novelty detector fuses the output of the five trained novelty detectors to generate a meta confidence score.",
        "x1": 2.027658462524414,
        "x2": 3.13950514793396,
        "y1": 5.951542377471924,
        "y2": 2.804408311843872
      },
      {
        "r": 0,
        "text": "During online testing, our method determines if online samples are poisoned or not via assessing their meta confidence scores output by the meta novelty detector.",
        "trunc_text": "During online testing, our method determines if online samples are poisoned or not via assessing their meta confidence s",
        "x1": 2.169438123703003,
        "x2": 3.478569269180298,
        "y1": 5.991618633270264,
        "y2": 2.9046385288238525
      },
      {
        "r": 0,
        "text": "We show the efficacy of our methodology through a broad range of backdoor attacks, including ablation studies and comparison to existing approaches.",
        "trunc_text": "We show the efficacy of our methodology through a broad range of backdoor attacks, including ablation studies and compar",
        "x1": 7.3221564292907715,
        "x2": 6.66100549697876,
        "y1": 5.755224704742432,
        "y2": 5.441906452178955
      },
      {
        "r": 0,
        "text": "Our methodology is promising since the proposed five metrics quantify the inherent differences between clean and poisoned samples.",
        "trunc_text": "Our methodology is promising since the proposed five metrics quantify the inherent differences between clean and poisone",
        "x1": 5.155832767486572,
        "x2": 5.281312465667725,
        "y1": 8.548938751220703,
        "y2": 3.8257551193237305
      },
      {
        "r": 0,
        "text": "Additionally, our detection method can be incrementally improved by appending more metrics that may be proposed to address future advanced attacks.",
        "trunc_text": "Additionally, our detection method can be incrementally improved by appending more metrics that may be proposed to addre",
        "x1": 7.3764166831970215,
        "x2": 6.6352996826171875,
        "y1": 5.487698078155518,
        "y2": 5.909052848815918
      },
      {
        "r": 0,
        "text": "To address this limitation, we have developed a synthetic dataset for short-term trajectory prediction tasks using the CARLA simulator.",
        "trunc_text": "To address this limitation, we have developed a synthetic dataset for short-term trajectory prediction tasks using the C",
        "x1": 3.3604371547698975,
        "x2": 4.983724594116211,
        "y1": 9.874434471130371,
        "y2": -0.45361971855163574
      },
      {
        "r": 0,
        "text": "This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking - and comprises 6000 perspective view images with corresponding IMU and odometry information for each frame.",
        "trunc_text": "This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicle",
        "x1": 3.4263927936553955,
        "x2": 4.918898105621338,
        "y1": 9.912771224975586,
        "y2": -0.20663675665855408
      },
      {
        "r": 0,
        "text": "Our datasets are publicly available on https://github.com/navigatinguncertainty.",
        "trunc_text": "Our datasets are publicly available on https://github.com/navigatinguncertainty.",
        "x1": 7.295665740966797,
        "x2": 7.588606834411621,
        "y1": 7.941954612731934,
        "y2": 1.9014531373977661
      },
      {
        "r": 0,
        "text": "Autonomous vehicles require accurate and reliable short-term trajectory predictions for safe and efficient driving.",
        "trunc_text": "Autonomous vehicles require accurate and reliable short-term trajectory predictions for safe and efficient driving.",
        "x1": 3.3967206478118896,
        "x2": 4.976822376251221,
        "y1": 9.872013092041016,
        "y2": -0.45711472630500793
      },
      {
        "r": 0,
        "text": "While most commercial automated vehicles currently use state machine-based algorithms for trajectory forecasting, recent efforts have focused on end-to-end data-driven systems.",
        "trunc_text": "While most commercial automated vehicles currently use state machine-based algorithms for trajectory forecasting, recent",
        "x1": 3.457207441329956,
        "x2": 5.074300765991211,
        "y1": 9.846257209777832,
        "y2": -0.3880079388618469
      },
      {
        "r": 0,
        "text": "Often, the design of these models is limited by the availability of datasets, which are typically restricted to generic scenarios.  ",
        "trunc_text": "Often, the design of these models is limited by the availability of datasets, which are typically restricted to generic ",
        "x1": 4.372227191925049,
        "x2": 5.596301078796387,
        "y1": 8.37406063079834,
        "y2": 1.7083513736724854
      },
      {
        "r": 0,
        "text": "This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking -  (LSTM) networks has also been developed.",
        "trunc_text": "This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicle",
        "x1": 3.5032765865325928,
        "x2": 5.1677565574646,
        "y1": 10.001121520996094,
        "y2": -0.1803484857082367
      },
      {
        "r": 0,
        "text": "This model can handle corner cases, such as slowing down near zebra crossings and stopping when pedestrians cross the road, without the need for explicit encoding of the surrounding environment.",
        "trunc_text": "This model can handle corner cases, such as slowing down near zebra crossings and stopping when pedestrians cross the ro",
        "x1": 3.480600118637085,
        "x2": 5.20236349105835,
        "y1": 9.989653587341309,
        "y2": -0.06324001401662827
      },
      {
        "r": 0,
        "text": "Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks.",
        "trunc_text": "Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language model",
        "x1": 4.944488048553467,
        "x2": 3.822627305984497,
        "y1": 4.0149359703063965,
        "y2": 5.954591274261475
      },
      {
        "r": 0,
        "text": "However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation.",
        "trunc_text": "However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, negle",
        "x1": 5.570333003997803,
        "x2": 3.762809991836548,
        "y1": 3.2077059745788574,
        "y2": 6.915409564971924
      },
      {
        "r": 0,
        "text": "In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation.",
        "trunc_text": "In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation.",
        "x1": 5.590153217315674,
        "x2": 3.7294111251831055,
        "y1": 3.234971761703491,
        "y2": 6.96213960647583
      },
      {
        "r": 0,
        "text": "Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation.",
        "trunc_text": "Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore",
        "x1": 5.652727127075195,
        "x2": 3.7890312671661377,
        "y1": 3.250483989715576,
        "y2": 6.929141044616699
      },
      {
        "r": 0,
        "text": "Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.",
        "trunc_text": "Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstr",
        "x1": 5.64139986038208,
        "x2": 3.6492464542388916,
        "y1": 3.1420843601226807,
        "y2": 6.909902572631836
      },
      {
        "r": 0,
        "text": "Code and datasets are available on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.",
        "trunc_text": "Code and datasets are available on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.",
        "x1": 7.282104969024658,
        "x2": 7.747049331665039,
        "y1": 7.835951805114746,
        "y2": 1.993767499923706
      },
      {
        "r": 0,
        "text": "Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders (VAEs) that aims at separating the common factors of variation between a background dataset (BG) (i.e., healthy subjects) and a target dataset (TG) (i.e., patients) from the ones that only exist in the target dataset.",
        "trunc_text": "Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders (VAEs) that aims at separating the common fa",
        "x1": 2.3883824348449707,
        "x2": 2.9211792945861816,
        "y1": 6.205433368682861,
        "y2": 3.084838390350342
      },
      {
        "r": 0,
        "text": "To do so, these methods separate the latent space into a set of salient features (i.e., proper to the target dataset) and a set of common features (i.e., exist in both datasets).",
        "trunc_text": "To do so, these methods separate the latent space into a set of salient features (i.e., proper to the target dataset) an",
        "x1": 3.4028830528259277,
        "x2": 4.038616180419922,
        "y1": 6.450288772583008,
        "y2": 3.3782970905303955
      },
      {
        "r": 0,
        "text": "Currently, all models fail to prevent the sharing of information between latent spaces effectively and to capture all salient factors of variation.",
        "trunc_text": "Currently, all models fail to prevent the sharing of information between latent spaces effectively and to capture all sa",
        "x1": 3.192985773086548,
        "x2": 3.5736782550811768,
        "y1": 6.248284816741943,
        "y2": 3.5013809204101562
      },
      {
        "r": 0,
        "text": "To this end, we introduce two crucial regularization losses: a disentangling term between common and salient representations and a classification term between background and target samples in the salient space.",
        "trunc_text": "To this end, we introduce two crucial regularization losses: a disentangling term between common and salient representat",
        "x1": 2.712233304977417,
        "x2": 2.82578182220459,
        "y1": 5.833287715911865,
        "y2": 3.6814026832580566
      },
      {
        "r": 0,
        "text": "We show a better performance than previous CA-VAEs methods on three medical applications and a natural images dataset (CelebA).",
        "trunc_text": "We show a better performance than previous CA-VAEs methods on three medical applications and a natural images dataset (C",
        "x1": 2.487687587738037,
        "x2": 3.2373485565185547,
        "y1": 6.919280529022217,
        "y2": 2.6005401611328125
      },
      {
        "r": 0,
        "text": "The code and dataset are open source at https://github.com/junjun-yan/ATL-PINN.",
        "trunc_text": "The code and dataset are open source at https://github.com/junjun-yan/ATL-PINN.",
        "x1": 7.429935455322266,
        "x2": 7.906059741973877,
        "y1": 7.767043590545654,
        "y2": 1.890486717224121
      },
      {
        "r": 0,
        "text": "Physics-informed neural networks (PINNs) have emerged as promising surrogate modes for solving partial differential equations (PDEs).",
        "trunc_text": "Physics-informed neural networks (PINNs) have emerged as promising surrogate modes for solving partial differential equa",
        "x1": 3.722743034362793,
        "x2": 3.926766872406006,
        "y1": 5.113875865936279,
        "y2": 4.491490364074707
      },
      {
        "r": 0,
        "text": "Their effectiveness lies in the ability to capture solution-related features through neural networks.",
        "trunc_text": "Their effectiveness lies in the ability to capture solution-related features through neural networks.",
        "x1": 3.908336877822876,
        "x2": 3.645832061767578,
        "y1": 4.850905895233154,
        "y2": 4.836113929748535
      },
      {
        "r": 0,
        "text": "However, original PINNs often suffer from bottlenecks, such as low accuracy and non-convergence, limiting their applicability in complex physical contexts.",
        "trunc_text": "However, original PINNs often suffer from bottlenecks, such as low accuracy and non-convergence, limiting their applicab",
        "x1": 3.8496179580688477,
        "x2": 4.0078911781311035,
        "y1": 5.306632995605469,
        "y2": 4.515000820159912
      },
      {
        "r": 0,
        "text": "To alleviate these issues, we proposed auxiliary-task learning-based physics-informed neural networks (ATL-PINNs), which provide four different auxiliary-task learning modes and investigate their performance compared with original PINNs.",
        "trunc_text": "To alleviate these issues, we proposed auxiliary-task learning-based physics-informed neural networks (ATL-PINNs), which",
        "x1": 3.7057392597198486,
        "x2": 3.8330225944519043,
        "y1": 5.04239559173584,
        "y2": 4.603259563446045
      },
      {
        "r": 0,
        "text": "We also employ the gradient cosine similarity algorithm to integrate auxiliary problem loss with the primary problem loss in ATL-PINNs, which aims to enhance the effectiveness of the auxiliary-task learning modes.",
        "trunc_text": "We also employ the gradient cosine similarity algorithm to integrate auxiliary problem loss with the primary problem los",
        "x1": 3.638476610183716,
        "x2": 3.6139769554138184,
        "y1": 4.823917865753174,
        "y2": 4.751458644866943
      },
      {
        "r": 0,
        "text": "To the best of our knowledge, this is the first study to introduce auxiliary-task learning modes in the context of physics-informed learning.",
        "trunc_text": "To the best of our knowledge, this is the first study to introduce auxiliary-task learning modes in the context of physi",
        "x1": 3.7271430492401123,
        "x2": 3.8576626777648926,
        "y1": 4.989552974700928,
        "y2": 4.628983974456787
      },
      {
        "r": 0,
        "text": "We conduct experiments on three PDE problems across different fields and scenarios.",
        "trunc_text": "We conduct experiments on three PDE problems across different fields and scenarios.",
        "x1": 5.251313209533691,
        "x2": 6.228801727294922,
        "y1": 8.733210563659668,
        "y2": 4.685134410858154
      },
      {
        "r": 0,
        "text": "Our findings demonstrate that the proposed auxiliary-task learning modes can significantly improve solution accuracy, achieving a maximum performance boost of 96.62% (averaging 28.23%) compared to the original single-task PINNs.",
        "trunc_text": "Our findings demonstrate that the proposed auxiliary-task learning modes can significantly improve solution accuracy, ac",
        "x1": 3.8521456718444824,
        "x2": 3.7041449546813965,
        "y1": 4.831492900848389,
        "y2": 4.767460346221924
      },
      {
        "r": 0,
        "text": "To facilitate the investigation, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues",
        "trunc_text": "To facilitate the investigation, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio",
        "x1": 2.5789570808410645,
        "x2": 5.630760669708252,
        "y1": 7.924156665802002,
        "y2": 2.2688162326812744
      },
      {
        "r": 0,
        "text": "We will release our dataset and codes to facilitate future studies.",
        "trunc_text": "We will release our dataset and codes to facilitate future studies.",
        "x1": 6.549209117889404,
        "x2": 7.138433456420898,
        "y1": 7.3929924964904785,
        "y2": 2.5169272422790527
      },
      {
        "r": 0,
        "text": "Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings.",
        "trunc_text": "Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings.",
        "x1": 4.274801731109619,
        "x2": 2.8675999641418457,
        "y1": 3.8944573402404785,
        "y2": 4.624189853668213
      },
      {
        "r": 0,
        "text": "One example is that humans can reason where and when an image is taken based on their knowledge.",
        "trunc_text": "One example is that humans can reason where and when an image is taken based on their knowledge.",
        "x1": 4.078944206237793,
        "x2": 2.797640562057495,
        "y1": 4.02854061126709,
        "y2": 4.700047969818115
      },
      {
        "r": 0,
        "text": "This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even outperform human's capability in reasoning times and location.",
        "trunc_text": "This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text r",
        "x1": 4.162717819213867,
        "x2": 2.7904887199401855,
        "y1": 3.8360185623168945,
        "y2": 4.725684642791748
      },
      {
        "r": 0,
        "text": "To address this question, we propose a two-stage \\recognition\\space and \\reasoning\\space probing task, applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. .",
        "trunc_text": "To address this question, we propose a two-stage \\recognition\\space and \\reasoning\\space probing task, applied to discri",
        "x1": 2.835683584213257,
        "x2": 2.911841630935669,
        "y1": 5.783890247344971,
        "y2": 4.284389019012451
      },
      {
        "r": 0,
        "text": "In the extensive experimental studies, we find that although VLMs can effectively retain relevant features in visual encoders, they still fail to make perfect reasoning.",
        "trunc_text": "In the extensive experimental studies, we find that although VLMs can effectively retain relevant features in visual enc",
        "x1": 2.7587730884552,
        "x2": 2.884739875793457,
        "y1": 5.649593830108643,
        "y2": 4.282622814178467
      },
      {
        "r": 0,
        "text": "Our model was evaluated on two benchmark tree counting datasets, Jiangsu, and Yosemite, as well as a new dataset, KCL-London, created by ourselves",
        "trunc_text": "Our model was evaluated on two benchmark tree counting datasets, Jiangsu, and Yosemite, as well as a new dataset, KCL-Lo",
        "x1": 3.6283862590789795,
        "x2": 4.208008766174316,
        "y1": 6.800710201263428,
        "y2": 3.044450521469116
      },
      {
        "r": 0,
        "text": "The codes and datasets are available at https://github.com/HAAClassic/TreeFormer.",
        "trunc_text": "The codes and datasets are available at https://github.com/HAAClassic/TreeFormer.",
        "x1": 7.195821285247803,
        "x2": 7.76480770111084,
        "y1": 7.580842971801758,
        "y2": 1.8469576835632324
      },
      {
        "r": 0,
        "text": "Automatic tree density estimation and counting using single aerial and satellite images is a challenging task in photogrammetry and remote sensing, yet has an important role in forest management.",
        "trunc_text": "Automatic tree density estimation and counting using single aerial and satellite images is a challenging task in photogr",
        "x1": 1.2861310243606567,
        "x2": 4.54293966293335,
        "y1": 7.286459922790527,
        "y2": 1.3901748657226562
      },
      {
        "r": 0,
        "text": "In this paper, we propose the first semisupervised transformer-based framework for tree counting which reduces the expensive tree annotations for remote sensing images.",
        "trunc_text": "In this paper, we propose the first semisupervised transformer-based framework for tree counting which reduces the expen",
        "x1": 1.3112822771072388,
        "x2": 2.327727794647217,
        "y1": 7.194958686828613,
        "y2": 2.753756284713745
      },
      {
        "r": 0,
        "text": "Our method, termed as TreeFormer, first develops a pyramid tree representation module based on transformer blocks to extract multi-scale features during the encoding stage.",
        "trunc_text": "Our method, termed as TreeFormer, first develops a pyramid tree representation module based on transformer blocks to ext",
        "x1": 1.6333774328231812,
        "x2": 2.3503215312957764,
        "y1": 6.70394229888916,
        "y2": 3.041409730911255
      },
      {
        "r": 0,
        "text": "Contextual attention-based feature fusion and tree density regressor modules are further designed to utilize the robust features from the encoder to estimate tree density maps in the decoder.",
        "trunc_text": "Contextual attention-based feature fusion and tree density regressor modules are further designed to utilize the robust ",
        "x1": 3.976454019546509,
        "x2": 2.4162919521331787,
        "y1": 3.7855377197265625,
        "y2": 4.593909740447998
      },
      {
        "r": 0,
        "text": "Moreover, we propose a pyramid learning strategy that includes local tree density consistency and local tree count ranking losses to utilize unlabeled images into the training process.",
        "trunc_text": "Moreover, we propose a pyramid learning strategy that includes local tree density consistency and local tree count ranki",
        "x1": 1.483494520187378,
        "x2": 2.308589220046997,
        "y1": 6.588554859161377,
        "y2": 2.777423143386841
      },
      {
        "r": 0,
        "text": "Finally, the tree counter token is introduced to regulate the network by computing the global tree counts for both labeled and unlabeled images. .",
        "trunc_text": "Finally, the tree counter token is introduced to regulate the network by computing the global tree counts for both label",
        "x1": 1.6581133604049683,
        "x2": 2.420788288116455,
        "y1": 6.544922351837158,
        "y2": 2.7788774967193604
      },
      {
        "r": 0,
        "text": "Our TreeFormer outperforms the state of the art semi-supervised methods under the same setting and exceeds the fully-supervised methods using the same number of labeled images.",
        "trunc_text": "Our TreeFormer outperforms the state of the art semi-supervised methods under the same setting and exceeds the fully-sup",
        "x1": 1.4713770151138306,
        "x2": 1.7504656314849854,
        "y1": 5.241911888122559,
        "y2": 3.209911584854126
      },
      {
        "r": 0,
        "text": "Despite recent advancements in speech emotion recognition (SER) models, state-of-the-art deep learning (DL) approaches face the challenge of the limited availability of annotated data.",
        "trunc_text": "Despite recent advancements in speech emotion recognition (SER) models, state-of-the-art deep learning (DL) approaches f",
        "x1": 4.934342861175537,
        "x2": 3.0485265254974365,
        "y1": 2.5942046642303467,
        "y2": 6.967033863067627
      },
      {
        "r": 0,
        "text": "Large language models (LLMs) have revolutionised our understanding of natural language, introducing emergent properties that broaden comprehension in language, speech, and vision.",
        "trunc_text": "Large language models (LLMs) have revolutionised our understanding of natural language, introducing emergent properties ",
        "x1": 5.200118541717529,
        "x2": 3.833117723464966,
        "y1": 3.848780393600464,
        "y2": 6.24756383895874
      },
      {
        "r": 0,
        "text": "This paper examines the potential of LLMs to annotate abundant speech data, aiming to enhance the state-of-the-art in SER.",
        "trunc_text": "This paper examines the potential of LLMs to annotate abundant speech data, aiming to enhance the state-of-the-art in SE",
        "x1": 4.979673862457275,
        "x2": 3.1823465824127197,
        "y1": 2.988126277923584,
        "y2": 6.62557315826416
      },
      {
        "r": 0,
        "text": "We evaluate this capability across various settings using publicly available speech emotion classification datasets.",
        "trunc_text": "We evaluate this capability across various settings using publicly available speech emotion classification datasets.",
        "x1": 4.969440460205078,
        "x2": 3.044816017150879,
        "y1": 2.592221975326538,
        "y2": 6.989037990570068
      },
      {
        "r": 0,
        "text": "Leveraging ChatGPT, we experimentally demonstrate the promising role of LLMs in speech emotion data annotation.",
        "trunc_text": "Leveraging ChatGPT, we experimentally demonstrate the promising role of LLMs in speech emotion data annotation.",
        "x1": 4.9997711181640625,
        "x2": 3.102858543395996,
        "y1": 2.690176486968994,
        "y2": 6.965448379516602
      },
      {
        "r": 0,
        "text": "Our evaluation encompasses single-shot and few-shots scenarios, revealing performance variability in SER.",
        "trunc_text": "Our evaluation encompasses single-shot and few-shots scenarios, revealing performance variability in SER.",
        "x1": 2.977625608444214,
        "x2": 3.807283878326416,
        "y1": 6.6997761726379395,
        "y2": 3.2508544921875
      },
      {
        "r": 0,
        "text": "Notably, we achieve improved results through data augmentation, incorporating ChatGPT-annotated samples into existing datasets.",
        "trunc_text": "Notably, we achieve improved results through data augmentation, incorporating ChatGPT-annotated samples into existing da",
        "x1": 6.308121204376221,
        "x2": 4.394996166229248,
        "y1": 3.5107202529907227,
        "y2": 6.9191460609436035
      },
      {
        "r": 0,
        "text": "Our work uncovers new frontiers in speech emotion classification, highlighting the increasing significance of LLMs in this field moving forward.",
        "trunc_text": "Our work uncovers new frontiers in speech emotion classification, highlighting the increasing significance of LLMs in th",
        "x1": 5.002160549163818,
        "x2": 3.0842158794403076,
        "y1": 2.646519899368286,
        "y2": 6.9390034675598145
      },
      {
        "r": 0,
        "text": "We have taken millions of images in the sorghum fields, manually selected 5,447 images that contain aphids, and annotated each aphid cluster in the image.",
        "trunc_text": "We have taken millions of images in the sorghum fields, manually selected 5,447 images that contain aphids, and annotate",
        "x1": 1.0175890922546387,
        "x2": 5.081602096557617,
        "y1": 7.674288272857666,
        "y2": 1.3988875150680542
      },
      {
        "r": 0,
        "text": "To use these images for machine learning models, we crop the images into patches and created a labeled dataset with over 151,000 image patches.",
        "trunc_text": "To use these images for machine learning models, we crop the images into patches and created a labeled dataset with over",
        "x1": 1.9241153001785278,
        "x2": 3.6635491847991943,
        "y1": 7.393822193145752,
        "y2": 1.6240894794464111
      },
      {
        "r": 0,
        "text": "Aphids are one of the main threats to crops, rural families, and global food security.",
        "trunc_text": "Aphids are one of the main threats to crops, rural families, and global food security.",
        "x1": 0.9922621250152588,
        "x2": 5.1808366775512695,
        "y1": 7.659412860870361,
        "y2": 1.3605737686157227
      },
      {
        "r": 0,
        "text": "Chemical pest control is a necessary component of crop production for maximizing yields, however, it is unnecessary to apply the chemical approaches to the entire fields in consideration of the environmental pollution and the cost.",
        "trunc_text": "Chemical pest control is a necessary component of crop production for maximizing yields, however, it is unnecessary to a",
        "x1": 1.0827093124389648,
        "x2": 5.178157329559326,
        "y1": 7.669562339782715,
        "y2": 1.42765212059021
      },
      {
        "r": 0,
        "text": "Thus, accurately localizing the aphid and estimating the infestation level is crucial to the precise local application of pesticides.",
        "trunc_text": "Thus, accurately localizing the aphid and estimating the infestation level is crucial to the precise local application o",
        "x1": 0.9558884501457214,
        "x2": 5.155966758728027,
        "y1": 7.701029300689697,
        "y2": 1.3419301509857178
      },
      {
        "r": 0,
        "text": "Aphid detection is very challenging as each individual aphid is really small and all aphids are crowded together as clusters.",
        "trunc_text": "Aphid detection is very challenging as each individual aphid is really small and all aphids are crowded together as clus",
        "x1": 0.9978330731391907,
        "x2": 5.075406551361084,
        "y1": 7.656454086303711,
        "y2": 1.3774245977401733
      },
      {
        "r": 0,
        "text": "In this paper, we propose to estimate the infection level by detecting aphid clusters.  ",
        "trunc_text": "In this paper, we propose to estimate the infection level by detecting aphid clusters.  ",
        "x1": 1.025062918663025,
        "x2": 5.150951862335205,
        "y1": 7.70684289932251,
        "y2": 1.3671807050704956
      },
      {
        "r": 0,
        "text": "Then, we i",
        "trunc_text": "Then, we i",
        "x1": 5.433053970336914,
        "x2": 6.246978282928467,
        "y1": 8.247071266174316,
        "y2": 4.0412492752075195
      },
      {
        "r": 0,
        "text": "Given a set of calibrated images of a scene, we present an approach that produces a simple, compact, and actionable 3D world representation by means of 3D primitives.",
        "trunc_text": "Given a set of calibrated images of a scene, we present an approach that produces a simple, compact, and actionable 3D w",
        "x1": 1.94248628616333,
        "x2": 3.5130066871643066,
        "y1": 8.899066925048828,
        "y2": 0.47358033061027527
      },
      {
        "r": 0,
        "text": "While many approaches focus on recovering high-fidelity 3D scenes, we focus on parsing a scene into mid-level 3D representations made of a small set of textured primitives.",
        "trunc_text": "While many approaches focus on recovering high-fidelity 3D scenes, we focus on parsing a scene into mid-level 3D represe",
        "x1": 1.8152436017990112,
        "x2": 3.5269503593444824,
        "y1": 8.834863662719727,
        "y2": 0.6056360602378845
      },
      {
        "r": 0,
        "text": "Such representations are interpretable, easy to manipulate and suited for physics-based simulations.",
        "trunc_text": "Such representations are interpretable, easy to manipulate and suited for physics-based simulations.",
        "x1": 2.9874308109283447,
        "x2": 4.041718482971191,
        "y1": 8.528803825378418,
        "y2": 0.6662194132804871
      },
      {
        "r": 0,
        "text": "Moreover, unlike existing primitive decomposition methods that rely on 3D input data, our approach operates directly on images through differentiable rendering.",
        "trunc_text": "Moreover, unlike existing primitive decomposition methods that rely on 3D input data, our approach operates directly on ",
        "x1": 1.8179550170898438,
        "x2": 3.620600700378418,
        "y1": 8.77206039428711,
        "y2": 0.7041364312171936
      },
      {
        "r": 0,
        "text": "Specifically, we model primitives as textured superquadric meshes and optimize their parameters from scratch with an image rendering loss.",
        "trunc_text": "Specifically, we model primitives as textured superquadric meshes and optimize their parameters from scratch with an ima",
        "x1": 1.8898886442184448,
        "x2": 3.5737242698669434,
        "y1": 8.749235153198242,
        "y2": 0.6814867854118347
      },
      {
        "r": 0,
        "text": "We highlight the importance of modeling transparency for each primitive, which is critical for optimization and also enables handling varying numbers of primitives.",
        "trunc_text": "We highlight the importance of modeling transparency for each primitive, which is critical for optimization and also ena",
        "x1": 1.7099626064300537,
        "x2": 3.617183208465576,
        "y1": 8.677459716796875,
        "y2": 0.6520715355873108
      },
      {
        "r": 0,
        "text": "We show that the resulting textured primitives faithfully reconstruct the input images and accurately model the visible 3D points, while providing amodal shape completions of unseen object regions.",
        "trunc_text": "We show that the resulting textured primitives faithfully reconstruct the input images and accurately model the visible ",
        "x1": 1.6966160535812378,
        "x2": 3.4007787704467773,
        "y1": 8.805636405944824,
        "y2": 0.5511245131492615
      },
      {
        "r": 0,
        "text": "We compare our approach to the state of the art on diverse scenes from DTU, and demonstrate its robustness on real-life captures from BlendedMVS and Nerfstudio.",
        "trunc_text": "We compare our approach to the state of the art on diverse scenes from DTU, and demonstrate its robustness on real-life ",
        "x1": 1.8438843488693237,
        "x2": 3.494466781616211,
        "y1": 8.756648063659668,
        "y2": 0.7405214905738831
      },
      {
        "r": 0,
        "text": "We also showcase how our results can be used to effortlessly edit a scene or perform physical simulations.",
        "trunc_text": "We also showcase how our results can be used to effortlessly edit a scene or perform physical simulations.",
        "x1": 3.059296131134033,
        "x2": 4.240525245666504,
        "y1": 8.630162239074707,
        "y2": 0.9411424994468689
      },
      {
        "r": 0,
        "text": "Code and video results are available at https://www.tmonnier.com/DBW .",
        "trunc_text": "Code and video results are available at https://www.tmonnier.com/DBW .",
        "x1": 7.57194709777832,
        "x2": 8.284815788269043,
        "y1": 7.537605285644531,
        "y2": 2.0773236751556396
      },
      {
        "r": 0,
        "text": "This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs",
        "trunc_text": "This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting ",
        "x1": 4.419724464416504,
        "x2": 2.0259525775909424,
        "y1": 2.037902593612671,
        "y2": 7.176662445068359
      },
      {
        "r": 0,
        "text": "To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language",
        "trunc_text": "To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language",
        "x1": 4.428634166717529,
        "x2": 2.0079543590545654,
        "y1": 2.0348339080810547,
        "y2": 7.181535720825195
      },
      {
        "r": 0,
        "text": "To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation",
        "trunc_text": "To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark th",
        "x1": 4.486783027648926,
        "x2": 2.0730412006378174,
        "y1": 2.077986240386963,
        "y2": 7.174710750579834
      },
      {
        "r": 0,
        "text": "Sign languages are the primary means of communication for many hard-of-hearing people worldwide.",
        "trunc_text": "Sign languages are the primary means of communication for many hard-of-hearing people worldwide.",
        "x1": 4.46205472946167,
        "x2": 2.0259742736816406,
        "y1": 2.012840986251831,
        "y2": 7.2064714431762695
      },
      {
        "r": 0,
        "text": "Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems.",
        "trunc_text": "Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several ",
        "x1": 4.48031759262085,
        "x2": 2.0250906944274902,
        "y1": 2.002873659133911,
        "y2": 7.198753356933594
      },
      {
        "r": 0,
        "text": "However, there is a dearth of sign language resources for the Indian sign language. .",
        "trunc_text": "However, there is a dearth of sign language resources for the Indian sign language. .",
        "x1": 4.43813943862915,
        "x2": 1.9940838813781738,
        "y1": 2.0293776988983154,
        "y2": 7.200251579284668
      },
      {
        "r": 0,
        "text": "To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language.",
        "trunc_text": "To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language.",
        "x1": 4.413400650024414,
        "x2": 2.008549928665161,
        "y1": 2.0066237449645996,
        "y2": 7.160276889801025
      },
      {
        "r": 0,
        "text": "To validchmark the created dataset with a transformer-based model for ISL translation.",
        "trunc_text": "To validchmark the created dataset with a transformer-based model for ISL translation.",
        "x1": 4.468490123748779,
        "x2": 2.092278480529785,
        "y1": 2.1644480228424072,
        "y2": 7.1454243659973145
      },
      {
        "r": 0,
        "text": "This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a novel dataset targeting real-life scenario reasoning",
        "trunc_text": "This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a novel dataset targeting real-life scenario ",
        "x1": 4.805031776428223,
        "x2": 4.2679314613342285,
        "y1": 6.268252372741699,
        "y2": 3.6002085208892822
      },
      {
        "r": 0,
        "text": "The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve its quality.",
        "trunc_text": "The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve i",
        "x1": 5.504358768463135,
        "x2": 6.225505828857422,
        "y1": 7.208755016326904,
        "y2": 2.4306797981262207
      },
      {
        "r": 0,
        "text": ", aiming to close the gap in artificial neural networks' ability to reason in everyday contexts.",
        "trunc_text": ", aiming to close the gap in artificial neural networks' ability to reason in everyday contexts.",
        "x1": 4.075873374938965,
        "x2": 3.447657823562622,
        "y1": 4.392430782318115,
        "y2": 5.076350212097168
      },
      {
        "r": 0,
        "text": "In contrast to domain knowledge reasoning datasets, LSR-Benchmark comprises free-text formatted questions with rich information on real-life scenarios, human behaviors, and character roles.",
        "trunc_text": "In contrast to domain knowledge reasoning datasets, LSR-Benchmark comprises free-text formatted questions with rich info",
        "x1": 4.776209354400635,
        "x2": 4.329326152801514,
        "y1": 6.234353542327881,
        "y2": 3.7374796867370605
      },
      {
        "r": 0,
        "text": "The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve its qualitto test the performance in LSR-Benchmark.",
        "trunc_text": "The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve i",
        "x1": 4.722474098205566,
        "x2": 4.423843860626221,
        "y1": 6.593704700469971,
        "y2": 3.355081558227539
      },
      {
        "r": 0,
        "text": "The results reveal that humans outperform these models significantly, indicating a persisting challenge for machine learning models in comprehending daily human life.",
        "trunc_text": "The results reveal that humans outperform these models significantly, indicating a persisting challenge for machine lear",
        "x1": 4.147916793823242,
        "x2": 3.3034722805023193,
        "y1": 4.305201530456543,
        "y2": 5.176050186157227
      },
      {
        "r": 0,
        "text": "The smarty4covid dataset contains audio signals of cough (4,676), regular breathing (4,665), deep breathing (4,695) and voice (4,291) as recorded by means of mobile devices following a crowd-sourcing approach",
        "trunc_text": "The smarty4covid dataset contains audio signals of cough (4,676), regular breathing (4,665), deep breathing (4,695) and ",
        "x1": 5.274426460266113,
        "x2": 6.082906723022461,
        "y1": 7.760245323181152,
        "y2": 2.0369455814361572
      },
      {
        "r": 0,
        "text": "Other self reported information is also included (e.g. COVID-19 virus tests), thus providing a comprehensive dataset for the development of COVID-19 risk detection models.",
        "trunc_text": "Other self reported information is also included (e.g. COVID-19 virus tests), thus providing a comprehensive dataset for",
        "x1": 4.748116970062256,
        "x2": 5.986507415771484,
        "y1": 7.899563312530518,
        "y2": 1.8965191841125488
      },
      {
        "r": 0,
        "text": "The smarty4covid dataset is released in the form of a web-ontology language (OWL) knowledge base enabling data consolidation from other relevant datasets, complex queries and reasoning",
        "trunc_text": "The smarty4covid dataset is released in the form of a web-ontology language (OWL) knowledge base enabling data consolida",
        "x1": 5.8729376792907715,
        "x2": 6.499192714691162,
        "y1": 7.457355499267578,
        "y2": 2.3996546268463135
      },
      {
        "r": 0,
        "text": "Harnessing the power of Artificial Intelligence (AI) and m-health towards detecting new bio-markers indicative of the onset and progress of respiratory abnormalities/conditions has greatly attracted the scientific and research interest especially during COVID-19 pandemic. .",
        "trunc_text": "Harnessing the power of Artificial Intelligence (AI) and m-health towards detecting new bio-markers indicative of the on",
        "x1": 4.245887756347656,
        "x2": 5.4894700050354,
        "y1": 7.866272926330566,
        "y2": 1.923842430114746
      },
      {
        "r": 0,
        "text": "The smarty4covid dataset is releasedtowards the development of models able to: (i) extract clinically informative respiratory indicators from regular breathing records, and (ii) identify cough, breath and voirisk detection models is proposed and validated.",
        "trunc_text": "The smarty4covid dataset is releasedtowards the development of models able to: (i) extract clinically informative respir",
        "x1": 4.192815780639648,
        "x2": 5.38972806930542,
        "y1": 7.810173511505127,
        "y2": 2.073244094848633
      },
      {
        "r": 0,
        "text": "This dataset collects nighttime images with different properties of nighttime environments, such as flare and extreme darkness",
        "trunc_text": "This dataset collects nighttime images with different properties of nighttime environments, such as flare and extreme da",
        "x1": 1.8092032670974731,
        "x2": 4.441562652587891,
        "y1": 10.097126007080078,
        "y2": 0.6488869190216064
      },
      {
        "r": 0,
        "text": "Nighttime surveillance suffers from degradation due to poor illumination and arduous human annotations.",
        "trunc_text": "Nighttime surveillance suffers from degradation due to poor illumination and arduous human annotations.",
        "x1": 1.7827441692352295,
        "x2": 4.439558506011963,
        "y1": 10.180203437805176,
        "y2": 0.6077513098716736
      },
      {
        "r": 0,
        "text": "It is challengable and remains a security risk at night.",
        "trunc_text": "It is challengable and remains a security risk at night.",
        "x1": 1.79729425907135,
        "x2": 6.514071464538574,
        "y1": 10.19304370880127,
        "y2": 6.185408115386963
      },
      {
        "r": 0,
        "text": "Existing methods rely on multi-spectral images to perceive objects in the dark, which are troubled by low resolution and color absence.",
        "trunc_text": "Existing methods rely on multi-spectral images to perceive objects in the dark, which are troubled by low resolution and",
        "x1": 1.671582579612732,
        "x2": 3.9494028091430664,
        "y1": 9.910664558410645,
        "y2": 0.3910331130027771
      },
      {
        "r": 0,
        "text": "We argue that the ultimate solution for nighttime surveillance is night-to-day translation, or Night2Day, which aims to translate a surveillance scene from nighttime to the daytime while maintaining semantic consistency.",
        "trunc_text": "We argue that the ultimate solution for nighttime surveillance is night-to-day translation, or Night2Day, which aims to ",
        "x1": 1.7948185205459595,
        "x2": 4.455989360809326,
        "y1": 10.19919204711914,
        "y2": 0.6396641135215759
      },
      {
        "r": 0,
        "text": "To achieve this, this paper presents a Disentangled Contrastive (DiCo) learning method.",
        "trunc_text": "To achieve this, this paper presents a Disentangled Contrastive (DiCo) learning method.",
        "x1": 2.62735915184021,
        "x2": 2.757108449935913,
        "y1": 5.801835536956787,
        "y2": 3.543853759765625
      },
      {
        "r": 0,
        "text": "Specifically, to address the poor and complex illumination in the nighttime scenes, we propose a learnable physical prior, i.e., the color invariant, which provides a stable perception of a highly dynamic night environment and can be incorporated into the learning pipeline of neural networks.",
        "trunc_text": "Specifically, to address the poor and complex illumination in the nighttime scenes, we propose a learnable physical prio",
        "x1": 1.8865889310836792,
        "x2": 4.269741535186768,
        "y1": 10.060565948486328,
        "y2": 0.4701743721961975
      },
      {
        "r": 0,
        "text": "Targeting the surveillance scenes, we develop a disentangled representation, which is an auxiliary pretext task that separates surveillance scenes into the foreground and background with contrastive learning.",
        "trunc_text": "Targeting the surveillance scenes, we develop a disentangled representation, which is an auxiliary pretext task that sep",
        "x1": 2.6183180809020996,
        "x2": 2.765502452850342,
        "y1": 5.891189098358154,
        "y2": 3.668407440185547
      },
      {
        "r": 0,
        "text": "Such a strategy can extract the semantics without supervision and boost our model to achieve instance-aware translation.",
        "trunc_text": "Such a strategy can extract the semantics without supervision and boost our model to achieve instance-aware translation.",
        "x1": 4.246288776397705,
        "x2": 3.076875686645508,
        "y1": 3.854450225830078,
        "y2": 6.004345417022705
      },
      {
        "r": 0,
        "text": "Finally, we incorporate all the modules above into generative adversarial networks and achieve high-fidelity translation.  ",
        "trunc_text": "Finally, we incorporate all the modules above into generative adversarial networks and achieve high-fidelity translation",
        "x1": 2.1459240913391113,
        "x2": 3.2943286895751953,
        "y1": 6.992866039276123,
        "y2": 2.2201271057128906
      },
      {
        "r": 0,
        "text": "It includes six scenes to support the study on nighttime surveillance.",
        "trunc_text": "It includes six scenes to support the study on nighttime surveillance.",
        "x1": 1.7612897157669067,
        "x2": 4.459425449371338,
        "y1": 10.18851375579834,
        "y2": 0.6087542176246643
      },
      {
        "r": 0,
        "text": "This dataset collects nighttime images with different properties of nigg works significantly.",
        "trunc_text": "This dataset collects nighttime images with different properties of nigg works significantly.",
        "x1": 1.8321584463119507,
        "x2": 4.453689098358154,
        "y1": 10.15855884552002,
        "y2": 0.6656143069267273
      },
      {
        "r": 0,
        "text": "In this paper, we present TRansPose, the first large-scale multispectral dataset that combines stereo RGB-D, thermal infrared (TIR) images, and object poses to promote transparent object research",
        "trunc_text": "In this paper, we present TRansPose, the first large-scale multispectral dataset that combines stereo RGB-D, thermal inf",
        "x1": 1.1555474996566772,
        "x2": 3.6562318801879883,
        "y1": 9.253486633300781,
        "y2": 0.20194289088249207
      },
      {
        "r": 0,
        "text": "The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects",
        "trunc_text": "The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laborat",
        "x1": 1.6543408632278442,
        "x2": 4.470349311828613,
        "y1": 8.003721237182617,
        "y2": 1.408911943435669
      },
      {
        "r": 0,
        "text": "It comprises a vast collection of 333,819 images and 4,000,056 annotations, providing instance-level segmentation masks, ground-truth poses, and completed depth information",
        "trunc_text": "It comprises a vast collection of 333,819 images and 4,000,056 annotations, providing instance-level segmentation masks,",
        "x1": 1.5733752250671387,
        "x2": 3.6690542697906494,
        "y1": 7.878899574279785,
        "y2": 1.2191529273986816
      },
      {
        "r": 0,
        "text": "The data was acquired using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda robot manipulator",
        "trunc_text": "The data was acquired using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Fran",
        "x1": 1.3264590501785278,
        "x2": 3.8356809616088867,
        "y1": 9.262505531311035,
        "y2": 0.08563169091939926
      },
      {
        "r": 0,
        "text": "TRansPose dataset can be accessed from the following link: https://sites.google.com/view/transpose-dataset",
        "trunc_text": "TRansPose dataset can be accessed from the following link: https://sites.google.com/view/transpose-dataset",
        "x1": 7.099598407745361,
        "x2": 7.579471111297607,
        "y1": 7.734786510467529,
        "y2": 2.0564956665039062
      },
      {
        "r": 0,
        "text": "Transparent objects are encountered frequently in our daily lives, yet recognizing them poses challenges for conventional vision sensors due to their unique material properties, not being well perceived from RGB or depth cameras.",
        "trunc_text": "Transparent objects are encountered frequently in our daily lives, yet recognizing them poses challenges for conventiona",
        "x1": 1.2197400331497192,
        "x2": 3.7357659339904785,
        "y1": 9.188495635986328,
        "y2": 0.24493589997291565
      },
      {
        "r": 0,
        "text": "Overcoming this limitation, thermal infrared cameras have emerged as a solution, offering improved visibility and shape information for transparent objects. .",
        "trunc_text": "Overcoming this limitation, thermal infrared cameras have emerged as a solution, offering improved visibility and shape ",
        "x1": 1.1826919317245483,
        "x2": 3.7593560218811035,
        "y1": 9.338752746582031,
        "y2": 0.19526013731956482
      },
      {
        "r": 0,
        "text": "The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects.",
        "trunc_text": "The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laborat",
        "x1": 1.674860954284668,
        "x2": 4.463253021240234,
        "y1": 7.936436176300049,
        "y2": 1.4300923347473145
      },
      {
        "r": 0,
        "text": "It comprises a vast colleced using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda robot manipulator.",
        "trunc_text": "It comprises a vast colleced using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and",
        "x1": 1.2420891523361206,
        "x2": 3.774343967437744,
        "y1": 9.253913879394531,
        "y2": 0.053416285663843155
      },
      {
        "r": 0,
        "text": "Spanning 87 sequences, TRansPose cbjects in plastic bags, and multi-stacked objects.",
        "trunc_text": "Spanning 87 sequences, TRansPose cbjects in plastic bags, and multi-stacked objects.",
        "x1": 1.8034096956253052,
        "x2": 4.456872463226318,
        "y1": 7.880393981933594,
        "y2": 1.4299975633621216
      },
      {
        "r": 0,
        "text": "this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs).",
        "trunc_text": "this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language mode",
        "x1": 5.024847984313965,
        "x2": 3.6320066452026367,
        "y1": 3.678225040435791,
        "y2": 6.255090713500977
      },
      {
        "r": 0,
        "text": "In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics",
        "trunc_text": "In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert ",
        "x1": 2.5551505088806152,
        "x2": 1.6461405754089355,
        "y1": 3.92838716506958,
        "y2": 5.093804836273193
      },
      {
        "r": 0,
        "text": "We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs.",
        "trunc_text": "We believe this dataset provides vital resources for the community, contributing towards the safe development and deploy",
        "x1": 6.504077434539795,
        "x2": 6.947755813598633,
        "y1": 7.156065464019775,
        "y2": 2.565173625946045
      },
      {
        "r": 0,
        "text": "In  This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes.",
        "trunc_text": "In  This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offer",
        "x1": 2.6132349967956543,
        "x2": 1.656477689743042,
        "y1": 3.768070936203003,
        "y2": 5.148281574249268
      },
      {
        "r": 0,
        "text": "In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert comparisonhasizing its potential for practical safety measures in LLMs.",
        "trunc_text": "In total, we have compiled safety meta-labels for 30,207 question-answer (QA) pairs and gathered 30,144 pairs of expert ",
        "x1": 2.4762299060821533,
        "x2": 1.6617474555969238,
        "y1": 4.086180210113525,
        "y2": 5.111872673034668
      },
      {
        "r": 0,
        "text": "We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM).",
        "trunc_text": "We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM).",
        "x1": 5.10956335067749,
        "x2": 3.6384572982788086,
        "y1": 3.5542867183685303,
        "y2": 6.316305637359619
      },
      {
        "r": 0,
        "text": "The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes.",
        "trunc_text": "The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limite",
        "x1": 5.177681922912598,
        "x2": 3.737189292907715,
        "y1": 3.569434404373169,
        "y2": 6.37566614151001
      },
      {
        "r": 0,
        "text": "We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context.",
        "trunc_text": "We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to",
        "x1": 5.160698890686035,
        "x2": 3.7170257568359375,
        "y1": 3.6599433422088623,
        "y2": 6.293128967285156
      },
      {
        "r": 0,
        "text": "Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses.",
        "trunc_text": "Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompt",
        "x1": 5.202235698699951,
        "x2": 3.811183214187622,
        "y1": 3.779425621032715,
        "y2": 6.342566967010498
      },
      {
        "r": 0,
        "text": "Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts.",
        "trunc_text": "Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can ef",
        "x1": 5.181756019592285,
        "x2": 3.702329635620117,
        "y1": 3.5561485290527344,
        "y2": 6.408458709716797
      },
      {
        "r": 0,
        "text": "The promising results demonstrate significant implications of the ICAE for its novel approach to the long context problem and its potential to reduce computation and memory overheads for LLM inference in practice, suggesting further research effort in context management for an LLM.",
        "trunc_text": "The promising results demonstrate significant implications of the ICAE for its novel approach to the long context proble",
        "x1": 5.172463417053223,
        "x2": 3.657147169113159,
        "y1": 3.5526840686798096,
        "y2": 6.403021812438965
      },
      {
        "r": 0,
        "text": "This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation.",
        "trunc_text": "This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and trans",
        "x1": 3.9532470703125,
        "x2": 2.621051549911499,
        "y1": 3.2188735008239746,
        "y2": 5.185556888580322
      },
      {
        "r": 0,
        "text": "The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words.",
        "trunc_text": "The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by",
        "x1": 2.3054189682006836,
        "x2": 3.2049882411956787,
        "y1": 8.56159782409668,
        "y2": 0.9254694581031799
      },
      {
        "r": 0,
        "text": " The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words.",
        "trunc_text": " The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied b",
        "x1": 2.2583014965057373,
        "x2": 3.2147583961486816,
        "y1": 8.551013946533203,
        "y2": 0.9219740629196167
      },
      {
        "r": 0,
        "text": "Our core contribution is to developnguage representation at scale.",
        "trunc_text": "Our core contribution is to developnguage representation at scale.",
        "x1": 4.503650665283203,
        "x2": 3.0458967685699463,
        "y1": 3.8568167686462402,
        "y2": 4.869194507598877
      },
      {
        "r": 0,
        "text": "Specifically, we utilize a multi-scale approach to generate video-related descriptions.",
        "trunc_text": "Specifically, we utilize a multi-scale approach to generate video-related descriptions.",
        "x1": 3.9279022216796875,
        "x2": 2.4931235313415527,
        "y1": 3.250530958175659,
        "y2": 5.229398727416992
      },
      {
        "r": 0,
        "text": "Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance.",
        "trunc_text": "Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via co",
        "x1": 3.8378238677978516,
        "x2": 2.439027786254883,
        "y1": 3.182424783706665,
        "y2": 4.965798854827881
      },
      {
        "r": 0,
        "text": "Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications.",
        "trunc_text": "Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications.",
        "x1": 1.7417066097259521,
        "x2": 2.9837238788604736,
        "y1": 8.108261108398438,
        "y2": 1.0583957433700562
      },
      {
        "r": 0,
        "text": "They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research.",
        "trunc_text": "They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system",
        "x1": 4.052515029907227,
        "x2": 2.5527820587158203,
        "y1": 3.1025726795196533,
        "y2": 5.306070327758789
      },
      {
        "r": 0,
        "text": "These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.",
        "trunc_text": "These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding a",
        "x1": 3.9576549530029297,
        "x2": 2.5054900646209717,
        "y1": 3.234697103500366,
        "y2": 5.2496538162231445
      },
      {
        "r": 0,
        "text": "We make our data and code publicly available in https://github.com/AI21Labs/factor.",
        "trunc_text": "We make our data and code publicly available in https://github.com/AI21Labs/factor.",
        "x1": 7.830145835876465,
        "x2": 8.297863960266113,
        "y1": 7.330089092254639,
        "y2": 2.540940523147583
      },
      {
        "r": 0,
        "text": "Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain.",
        "trunc_text": "Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factua",
        "x1": 3.072904586791992,
        "x2": 2.0130043029785156,
        "y1": 3.637228488922119,
        "y2": 5.633139610290527
      },
      {
        "r": 0,
        "text": "Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts.",
        "trunc_text": "Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the se",
        "x1": 2.984764814376831,
        "x2": 1.9522143602371216,
        "y1": 3.6976468563079834,
        "y2": 5.541822910308838
      },
      {
        "r": 0,
        "text": "We propose FACTOR:",
        "trunc_text": "We propose FACTOR:",
        "x1": 4.781832695007324,
        "x2": 6.155531883239746,
        "y1": 8.214547157287598,
        "y2": 4.119602203369141
      },
      {
        "r": 0,
        "text": "Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality.",
        "trunc_text": "Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality.",
        "x1": 2.9682695865631104,
        "x2": 1.874060034751892,
        "y1": 3.5990781784057617,
        "y2": 5.5275115966796875
      },
      {
        "r": 0,
        "text": "FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements.",
        "trunc_text": "FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate ",
        "x1": 2.959289312362671,
        "x2": 1.9289075136184692,
        "y1": 3.6615118980407715,
        "y2": 5.519798278808594
      },
      {
        "r": 0,
        "text": "We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR.",
        "trunc_text": "We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR.",
        "x1": 4.893603324890137,
        "x2": 4.52067756652832,
        "y1": 6.277040958404541,
        "y2": 3.960928440093994
      },
      {
        "r": 0,
        "text": "We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.",
        "trunc_text": "We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii",
        "x1": 2.9839344024658203,
        "x2": 1.9558334350585938,
        "y1": 3.766167640686035,
        "y2": 5.448879241943359
      },
      {
        "r": 0,
        "text": "LCC provided an anonymised dataset comprising 14360 records of young people under the age of 18.",
        "trunc_text": "LCC provided an anonymised dataset comprising 14360 records of young people under the age of 18.",
        "x1": 5.283229351043701,
        "x2": 6.067684650421143,
        "y1": 7.671830654144287,
        "y2": 2.254228115081787
      },
      {
        "r": 0,
        "text": "Local authorities in England, such as Leicestershire County Council (LCC), provide Early Help services that can be offered at any point in a young person's life when they experience difficulties that cannot be supported by universal services alone, such as schools.",
        "trunc_text": "Local authorities in England, such as Leicestershire County Council (LCC), provide Early Help services that can be offer",
        "x1": 3.8634586334228516,
        "x2": 5.162888050079346,
        "y1": 7.691708564758301,
        "y2": 2.1476826667785645
      },
      {
        "r": 0,
        "text": "This paper investigates the utilisation of machine learning (ML) to assist experts in identifying families that may need to be referred for Early Help assessment and support.  ",
        "trunc_text": "This paper investigates the utilisation of machine learning (ML) to assist experts in identifying families that may need",
        "x1": 3.8392770290374756,
        "x2": 5.056881427764893,
        "y1": 7.670444965362549,
        "y2": 2.1991384029388428
      },
      {
        "r": 0,
        "text": "The dataset was pre-processed, machine learning models were build, and experiments were conducted to validate and test the performance of the models.",
        "trunc_text": "The dataset was pre-processed, machine learning models were build, and experiments were conducted to validate and test t",
        "x1": 4.899386882781982,
        "x2": 5.446129322052002,
        "y1": 7.018771171569824,
        "y2": 3.053439140319824
      },
      {
        "r": 0,
        "text": "Bias mitigation techniques were applied to improve the fairness of these models.",
        "trunc_text": "Bias mitigation techniques were applied to improve the fairness of these models.",
        "x1": 3.7351534366607666,
        "x2": 3.601408004760742,
        "y1": 5.987747669219971,
        "y2": 4.132049560546875
      },
      {
        "r": 0,
        "text": "During testing, while the models demonstrated the capability to identify young people requiring intervention or early help, they also produced a significant number of false positives, especially when constructed with imbalanced data, incorrectly identifying individuals who most likely did not need an Early Help referral.",
        "trunc_text": "During testing, while the models demonstrated the capability to identify young people requiring intervention or early he",
        "x1": 3.840733528137207,
        "x2": 5.004271030426025,
        "y1": 7.804636478424072,
        "y2": 2.182842254638672
      },
      {
        "r": 0,
        "text": "This paper empirically explores the suitability of data-driven ML models for identifying young people who may require Early Help services and discusses their appropriateness and limitations for this task.",
        "trunc_text": "This paper empirically explores the suitability of data-driven ML models for identifying young people who may require Ea",
        "x1": 3.86464262008667,
        "x2": 5.082581520080566,
        "y1": 7.729156017303467,
        "y2": 2.1672964096069336
      },
      {
        "r": 0,
        "text": "Experiments on a new dataset of real images show that adding RePoGen data to the COCO surpasses previous attempts to top-view pose estimation and significantly improves performance on the bottom-view dataset",
        "trunc_text": "Experiments on a new dataset of real images show that adding RePoGen data to the COCO surpasses previous attempts to top",
        "x1": 2.0892086029052734,
        "x2": 3.4395382404327393,
        "y1": 8.059172630310059,
        "y2": 1.0880658626556396
      },
      {
        "r": 0,
        "text": "The code and the datasets are available on the project website.",
        "trunc_text": "The code and the datasets are available on the project website.",
        "x1": 7.343008518218994,
        "x2": 7.870741367340088,
        "y1": 7.741029739379883,
        "y2": 2.1971828937530518
      },
      {
        "r": 0,
        "text": "Human Pose Estimation is a thoroughly researched problem; however, most datasets focus on the side and front-view scenarios.",
        "trunc_text": "Human Pose Estimation is a thoroughly researched problem; however, most datasets focus on the side and front-view scenar",
        "x1": 2.351846694946289,
        "x2": 3.9057440757751465,
        "y1": 9.410027503967285,
        "y2": -0.2647222578525543
      },
      {
        "r": 0,
        "text": "We address the limitation by proposing a novel approach that tackles the challenges posed by extreme viewpoints and poses.",
        "trunc_text": "We address the limitation by proposing a novel approach that tackles the challenges posed by extreme viewpoints and pose",
        "x1": 4.309089183807373,
        "x2": 5.3316168785095215,
        "y1": 8.799115180969238,
        "y2": 0.8739465475082397
      },
      {
        "r": 0,
        "text": "We introduce a new method for synthetic data generation - RePoGen, RarE POses GENerator - with comprehensive control over pose and view to augment the COCO dataset. .",
        "trunc_text": "We introduce a new method for synthetic data generation - RePoGen, RarE POses GENerator - with comprehensive control ove",
        "x1": 2.1325082778930664,
        "x2": 3.5008041858673096,
        "y1": 8.014270782470703,
        "y2": 1.0959439277648926
      },
      {
        "r": 0,
        "text": "Through an extensive ablation study on both the top and bottom view data, we elucidate the contributions of methodological choices and demonstrate improved performance.",
        "trunc_text": "Through an extensive ablation study on both the top and bottom view data, we elucidate the contributions of methodologic",
        "x1": 5.834446907043457,
        "x2": 6.646509170532227,
        "y1": 8.692609786987305,
        "y2": 5.294326305389404
      },
      {
        "r": 0,
        "text": "We propose IntelliGraphs, a set of five new Knowledge Graph datasets.",
        "trunc_text": "We propose IntelliGraphs, a set of five new Knowledge Graph datasets.",
        "x1": 3.286736011505127,
        "x2": 2.932844877243042,
        "y1": 4.788530349731445,
        "y2": 4.118474960327148
      },
      {
        "r": 0,
        "text": "We also present the dataset generator that produced the synthetic datasets.",
        "trunc_text": "We also present the dataset generator that produced the synthetic datasets.",
        "x1": 5.219981670379639,
        "x2": 5.323680400848389,
        "y1": 6.984155654907227,
        "y2": 2.2553670406341553
      },
      {
        "r": 0,
        "text": "Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations.",
        "trunc_text": "Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations.",
        "x1": 3.153109312057495,
        "x2": 2.8177218437194824,
        "y1": 4.785407543182373,
        "y2": 4.086940288543701
      },
      {
        "r": 0,
        "text": "A key task in the literature is predicting missing links between entities.",
        "trunc_text": "A key task in the literature is predicting missing links between entities.",
        "x1": 3.7027060985565186,
        "x2": 2.4882144927978516,
        "y1": 3.209271192550659,
        "y2": 6.01980447769165
      },
      {
        "r": 0,
        "text": "However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure.",
        "trunc_text": "However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure.",
        "x1": 3.3020966053009033,
        "x2": 2.842487096786499,
        "y1": 4.697004795074463,
        "y2": 4.242782115936279
      },
      {
        "r": 0,
        "text": "Semantics is crucial in several downstream tasks, such as query answering or reasoning.",
        "trunc_text": "Semantics is crucial in several downstream tasks, such as query answering or reasoning.",
        "x1": 4.4638895988464355,
        "x2": 3.574233055114746,
        "y1": 4.204841613769531,
        "y2": 5.667728900909424
      },
      {
        "r": 0,
        "text": "We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs.  ",
        "trunc_text": "We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs.  ",
        "x1": 3.217851161956787,
        "x2": 3.044847249984741,
        "y1": 4.946850776672363,
        "y2": 4.037087917327881
      },
      {
        "r": 0,
        "text": "The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference.",
        "trunc_text": "The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference",
        "x1": 3.3593902587890625,
        "x2": 3.012685775756836,
        "y1": 4.677827835083008,
        "y2": 4.240941047668457
      },
      {
        "r": 0,
        "text": "We also present the dataset generator that produced the synthetic datased on traditional KGEs.",
        "trunc_text": "We also present the dataset generator that produced the synthetic datased on traditional KGEs.",
        "x1": 5.226086616516113,
        "x2": 5.421627998352051,
        "y1": 6.974915504455566,
        "y2": 2.3074846267700195
      },
      {
        "r": 0,
        "text": "We evaluate their expressiveness and show that these models cannot capture the semantics.",
        "trunc_text": "We evaluate their expressiveness and show that these models cannot capture the semantics.",
        "x1": 4.318480968475342,
        "x2": 3.1575419902801514,
        "y1": 4.068716049194336,
        "y2": 5.857518196105957
      },
      {
        "r": 0,
        "text": "We believe this benchmark will encourage the development of machine learning models that emphasize semantic understanding.",
        "trunc_text": "We believe this benchmark will encourage the development of machine learning models that emphasize semantic understandin",
        "x1": 3.9066121578216553,
        "x2": 3.162229537963867,
        "y1": 4.266252040863037,
        "y2": 5.173041820526123
      },
      {
        "r": 0,
        "text": "In the realm of Tiny AI, we introduce \"You Only Look at Interested Cells\" (YOLIC), an efficient method for object localization and classification on edge devices.",
        "trunc_text": "In the realm of Tiny AI, we introduce \"You Only Look at Interested Cells\" (YOLIC), an efficient method for object locali",
        "x1": 0.4517362713813782,
        "x2": 1.985986590385437,
        "y1": 7.474802017211914,
        "y2": 1.3670985698699951
      },
      {
        "r": 0,
        "text": "Seamlessly blending the strengths of semantic segmentation and object detection, YOLIC offers superior computational efficiency and precision.",
        "trunc_text": "Seamlessly blending the strengths of semantic segmentation and object detection, YOLIC offers superior computational eff",
        "x1": 0.55210280418396,
        "x2": 2.018122434616089,
        "y1": 7.348533630371094,
        "y2": 1.3992501497268677
      },
      {
        "r": 0,
        "text": "By adopting Cells of Interest for classification instead of individual pixels, YOLIC encapsulates relevant information, reduces computational load, and enables rough object shape inference.",
        "trunc_text": "By adopting Cells of Interest for classification instead of individual pixels, YOLIC encapsulates relevant information, ",
        "x1": 0.4269891679286957,
        "x2": 2.009557008743286,
        "y1": 7.451499938964844,
        "y2": 1.3421317338943481
      },
      {
        "r": 0,
        "text": "Importantly, the need for bounding box regression is obviated, as YOLIC capitalizes on the predetermined cell configuration that provides information about potential object location, size, and shape.",
        "trunc_text": "Importantly, the need for bounding box regression is obviated, as YOLIC capitalizes on the predetermined cell configurat",
        "x1": 0.42352160811424255,
        "x2": 2.0031540393829346,
        "y1": 7.514917373657227,
        "y2": 1.2475155591964722
      },
      {
        "r": 0,
        "text": "To tackle the issue of single-label classification limitations, a multi-label classification approach is applied to each cell, effectively recognizing overlapping or closely situated objects.",
        "trunc_text": "To tackle the issue of single-label classification limitations, a multi-label classification approach is applied to each",
        "x1": 1.2840995788574219,
        "x2": 0.791869044303894,
        "y1": 4.443398475646973,
        "y2": 3.768854856491089
      },
      {
        "r": 0,
        "text": "This paper presents extensive experiments on multiple datasets, demonstrating that YOLIC achieves detection performance comparable to the state-of-the-art YOLO algorithms while surpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU.",
        "trunc_text": "This paper presents extensive experiments on multiple datasets, demonstrating that YOLIC achieves detection performance ",
        "x1": 0.41193050146102905,
        "x2": 1.888379454612732,
        "y1": 7.325236797332764,
        "y2": 1.4942655563354492
      },
      {
        "r": 0,
        "text": "All resources related to this study, including datasets, cell designer, image annotation tool, and source code, have been made publicly available on our project website at https://kai3316.github.io/yolic.github.io",
        "trunc_text": "All resources related to this study, including datasets, cell designer, image annotation tool, and source code, have bee",
        "x1": 7.010490894317627,
        "x2": 7.685371398925781,
        "y1": 8.01577091217041,
        "y2": 1.930368185043335
      },
      {
        "r": 0,
        "text": "Most of the existing LiDAR-inertial navigation systems are based on frame-to-map registrations, leading to inconsistency in state estimation.",
        "trunc_text": "Most of the existing LiDAR-inertial navigation systems are based on frame-to-map registrations, leading to inconsistency",
        "x1": 1.476702332496643,
        "x2": 3.0824832916259766,
        "y1": 9.540315628051758,
        "y2": -0.4017775356769562
      },
      {
        "r": 0,
        "text": "The newest solid-state LiDAR with a non-repetitive scanning pattern makes it possible to achieve a consistent LiDAR-inertial estimator by employing a frame-to-frame data association.",
        "trunc_text": "The newest solid-state LiDAR with a non-repetitive scanning pattern makes it possible to achieve a consistent LiDAR-iner",
        "x1": 1.481720209121704,
        "x2": 3.061177968978882,
        "y1": 9.555747985839844,
        "y2": -0.3883163630962372
      },
      {
        "r": 0,
        "text": "In this letter, we propose a robust and consistent frame-to-frame LiDAR-inertial navigation system (FF-LINS) for solid-state LiDARs.",
        "trunc_text": "In this letter, we propose a robust and consistent frame-to-frame LiDAR-inertial navigation system (FF-LINS) for solid-s",
        "x1": 1.4851901531219482,
        "x2": 3.0735833644866943,
        "y1": 9.52778148651123,
        "y2": -0.3683302700519562
      },
      {
        "r": 0,
        "text": "With the INS-centric LiDAR frame processing, the keyframe point-cloud map is built using the accumulated point clouds to construct the frame-to-frame data association.",
        "trunc_text": "With the INS-centric LiDAR frame processing, the keyframe point-cloud map is built using the accumulated point clouds to",
        "x1": 1.5104273557662964,
        "x2": 3.090989828109741,
        "y1": 9.458608627319336,
        "y2": -0.32252582907676697
      },
      {
        "r": 0,
        "text": "The LiDAR frame-to-frame and the inertial measurement unit (IMU) preintegration measurements are tightly integrated using the factor graph optimization, with online calibration of the LiDAR-IMU extrinsic and time-delay parameters.",
        "trunc_text": "The LiDAR frame-to-frame and the inertial measurement unit (IMU) preintegration measurements are tightly integrated usin",
        "x1": 1.47223961353302,
        "x2": 3.0548009872436523,
        "y1": 9.555376052856445,
        "y2": -0.36745721101760864
      },
      {
        "r": 0,
        "text": "The experiments on the public and private datasets demonstrate that the proposed FF-LINS achieves superior accuracy and robustness than the state-of-the-art systems.",
        "trunc_text": "The experiments on the public and private datasets demonstrate that the proposed FF-LINS achieves superior accuracy and ",
        "x1": 4.420130252838135,
        "x2": 5.095881938934326,
        "y1": 7.298812389373779,
        "y2": 2.781773805618286
      },
      {
        "r": 0,
        "text": "Besides, the LiDAR-IMU extrinsic and time-delay parameters are estimated effectively, and the online calibration notably improves the pose accuracy.",
        "trunc_text": "Besides, the LiDAR-IMU extrinsic and time-delay parameters are estimated effectively, and the online calibration notably",
        "x1": 1.425889492034912,
        "x2": 3.055384397506714,
        "y1": 9.408090591430664,
        "y2": -0.3203655183315277
      },
      {
        "r": 0,
        "text": "The proposed FF-LINS and the employed datasets are open-sourced on GitHub (https://github.com/i2Nav-WHU/FF-LINS).",
        "trunc_text": "The proposed FF-LINS and the employed datasets are open-sourced on GitHub (https://github.com/i2Nav-WHU/FF-LINS).",
        "x1": 6.95054292678833,
        "x2": 7.668269157409668,
        "y1": 7.9620466232299805,
        "y2": 1.893905758857727
      },
      {
        "r": 0,
        "text": "Considering these limitations, we introduce the first video-based retinal dataset by employing handheld devices for data acquisition.",
        "trunc_text": "Considering these limitations, we introduce the first video-based retinal dataset by employing handheld devices for data",
        "x1": 1.988149881362915,
        "x2": 3.530791997909546,
        "y1": 7.822462558746338,
        "y2": 1.2577635049819946
      },
      {
        "r": 0,
        "text": "The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 years old.",
        "trunc_text": "The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients f",
        "x1": 2.5973334312438965,
        "x2": 3.7523882389068604,
        "y1": 7.499630451202393,
        "y2": 1.8356819152832031
      },
      {
        "r": 0,
        "text": "Specifically, the dataset provides three levels of spatial annotations",
        "trunc_text": "Specifically, the dataset provides three levels of spatial annotations",
        "x1": 1.8942384719848633,
        "x2": 4.048120975494385,
        "y1": 7.741246223449707,
        "y2": 1.4995009899139404
      },
      {
        "r": 0,
        "text": "In addition, the dataset offers temporal annotations",
        "trunc_text": "In addition, the dataset offers temporal annotations",
        "x1": 4.8063812255859375,
        "x2": 5.423974514007568,
        "y1": 7.542178630828857,
        "y2": 1.9797914028167725
      },
      {
        "r": 0,
        "text": "We hope this challenging dataset would significantly contribute to the development of eye disease diagnosis and early prevention.",
        "trunc_text": "We hope this challenging dataset would significantly contribute to the development of eye disease diagnosis and early pr",
        "x1": 4.05735969543457,
        "x2": 5.752993106842041,
        "y1": 7.847346305847168,
        "y2": 2.2025325298309326
      },
      {
        "r": 0,
        "text": "Retinal vessel segmentation is generally grounded in image-based datasets collected with bench-top devices.",
        "trunc_text": "Retinal vessel segmentation is generally grounded in image-based datasets collected with bench-top devices.",
        "x1": 1.2234524488449097,
        "x2": 3.0544137954711914,
        "y1": 7.312102317810059,
        "y2": 1.2972486019134521
      },
      {
        "r": 0,
        "text": "The static images naturally lose the dynamic characteristics of retina fluctuation, resulting in diminished dataset richness, and the usage of bench-top devices further restricts dataset scalability due to its limited accessibility.  ",
        "trunc_text": "The static images naturally lose the dynamic characteristics of retina fluctuation, resulting in diminished dataset rich",
        "x1": 2.1335289478302,
        "x2": 3.6063344478607178,
        "y1": 7.9066972732543945,
        "y2": 1.2171951532363892
      },
      {
        "r": 0,
        "text": "The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 he landscape of vasculature segmentation.",
        "trunc_text": "The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients f",
        "x1": 2.412266492843628,
        "x2": 3.513892412185669,
        "y1": 7.436997890472412,
        "y2": 1.6284910440444946
      },
      {
        "r": 0,
        "text": "Specifically, the dataset provides three levels of spatial annotations: binary vessel masks for overall retinal structure delineation, general vein-artery masks for distinguishing the vein and artery, and fine-grained vein-artery masks for further characterizing the granulari annotations that capture the vessel pulsation characteristics, assisting in detecting ocular diseases that require fine-grained recognition of hemodynamic fluctuation.",
        "trunc_text": "Specifically, the dataset provides three levels of spatial annotations: binary vessel masks for overall retinal structur",
        "x1": 1.7210348844528198,
        "x2": 3.6482090950012207,
        "y1": 7.518448829650879,
        "y2": 1.5445380210876465
      },
      {
        "r": 0,
        "text": "In application, our dataset exhibits a significant domain shift with respect to data captured by bench-top devices, thus posing great chaprovide evaluation metrics and benchmark results on our dataset, reflecting both the potential and challenges it offers for vessel segmentation tasks.",
        "trunc_text": "In application, our dataset exhibits a significant domain shift with respect to data captured by bench-top devices, thus",
        "x1": 0.8552289009094238,
        "x2": 2.6974668502807617,
        "y1": 7.046570301055908,
        "y2": 1.4330096244812012
      },
      {
        "r": 0,
        "text": "This study addressed the complex task of sentiment analysis on a dataset of 119,988 original tweets from Weibo using a Convolutional Neural Network (CNN), offering a new approach to Natural Language Processing (NLP).",
        "trunc_text": "This study addressed the complex task of sentiment analysis on a dataset of 119,988 original tweets from Weibo using a C",
        "x1": 3.463686227798462,
        "x2": 2.9272069931030273,
        "y1": 3.7460689544677734,
        "y2": 5.49973726272583
      },
      {
        "r": 0,
        "text": "The data, sourced from Baidu's PaddlePaddle AI platform, were meticulously preprocessed, tokenized, and categorized based on sentiment labels.",
        "trunc_text": "The data, sourced from Baidu's PaddlePaddle AI platform, were meticulously preprocessed, tokenized, and categorized base",
        "x1": 2.5863187313079834,
        "x2": 3.954294443130493,
        "y1": 3.49735426902771,
        "y2": 6.6991777420043945
      },
      {
        "r": 0,
        "text": "A CNN-based model was utilized, leveraging word embeddings for feature extraction, and trained to perform sentiment classification.",
        "trunc_text": "A CNN-based model was utilized, leveraging word embeddings for feature extraction, and trained to perform sentiment clas",
        "x1": 3.5360639095306396,
        "x2": 2.9118330478668213,
        "y1": 3.754868745803833,
        "y2": 5.435330867767334
      },
      {
        "r": 0,
        "text": "The model achieved a macro-average F1-score of approximately 0.73 on the test set, showing balanced performance across positive, neutral, and negative sentiments.",
        "trunc_text": "The model achieved a macro-average F1-score of approximately 0.73 on the test set, showing balanced performance across p",
        "x1": 4.205470561981201,
        "x2": 5.099634170532227,
        "y1": 6.119970321655273,
        "y2": 3.93247127532959
      },
      {
        "r": 0,
        "text": "The findings underscore the effectiveness of CNNs for sentiment analysis tasks, with implications for practical applications in social media analysis, market research, and policy studies.",
        "trunc_text": "The findings underscore the effectiveness of CNNs for sentiment analysis tasks, with implications for practical applicat",
        "x1": 3.5024006366729736,
        "x2": 2.9213290214538574,
        "y1": 3.827631950378418,
        "y2": 5.445930004119873
      },
      {
        "r": 0,
        "text": "The complete experimental content and code have been made publicly available on the Kaggle data platform for further research and development.",
        "trunc_text": "The complete experimental content and code have been made publicly available on the Kaggle data platform for further res",
        "x1": 7.721835136413574,
        "x2": 8.224675178527832,
        "y1": 7.3529863357543945,
        "y2": 2.5913383960723877
      },
      {
        "r": 0,
        "text": "Future work may involve exploring different architectures, such as Recurrent Neural Networks (RNN) or transformers, or using more complex pre-trained models like BERT, to further improve the model's ability to understand linguistic nuances and context.",
        "trunc_text": "Future work may involve exploring different architectures, such as Recurrent Neural Networks (RNN) or transformers, or u",
        "x1": 4.497585296630859,
        "x2": 3.590989589691162,
        "y1": 4.1450676918029785,
        "y2": 5.315431594848633
      },
      {
        "r": 0,
        "text": "To address the scarcity of annotated corpora with realistic multi-issue negotiation dialogues, we use GPT-3 to build GPT-Negochat, a synthesized dataset that we make publicly available.",
        "trunc_text": "To address the scarcity of annotated corpora with realistic multi-issue negotiation dialogues, we use GPT-3 to build GPT",
        "x1": 6.170273780822754,
        "x2": 4.369672775268555,
        "y1": 3.5205280780792236,
        "y2": 7.120217800140381
      },
      {
        "r": 0,
        "text": "Automated negotiation support systems aim to help human negotiators reach more favorable outcomes in multi-issue negotiations (e.g., an employer and a candidate negotiating over issues such as salary, hours, and promotions before a job offer).",
        "trunc_text": "Automated negotiation support systems aim to help human negotiators reach more favorable outcomes in multi-issue negotia",
        "x1": 6.1233086585998535,
        "x2": 4.434001922607422,
        "y1": 3.7731869220733643,
        "y2": 7.270095348358154
      },
      {
        "r": 0,
        "text": "To be successful, these systems must accurately track agreements reached by participants in real-time.",
        "trunc_text": "To be successful, these systems must accurately track agreements reached by participants in real-time.",
        "x1": 5.994573593139648,
        "x2": 4.461336135864258,
        "y1": 3.9376416206359863,
        "y2": 7.297337055206299
      },
      {
        "r": 0,
        "text": "Existing approaches either focus on task-oriented dialogues or produce unstructured outputs, rendering them unsuitable for this objective.",
        "trunc_text": "Existing approaches either focus on task-oriented dialogues or produce unstructured outputs, rendering them unsuitable f",
        "x1": 5.670635223388672,
        "x2": 3.673529624938965,
        "y1": 3.1784026622772217,
        "y2": 6.952063083648682
      },
      {
        "r": 0,
        "text": "Our work introduces the novel task of agreement tracking for two-party multi-issue negotiations, which requires continuous monitoring of agreements within a structured state space.  ",
        "trunc_text": "Our work introduces the novel task of agreement tracking for two-party multi-issue negotiations, which requires continuo",
        "x1": 6.069507122039795,
        "x2": 4.432760238647461,
        "y1": 3.791464328765869,
        "y2": 7.309289932250977
      },
      {
        "r": 0,
        "text": "We present a strong initial baseline for our task by transfer-learning a T5 model trained on the MultiWOZ 2.4 corpus.",
        "trunc_text": "We present a strong initial baseline for our task by transfer-learning a T5 model trained on the MultiWOZ 2.4 corpus.",
        "x1": 3.806551694869995,
        "x2": 2.9751319885253906,
        "y1": 3.510150194168091,
        "y2": 5.764016151428223
      },
      {
        "r": 0,
        "text": "Pre-training T5-small and T5-base on MultiWOZ 2.4's DST task enhances results by 21% and 9% respectively over training solely on GPT-Negochat.",
        "trunc_text": "Pre-training T5-small and T5-base on MultiWOZ 2.4's DST task enhances results by 21% and 9% respectively over training s",
        "x1": 6.322911262512207,
        "x2": 4.458855628967285,
        "y1": 3.586141586303711,
        "y2": 6.770811080932617
      },
      {
        "r": 0,
        "text": "We validate our method's sample-efficiency via smaller training subset experiments.",
        "trunc_text": "We validate our method's sample-efficiency via smaller training subset experiments.",
        "x1": 3.741530656814575,
        "x2": 4.302059173583984,
        "y1": 6.320467948913574,
        "y2": 3.741445541381836
      },
      {
        "r": 0,
        "text": "By releasing GPT-Negochat and our baseline models, we aim to encourage further research in multi-issue negotiation dialogue agreement tracking.",
        "trunc_text": "By releasing GPT-Negochat and our baseline models, we aim to encourage further research in multi-issue negotiation dialo",
        "x1": 6.19503927230835,
        "x2": 4.359574317932129,
        "y1": 3.613154172897339,
        "y2": 7.185267448425293
      },
      {
        "r": 0,
        "text": "This work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset for autonomous driving on water surfaces",
        "trunc_text": "This work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset for autonomous driving on water surf",
        "x1": 0.6312286853790283,
        "x2": 2.6168832778930664,
        "y1": 9.24628734588623,
        "y2": -0.2760448157787323
      },
      {
        "r": 0,
        "text": "Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at pixel-level and point-level, respectively",
        "trunc_text": "Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at p",
        "x1": 0.6727727055549622,
        "x2": 2.62404465675354,
        "y1": 9.21190071105957,
        "y2": -0.2666376233100891
      },
      {
        "r": 0,
        "text": "WaterScenes dataset is public on https://waterscenes.github.io.",
        "trunc_text": "WaterScenes dataset is public on https://waterscenes.github.io.",
        "x1": 7.230171203613281,
        "x2": 7.71897029876709,
        "y1": 7.922447681427002,
        "y2": 2.0012047290802
      },
      {
        "r": 0,
        "text": "Autonomous driving on water surfaces plays an essential role in executing hazardous and time-consuming missions, such as maritime surveillance, survivors rescue, environmental monitoring, hydrography mapping and waste cleaning. .",
        "trunc_text": "Autonomous driving on water surfaces plays an essential role in executing hazardous and time-consuming missions, such as",
        "x1": 0.6071804761886597,
        "x2": 2.5818283557891846,
        "y1": 9.245743751525879,
        "y2": -0.30369967222213745
      },
      {
        "r": 0,
        "text": "Equipped with a 4D radar and a monocular camera, our Unmanned Surface Vehicle (USV) proffers all-weather solutions for discerning object-related information, including color, shape, texture, range, velocity, azimuth, and elevation.",
        "trunc_text": "Equipped with a 4D radar and a monocular camera, our Unmanned Surface Vehicle (USV) proffers all-weather solutions for d",
        "x1": 0.6103013753890991,
        "x2": 2.5797722339630127,
        "y1": 9.20933723449707,
        "y2": -0.27591672539711
      },
      {
        "r": 0,
        "text": "Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at pixelprovide annotations for free-space segmentation and waterline segmentation.",
        "trunc_text": "Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at p",
        "x1": 0.4754388630390167,
        "x2": 2.3964030742645264,
        "y1": 9.06745433807373,
        "y2": -0.13817283511161804
      },
      {
        "r": 0,
        "text": "Leveraging the multi-task and multi-modal data, we conduct numerous experiments on the single modality of radar and camera, as well as the fused modalities.",
        "trunc_text": "Leveraging the multi-task and multi-modal data, we conduct numerous experiments on the single modality of radar and came",
        "x1": 0.7901484370231628,
        "x2": 2.763660430908203,
        "y1": 9.200766563415527,
        "y2": -0.18974275887012482
      },
      {
        "r": 0,
        "text": "Results demonstrate that 4D radar-camera fusion can considerably enhance the robustness of perception on water surfaces, especially in adverse lighting and weather conditions.",
        "trunc_text": "Results demonstrate that 4D radar-camera fusion can considerably enhance the robustness of perception on water surfaces,",
        "x1": 0.6490762829780579,
        "x2": 2.6522486209869385,
        "y1": 9.224425315856934,
        "y2": -0.27735450863838196
      },
      {
        "r": 0,
        "text": "This paper presents the introduction of a framework called \\textit{Ashaar} https://github.com/ARBML/Ashaar, which encompasses a collection of datasets and pre-trained models designed specifically for the analysis and generation of Arabic poetry.",
        "trunc_text": "This paper presents the introduction of a framework called \\textit{Ashaar} https://github.com/ARBML/Ashaar, which encomp",
        "x1": 3.323164939880371,
        "x2": 1.1430792808532715,
        "y1": 1.5877938270568848,
        "y2": 6.8247857093811035
      },
      {
        "r": 0,
        "text": "Furthermore, as part of this endeavor, we provide four datasets: one for poetry generation, another for diacritization, and two for Arudi-style prediction.",
        "trunc_text": "Furthermore, as part of this endeavor, we provide four datasets: one for poetry generation, another for diacritization, ",
        "x1": 3.245338201522827,
        "x2": 1.004160761833191,
        "y1": 1.6127252578735352,
        "y2": 6.727263927459717
      },
      {
        "r": 0,
        "text": "These datasets aim to facilitate research and development in the field of Arabic poetry by enabling researchers and enthusiasts to delve into the nuances of this rich literary tradition.",
        "trunc_text": "These datasets aim to facilitate research and development in the field of Arabic poetry by enabling researchers and enth",
        "x1": 3.334397315979004,
        "x2": 1.0941295623779297,
        "y1": 1.5698314905166626,
        "y2": 6.835604667663574
      },
      {
        "r": 0,
        "text": "Poetry holds immense significance within the cultural and traditional fabric of any nation.",
        "trunc_text": "Poetry holds immense significance within the cultural and traditional fabric of any nation.",
        "x1": 3.3157050609588623,
        "x2": 1.0571545362472534,
        "y1": 1.5436980724334717,
        "y2": 6.810660362243652
      },
      {
        "r": 0,
        "text": "It serves as a vehicle for poets to articulate their emotions, preserve customs, and convey the essence of their culture.",
        "trunc_text": "It serves as a vehicle for poets to articulate their emotions, preserve customs, and convey the essence of their culture",
        "x1": 3.2984695434570312,
        "x2": 1.0587939023971558,
        "y1": 1.5662040710449219,
        "y2": 6.792717456817627
      },
      {
        "r": 0,
        "text": "Arabic poetry is no exception, having played a cherished role in the heritage of the Arabic community throughout history and maintaining its relevance in the present era.",
        "trunc_text": "Arabic poetry is no exception, having played a cherished role in the heritage of the Arabic community throughout history",
        "x1": 3.3707807064056396,
        "x2": 1.0749415159225464,
        "y1": 1.520310878753662,
        "y2": 6.857374668121338
      },
      {
        "r": 0,
        "text": "Typically, comprehending Arabic poetry necessitates the expertise of a linguist who can analyze its content and assess its quality.  ",
        "trunc_text": "Typically, comprehending Arabic poetry necessitates the expertise of a linguist who can analyze its content and assess i",
        "x1": 3.325005292892456,
        "x2": 1.08867347240448,
        "y1": 1.5565375089645386,
        "y2": 6.8554277420043945
      },
      {
        "r": 0,
        "text": "The pipeline established within our proposed approach encompasses various aspects of poetry, such as meter, theme, and era classification.",
        "trunc_text": "The pipeline established within our proposed approach encompasses various aspects of poetry, such as meter, theme, and e",
        "x1": 3.2932682037353516,
        "x2": 1.0658811330795288,
        "y1": 1.5697258710861206,
        "y2": 6.797158718109131
      },
      {
        "r": 0,
        "text": "It also incorporates automatic poetry diacritization, enabling more intricate analyses like automated extraction of the \\textit{Arudi} style.",
        "trunc_text": "It also incorporates automatic poetry diacritization, enabling more intricate analyses like automated extraction of the ",
        "x1": 3.3081138134002686,
        "x2": 1.0939580202102661,
        "y1": 1.6037938594818115,
        "y2": 6.762388706207275
      },
      {
        "r": 0,
        "text": "Additionally, we explore the feasibility of generating conditional poetry through the pre-training of a character-based GPT model.",
        "trunc_text": "Additionally, we explore the feasibility of generating conditional poetry through the pre-training of a character-based ",
        "x1": 3.3086180686950684,
        "x2": 1.1047333478927612,
        "y1": 1.5841485261917114,
        "y2": 6.797357082366943
      },
      {
        "r": 0,
        "text": "These datasets aim to facilitate research and development in the field of Arabic poetry b",
        "trunc_text": "These datasets aim to facilitate research and development in the field of Arabic poetry b",
        "x1": 3.3251852989196777,
        "x2": 1.1063321828842163,
        "y1": 1.5855764150619507,
        "y2": 6.829399108886719
      },
      {
        "r": 0,
        "text": "We build LogBench, the first logging statement generation dataset.",
        "trunc_text": "We build LogBench, the first logging statement generation dataset.",
        "x1": 7.15985107421875,
        "x2": 5.548737525939941,
        "y1": 4.403253555297852,
        "y2": 6.723718166351318
      },
      {
        "r": 0,
        "text": "Automated logging statement generation techniques facilitate developers in writing appropriate logging statements that document software behaviors.",
        "trunc_text": "Automated logging statement generation techniques facilitate developers in writing appropriate logging statements that d",
        "x1": 7.172103404998779,
        "x2": 5.535370349884033,
        "y1": 4.398064136505127,
        "y2": 6.7442545890808105
      },
      {
        "r": 0,
        "text": "Current retrieval-based and learning-based logging methods fail to provide accurate logging statements in complex software.",
        "trunc_text": "Current retrieval-based and learning-based logging methods fail to provide accurate logging statements in complex softwa",
        "x1": 7.1477556228637695,
        "x2": 5.558080196380615,
        "y1": 4.429258346557617,
        "y2": 6.736593723297119
      },
      {
        "r": 0,
        "text": "Although existing large language models (LLMs) might be a good fit for the task due to their great success in natural language generation and programming language comprehension, their effectiveness and generalization capabilities have not been explored.",
        "trunc_text": "Although existing large language models (LLMs) might be a good fit for the task due to their great success in natural la",
        "x1": 5.197202205657959,
        "x2": 3.786468029022217,
        "y1": 3.8997387886047363,
        "y2": 6.247593879699707
      },
      {
        "r": 0,
        "text": "To this end, this paper performs the first extensive study on applying LLMs for logging statement generation.  ",
        "trunc_text": "To this end, this paper performs the first extensive study on applying LLMs for logging statement generation.  ",
        "x1": 7.12575101852417,
        "x2": 5.501861572265625,
        "y1": 4.406055927276611,
        "y2": 6.725779056549072
      },
      {
        "r": 0,
        "text": "On LogBench, we evaluate the effectiveness and generalization capabilities of eight state-of-the-art LLMs, which include general-purpose and code-specific models ranging from 60M to 175B in size.",
        "trunc_text": "On LogBench, we evaluate the effectiveness and generalization capabilities of eight state-of-the-art LLMs, which include",
        "x1": 6.467416763305664,
        "x2": 5.085686206817627,
        "y1": 4.529810905456543,
        "y2": 6.388935089111328
      },
      {
        "r": 0,
        "text": "Specifically, we evaluate LLM's logging effectiveness by studying 1) their ability to decide logging ingredients, 2) the impact of the internal characteristics of LLMs, and 3) the influence of external factors.",
        "trunc_text": "Specifically, we evaluate LLM's logging effectiveness by studying 1) their ability to decide logging ingredients, 2) the",
        "x1": 7.089049816131592,
        "x2": 5.487966537475586,
        "y1": 4.451214790344238,
        "y2": 6.690939903259277
      },
      {
        "r": 0,
        "text": "We further evaluate LLM's logging generalization capabilities using unseen data derived from code transformation techniques.",
        "trunc_text": "We further evaluate LLM's logging generalization capabilities using unseen data derived from code transformation techniq",
        "x1": 7.089544296264648,
        "x2": 5.526404857635498,
        "y1": 4.458937644958496,
        "y2": 6.690867900848389
      },
      {
        "r": 0,
        "text": "Our study demonstrates that existing LLMs fall short of practical requirements for generating proper logging statement texts.",
        "trunc_text": "Our study demonstrates that existing LLMs fall short of practical requirements for generating proper logging statement t",
        "x1": 7.108256816864014,
        "x2": 5.493927478790283,
        "y1": 4.404536247253418,
        "y2": 6.743771076202393
      },
      {
        "r": 0,
        "text": "We also disclose the impact of internal characteristics and external factors for LLMs in automated logging.",
        "trunc_text": "We also disclose the impact of internal characteristics and external factors for LLMs in automated logging.",
        "x1": 7.096735000610352,
        "x2": 5.50745964050293,
        "y1": 4.436300754547119,
        "y2": 6.728611469268799
      },
      {
        "r": 0,
        "text": "In addition, we observe that existing LLMs cannot generalize to logging unseen code, revealing their unsatisfactory generalization capabilities.",
        "trunc_text": "In addition, we observe that existing LLMs cannot generalize to logging unseen code, revealing their unsatisfactory gene",
        "x1": 7.04311990737915,
        "x2": 5.4907450675964355,
        "y1": 4.484971046447754,
        "y2": 6.664766788482666
      },
      {
        "r": 0,
        "text": "Based on our findings, we further discuss three implications that can enhance logging statement generation in the future, such as developing a unified metric for logging quality, incorporating shareable code knowledge into LLMs, and devising suitable prompts.",
        "trunc_text": "Based on our findings, we further discuss three implications that can enhance logging statement generation in the future",
        "x1": 7.133904933929443,
        "x2": 5.525523662567139,
        "y1": 4.427646160125732,
        "y2": 6.756721019744873
      },
      {
        "r": 0,
        "text": "To address this gap, we introduce the Brazilian Leading Universities Entrance eXams (BLUEX), a dataset of entrance exams from the two leading universities in Brazil: UNICAMP and USP",
        "trunc_text": "To address this gap, we introduce the Brazilian Leading Universities Entrance eXams (BLUEX), a dataset of entrance exams",
        "x1": 4.586948394775391,
        "x2": 4.5865888595581055,
        "y1": 6.407375335693359,
        "y2": 3.4063456058502197
      },
      {
        "r": 0,
        "text": "The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects",
        "trunc_text": "The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects",
        "x1": 3.3485026359558105,
        "x2": 2.814943790435791,
        "y1": 3.5522959232330322,
        "y2": 5.724035263061523
      },
      {
        "r": 0,
        "text": "The dataset is also annotated to indicate the position of images in each question, providing a valuable resource for advancing the state-of-the-art in multimodal language understanding and reasoning.",
        "trunc_text": "The dataset is also annotated to indicate the position of images in each question, providing a valuable resource for adv",
        "x1": 4.1653265953063965,
        "x2": 2.6558640003204346,
        "y1": 3.573190212249756,
        "y2": 4.889575004577637
      },
      {
        "r": 0,
        "text": "The data and relevant code can be found at https://github.com/Portuguese-Benchmark-Datasets/BLUEX",
        "trunc_text": "The data and relevant code can be found at https://github.com/Portuguese-Benchmark-Datasets/BLUEX",
        "x1": 4.588359832763672,
        "x2": 4.487637042999268,
        "y1": 6.456459999084473,
        "y2": 3.4015867710113525
      },
      {
        "r": 0,
        "text": "One common trend in recent studies of language models (LMs) is the use of standardized tests for evaluation.",
        "trunc_text": "One common trend in recent studies of language models (LMs) is the use of standardized tests for evaluation.",
        "x1": 5.059284687042236,
        "x2": 3.7202157974243164,
        "y1": 3.7951035499572754,
        "y2": 6.2242231369018555
      },
      {
        "r": 0,
        "text": "However, despite being the fifth most spoken language worldwide, few such evaluations have been conducted in Portuguese.",
        "trunc_text": "However, despite being the fifth most spoken language worldwide, few such evaluations have been conducted in Portuguese.",
        "x1": 4.481550216674805,
        "x2": 2.702950954437256,
        "y1": 3.0300564765930176,
        "y2": 6.594607830047607
      },
      {
        "r": 0,
        "text": "This is mainly due to the lack of high-quality datasets available to the community for carrying out evaluations in Portuguese. .",
        "trunc_text": "This is mainly due to the lack of high-quality datasets available to the community for carrying out evaluations in Portu",
        "x1": 4.903626441955566,
        "x2": 2.7451279163360596,
        "y1": 5.741464138031006,
        "y2": 6.505377769470215
      },
      {
        "r": 0,
        "text": "The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects.",
        "trunc_text": "The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects.",
        "x1": 3.33402156829834,
        "x2": 2.824840545654297,
        "y1": 3.5278589725494385,
        "y2": 5.731333255767822
      },
      {
        "r": 0,
        "text": "Furthermore, BLUEX includes a collection of recently administered examnnotated to indicate the position of images in each question, providing a valuable resource for advancing the state-of-the-art in multimodal language understanding and reasoning.",
        "trunc_text": "Furthermore, BLUEX includes a collection of recently administered examnnotated to indicate the position of images in eac",
        "x1": 4.369040489196777,
        "x2": 2.7326724529266357,
        "y1": 3.5941145420074463,
        "y2": 4.803006172180176
      },
      {
        "r": 0,
        "text": "We describe the creation and characteristics of BLUEX and establish a benchmark through exund at https://github.com/Portuguese-Benchmark-Datasets/BLUEX",
        "trunc_text": "We describe the creation and characteristics of BLUEX and establish a benchmark through exund at https://github.com/Port",
        "x1": 4.531411647796631,
        "x2": 4.438405990600586,
        "y1": 6.425536155700684,
        "y2": 3.303913116455078
      },
      {
        "r": 0,
        "text": "We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014).",
        "trunc_text": "We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014).",
        "x1": 3.5758745670318604,
        "x2": 2.7117421627044678,
        "y1": 3.281181573867798,
        "y2": 6.0122151374816895
      },
      {
        "r": 0,
        "text": "We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic.  ",
        "trunc_text": "We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis",
        "x1": 3.4996261596679688,
        "x2": 2.889939308166504,
        "y1": 3.947049379348755,
        "y2": 5.517518520355225
      },
      {
        "r": 0,
        "text": "To this end, we modify the original texts using a set of phrases - modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009).",
        "trunc_text": "To this end, we modify the original texts using a set of phrases - modifiers that correspond to universal quantifiers, e",
        "x1": 3.3668015003204346,
        "x2": 2.4951424598693848,
        "y1": 3.71126127243042,
        "y2": 5.615832328796387
      },
      {
        "r": 0,
        "text": "We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis.",
        "trunc_text": "We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis.",
        "x1": 3.2903521060943604,
        "x2": 2.349764585494995,
        "y1": 3.486912250518799,
        "y2": 5.66431188583374
      },
      {
        "r": 0,
        "text": "Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules.",
        "trunc_text": "Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules.",
        "x1": 3.2056500911712646,
        "x2": 2.122570514678955,
        "y1": 3.4394869804382324,
        "y2": 5.6712470054626465
      },
      {
        "r": 0,
        "text": "We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios.",
        "trunc_text": "We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by n",
        "x1": 3.5368564128875732,
        "x2": 2.935168504714966,
        "y1": 3.961470127105713,
        "y2": 5.391963958740234
      },
      {
        "r": 0,
        "text": "We found that the performance of NLI models under the zero-shot setting is poor, especially for modified sentences with negation and existential quantifiers.",
        "trunc_text": "We found that the performance of NLI models under the zero-shot setting is poor, especially for modified sentences with ",
        "x1": 3.5226993560791016,
        "x2": 2.9567818641662598,
        "y1": 4.034978866577148,
        "y2": 5.367992877960205
      },
      {
        "r": 0,
        "text": "After fine-tuning this dataset, we observe that models continue to perform poorly over negation, existential and universal modifiers.",
        "trunc_text": "After fine-tuning this dataset, we observe that models continue to perform poorly over negation, existential and univers",
        "x1": 3.7231431007385254,
        "x2": 3.2582931518554688,
        "y1": 4.326298236846924,
        "y2": 5.230172634124756
      },
      {
        "r": 0,
        "text": "Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded.",
        "trunc_text": "Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose comp",
        "x1": 2.668647527694702,
        "x2": 3.8373301029205322,
        "y1": 7.446542739868164,
        "y2": 2.091285467147827
      },
      {
        "r": 0,
        "text": "We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer.",
        "trunc_text": "We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision t",
        "x1": 2.7355082035064697,
        "x2": 3.8630423545837402,
        "y1": 7.39333963394165,
        "y2": 2.0406734943389893
      },
      {
        "r": 0,
        "text": "Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data.",
        "trunc_text": "Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also",
        "x1": 2.704742670059204,
        "x2": 3.8016610145568848,
        "y1": 7.361117362976074,
        "y2": 2.2581515312194824
      },
      {
        "r": 0,
        "text": "This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image.",
        "trunc_text": "This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face ",
        "x1": 2.6529810428619385,
        "x2": 3.6867620944976807,
        "y1": 7.22603702545166,
        "y2": 2.3002257347106934
      },
      {
        "r": 0,
        "text": "To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities.",
        "trunc_text": "To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performan",
        "x1": 4.351984977722168,
        "x2": 4.434479236602783,
        "y1": 6.392691135406494,
        "y2": 3.542738914489746
      },
      {
        "r": 0,
        "text": "The ground truth annotations for this benchmark have been meticulously generated by human annotators, resulting in high accuracy answers due to the smart aggregation of votes.",
        "trunc_text": "The ground truth annotations for this benchmark have been meticulously generated by human annotators, resulting in high ",
        "x1": 2.5864086151123047,
        "x2": 1.6315735578536987,
        "y1": 3.862306594848633,
        "y2": 5.069143772125244
      },
      {
        "r": 0,
        "text": "Furthermore, we compare our model's age recognition performance with human-level accuracy and demonstrate that it significantly outperforms humans across a majority of age ranges.",
        "trunc_text": "Furthermore, we compare our model's age recognition performance with human-level accuracy and demonstrate that it signif",
        "x1": 2.765547513961792,
        "x2": 3.9242007732391357,
        "y1": 7.402948379516602,
        "y2": 2.1350440979003906
      },
      {
        "r": 0,
        "text": "Finally, we grant public access to our models, along with the code for validation and inference.",
        "trunc_text": "Finally, we grant public access to our models, along with the code for validation and inference.",
        "x1": 7.759318828582764,
        "x2": 8.216131210327148,
        "y1": 6.9591064453125,
        "y2": 2.9158859252929688
      },
      {
        "r": 0,
        "text": "To bridge this gap, we introduce a dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digital 3D models of space layouts of apartment buildings",
        "trunc_text": "To bridge this gap, we introduce a dataset, SAGC-A68, which comprises access graphs automatically generated from 68 digi",
        "x1": 2.810443162918091,
        "x2": 3.6068758964538574,
        "y1": 8.088621139526367,
        "y2": 2.773437738418579
      },
      {
        "r": 0,
        "text": "This graph-based dataset is well-suited for developing GDL models for space function and space element classification",
        "trunc_text": "This graph-based dataset is well-suited for developing GDL models for space function and space element classification",
        "x1": 3.0094923973083496,
        "x2": 3.356222152709961,
        "y1": 5.619699954986572,
        "y2": 3.0611984729766846
      },
      {
        "r": 0,
        "text": "The dataset and code used in the experiment are available online.",
        "trunc_text": "The dataset and code used in the experiment are available online.",
        "x1": 7.194437026977539,
        "x2": 7.768239974975586,
        "y1": 7.690324306488037,
        "y2": 2.368211269378662
      },
      {
        "r": 0,
        "text": "The analysis of building models for usable area, building safety, and energy use requires accurate classification data of spaces and space elements.",
        "trunc_text": "The analysis of building models for usable area, building safety, and energy use requires accurate classification data o",
        "x1": 2.8966643810272217,
        "x2": 3.6986753940582275,
        "y1": 8.019601821899414,
        "y2": 2.7377407550811768
      },
      {
        "r": 0,
        "text": "To reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable.",
        "trunc_text": "To reduce input model preparation effort and errors, automated classification of spaces and space elements is desirable.",
        "x1": 3.041489839553833,
        "x2": 3.542616605758667,
        "y1": 5.682618141174316,
        "y2": 2.9341630935668945
      },
      {
        "r": 0,
        "text": "A barrier hindering the utilization of Graph Deep Learning (GDL) methods to space function and space element classification is a lack of suitable datasets. .",
        "trunc_text": "A barrier hindering the utilization of Graph Deep Learning (GDL) methods to space function and space element classificat",
        "x1": 2.957947015762329,
        "x2": 3.199383020401001,
        "y1": 5.494500160217285,
        "y2": 3.244664192199707
      },
      {
        "r": 0,
        "text": "This graph-based dataset is well-suited for developing GDL models for space function and space element classification.",
        "trunc_text": "This graph-based dataset is well-suited for developing GDL models for space function and space element classification.",
        "x1": 2.960510492324829,
        "x2": 3.452850103378296,
        "y1": 5.633845806121826,
        "y2": 3.061683177947998
      },
      {
        "r": 0,
        "text": "To demonstrate the potential of the dataset, we employ.",
        "trunc_text": "To demonstrate the potential of the dataset, we employ.",
        "x1": 5.077830791473389,
        "x2": 5.876090049743652,
        "y1": 7.404653072357178,
        "y2": 2.434581995010376
      },
      {
        "r": 0,
        "text": "https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68.",
        "trunc_text": "https://doi.org/10.5281/zenodo.7805872, https://github.com/A2Amir/SAGC-A68.",
        "x1": 7.711297988891602,
        "x2": 8.194258689880371,
        "y1": 7.782539367675781,
        "y2": 2.095126152038574
      },
      {
        "r": 0,
        "text": "We position the contribution of this work in two folds: (i)-we collect and curate nearly 2k high-quality tabular datasets, each of which is guaranteed to possess clear semantics, clean labels, and other necessary meta information.",
        "trunc_text": "We position the contribution of this work in two folds: (i)-we collect and curate nearly 2k high-quality tabular dataset",
        "x1": 5.479537010192871,
        "x2": 6.032663345336914,
        "y1": 6.669807434082031,
        "y2": 2.8109395503997803
      },
      {
        "r": 0,
        "text": "Tabular data -- also known as structured data -- is one of the most common data forms in existence, thanks to the stable development and scaled deployment of database systems in the last few decades.",
        "trunc_text": "Tabular data -- also known as structured data -- is one of the most common data forms in existence, thanks to the stable",
        "x1": 5.973710060119629,
        "x2": 6.501217365264893,
        "y1": 6.5584330558776855,
        "y2": 3.3068602085113525
      },
      {
        "r": 0,
        "text": "At present however, despite the blast brought by large pre-trained models in other domains such as ChatGPT or SAM, how can we extract common knowledge across tables at a scale that may eventually lead to generalizable representation for tabular data remains a full blank.",
        "trunc_text": "At present however, despite the blast brought by large pre-trained models in other domains such as ChatGPT or SAM, how c",
        "x1": 5.808750152587891,
        "x2": 6.397407054901123,
        "y1": 6.373314380645752,
        "y2": 3.416879177093506
      },
      {
        "r": 0,
        "text": "Indeed, there have been a few works around this topic.",
        "trunc_text": "Indeed, there have been a few works around this topic.",
        "x1": 5.124902725219727,
        "x2": 6.098106861114502,
        "y1": 8.065896987915039,
        "y2": 4.270260810852051
      },
      {
        "r": 0,
        "text": "Most (if not all) of them are limited in the scope of a single table or fixed form of a schema.",
        "trunc_text": "Most (if not all) of them are limited in the scope of a single table or fixed form of a schema.",
        "x1": 5.886633396148682,
        "x2": 6.439324855804443,
        "y1": 6.532609462738037,
        "y2": 3.297384738922119
      },
      {
        "r": 0,
        "text": "In this work, we first identify the crucial research challenges behind tabular data pre-training, particularly towards the cross-table scenario.  ",
        "trunc_text": "In this work, we first identify the crucial research challenges behind tabular data pre-training, particularly towards t",
        "x1": 5.787910461425781,
        "x2": 6.362657070159912,
        "y1": 6.302872657775879,
        "y2": 3.440767288208008
      },
      {
        "r": 0,
        "text": "(ii)-we propose a novel framework that allows cross-table pre-training dubbed as CT-BERT.",
        "trunc_text": "(ii)-we propose a novel framework that allows cross-table pre-training dubbed as CT-BERT.",
        "x1": 5.239211082458496,
        "x2": 4.274982929229736,
        "y1": 5.257806301116943,
        "y2": 4.731866836547852
      },
      {
        "r": 0,
        "text": "Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is fully compatible with both supervised and self-supervised schemes, where the specific instantiation of CT-BERT is very much dependent on the downstream tasks.",
        "trunc_text": "Noticeably, in light of pioneering the scaled cross-table training, CT-BERT is fully compatible with both supervised and",
        "x1": 5.235476016998291,
        "x2": 4.271312236785889,
        "y1": 5.285750865936279,
        "y2": 4.7044997215271
      },
      {
        "r": 0,
        "text": "We further propose and implement a contrastive-learning-based and masked table modeling (MTM) objective into CT-BERT, that is inspired from computer vision and natural language processing communities but sophistically tailored to tables.",
        "trunc_text": "We further propose and implement a contrastive-learning-based and masked table modeling (MTM) objective into CT-BERT, th",
        "x1": 5.263607978820801,
        "x2": 4.288093566894531,
        "y1": 5.2127251625061035,
        "y2": 4.763661861419678
      },
      {
        "r": 0,
        "text": "The extensive empirical results on 15 datasets demonstrate CT-BERT's state-of-the-art performance, where both its supervised and self-supervised setups significantly outperform the prior approaches.",
        "trunc_text": "The extensive empirical results on 15 datasets demonstrate CT-BERT's state-of-the-art performance, where both its superv",
        "x1": 5.211376190185547,
        "x2": 4.290110111236572,
        "y1": 5.2453293800354,
        "y2": 4.710086822509766
      },
      {
        "r": 0,
        "text": "The dataset is publicly available at: https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.",
        "trunc_text": "The dataset is publicly available at: https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.",
        "x1": 7.206014156341553,
        "x2": 7.4977827072143555,
        "y1": 7.979833602905273,
        "y2": 1.800856113433838
      },
      {
        "r": 0,
        "text": "Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years.",
        "trunc_text": "Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the",
        "x1": 3.6896631717681885,
        "x2": 2.507697105407715,
        "y1": 3.1654250621795654,
        "y2": 6.065776348114014
      },
      {
        "r": 0,
        "text": "To promote the historical RE research, we present HistRED constructed from Yeonhaengnok.",
        "trunc_text": "To promote the historical RE research, we present HistRED constructed from Yeonhaengnok.",
        "x1": 5.414132595062256,
        "x2": 6.166650772094727,
        "y1": 7.262281894683838,
        "y2": 2.5330045223236084
      },
      {
        "r": 0,
        "text": "Yeonhaengnok is a collection of records originally written in Hanja, the classical Chinese writing, which has later been translated into Korean.",
        "trunc_text": "Yeonhaengnok is a collection of records originally written in Hanja, the classical Chinese writing, which has later been",
        "x1": 4.142621994018555,
        "x2": 2.0416574478149414,
        "y1": 2.247854709625244,
        "y2": 6.855374813079834
      },
      {
        "r": 0,
        "text": "HistRED provides bilingual annotations such that RE can be performed on Korean and Hanja texts.",
        "trunc_text": "HistRED provides bilingual annotations such that RE can be performed on Korean and Hanja texts.",
        "x1": 4.207819938659668,
        "x2": 2.369358777999878,
        "y1": 2.67109751701355,
        "y2": 6.527689456939697
      },
      {
        "r": 0,
        "text": "In addition, HistRED supports various self-contained subtexts with different lengths, from a sentence level to a document level, supporting diverse context settings for researchers to evaluate the robustness of their RE models.",
        "trunc_text": "In addition, HistRED supports various self-contained subtexts with different lengths, from a sentence level to a documen",
        "x1": 4.005673885345459,
        "x2": 2.558095693588257,
        "y1": 3.0768656730651855,
        "y2": 6.201592922210693
      },
      {
        "r": 0,
        "text": "To demonstrate the usefulness of our dataset, we propose a bilingual RE model that leverages both Korean and Hanja contexts to predict relations between entities.",
        "trunc_text": "To demonstrate the usefulness of our dataset, we propose a bilingual RE model that leverages both Korean and Hanja conte",
        "x1": 4.133599281311035,
        "x2": 2.373619556427002,
        "y1": 2.8007264137268066,
        "y2": 6.351179122924805
      },
      {
        "r": 0,
        "text": "Our model outperforms monolingual baselines on HistRED, showing that employing multiple language contexts supplements the RE predictions.",
        "trunc_text": "Our model outperforms monolingual baselines on HistRED, showing that employing multiple language contexts supplements th",
        "x1": 4.098334789276123,
        "x2": 2.5077943801879883,
        "y1": 2.8954458236694336,
        "y2": 6.249173641204834
      },
      {
        "r": 0,
        "text": "This research paper introduces a new curated dataset and a deep learning-based approach to solve these problems using convolutional neural networks (CNNs) and comprehensive data processing techniques.",
        "trunc_text": "This research paper introduces a new curated dataset and a deep learning-based approach to solve these problems using co",
        "x1": 2.234799385070801,
        "x2": 3.0388612747192383,
        "y1": 6.352397441864014,
        "y2": 2.5886712074279785
      },
      {
        "r": 0,
        "text": "Our dataset includes curated images and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites.",
        "trunc_text": "Our dataset includes curated images and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites.",
        "x1": 1.8385756015777588,
        "x2": 4.450088024139404,
        "y1": 8.318718910217285,
        "y2": 1.1387795209884644
      },
      {
        "r": 0,
        "text": "We design the dataset considering different spatial and temporal resolution requirements",
        "trunc_text": "We design the dataset considering different spatial and temporal resolution requirements",
        "x1": 4.776400089263916,
        "x2": 5.599201202392578,
        "y1": 7.642829895019531,
        "y2": 2.0453407764434814
      },
      {
        "r": 0,
        "text": "Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation",
        "trunc_text": "Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation",
        "x1": 1.2142642736434937,
        "x2": 7.510683059692383,
        "y1": 7.3330793380737305,
        "y2": 1.6652460098266602
      },
      {
        "r": 0,
        "text": "Deforestation estimation and fire detection in the Amazon forest poses a significant challenge due to the vast size of the area and the limited accessibility.",
        "trunc_text": "Deforestation estimation and fire detection in the Amazon forest poses a significant challenge due to the vast size of t",
        "x1": 1.203692078590393,
        "x2": 4.764154434204102,
        "y1": 7.405953884124756,
        "y2": 1.326626181602478
      },
      {
        "r": 0,
        "text": "However, these are crucial problems that lead to severe environmental consequences, including climate change, global warming, and biodiversity loss.",
        "trunc_text": "However, these are crucial problems that lead to severe environmental consequences, including climate change, global war",
        "x1": 1.2877562046051025,
        "x2": 5.185037612915039,
        "y1": 7.502649784088135,
        "y2": 1.5041788816452026
      },
      {
        "r": 0,
        "text": "To effectively address this problem, multimodal satellite imagery and remote sensing offer a promising solution for estimating deforestation and detecting wildfire in the Amazonia region.  ",
        "trunc_text": "To effectively address this problem, multimodal satellite imagery and remote sensing offer a promising solution for esti",
        "x1": 1.2241474390029907,
        "x2": 4.64336633682251,
        "y1": 7.477456569671631,
        "y2": 1.2361481189727783
      },
      {
        "r": 0,
        "text": "We design the dataset considering different spatial and temporal resolution requiremeimages from the region.",
        "trunc_text": "We design the dataset considering different spatial and temporal resolution requiremeimages from the region.",
        "x1": 1.7054071426391602,
        "x2": 3.96795916557312,
        "y1": 8.041056632995605,
        "y2": 1.400327444076538
      },
      {
        "r": 0,
        "text": "Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-defo",
        "trunc_text": "Our code, models and dataset are open source: https://github.com/h2oai/cvpr-multiearth-defo",
        "x1": 7.325484275817871,
        "x2": 7.658346176147461,
        "y1": 7.530831813812256,
        "y2": 1.7859774827957153
      },
      {
        "r": 0,
        "text": "We address this limitation by constructing MultiVENT, a dataset of multilingual, event-centric videos grounded in text documents across five target languages",
        "trunc_text": "We address this limitation by constructing MultiVENT, a dataset of multilingual, event-centric videos grounded in text d",
        "x1": 3.786665678024292,
        "x2": 2.403034210205078,
        "y1": 3.0002598762512207,
        "y2": 5.359703063964844
      },
      {
        "r": 0,
        "text": "Everyday news coverage has shifted from traditional broadcasts towards a wide range of presentation formats such as first-hand, unedited video footage.",
        "trunc_text": "Everyday news coverage has shifted from traditional broadcasts towards a wide range of presentation formats such as firs",
        "x1": 3.7349863052368164,
        "x2": 2.429687738418579,
        "y1": 2.8973493576049805,
        "y2": 5.442149639129639
      },
      {
        "r": 0,
        "text": "Datasets that reflect the diverse array of multimodal, multilingual news sources available online could be used to teach models to benefit from this shift, but existing news video datasets focus on traditional news broadcasts produced for English-speaking audiences. .",
        "trunc_text": "Datasets that reflect the diverse array of multimodal, multilingual news sources available online could be used to teach",
        "x1": 3.7757458686828613,
        "x2": 2.4894137382507324,
        "y1": 2.949078321456909,
        "y2": 5.42963981628418
      },
      {
        "r": 0,
        "text": "MultiVENT includes both news broadcast videos and non-professional event footage, which we use to analyze the state of online news videos and how they can be leveraged to build robust, factually accurate models.",
        "trunc_text": "MultiVENT includes both news broadcast videos and non-professional event footage, which we use to analyze the state of o",
        "x1": 3.741652250289917,
        "x2": 2.3920435905456543,
        "y1": 2.9528958797454834,
        "y2": 5.367992877960205
      },
      {
        "r": 0,
        "text": "Finally, we provide a model for complex, multilingual video retrieval to serve as a baseline for information retrieval using MultiVENT.",
        "trunc_text": "Finally, we provide a model for complex, multilingual video retrieval to serve as a baseline for information retrieval u",
        "x1": 3.8051114082336426,
        "x2": 2.4127283096313477,
        "y1": 2.9918081760406494,
        "y2": 5.292718887329102
      },
      {
        "r": 0,
        "text": "To address this, we establish a large-scale dataset, namely the Tuberculosis X-ray (TBX11K) dataset, which contains 11,200 chest X-ray (CXR) images with corresponding bounding box annotations for TB areas",
        "trunc_text": "To address this, we establish a large-scale dataset, namely the Tuberculosis X-ray (TBX11K) dataset, which contains 11,2",
        "x1": 1.412716269493103,
        "x2": 2.9242260456085205,
        "y1": 6.993727207183838,
        "y2": 1.8784266710281372
      },
      {
        "r": 0,
        "text": "This dataset enables the training of sophisticated detectors for high-quality CTD",
        "trunc_text": "This dataset enables the training of sophisticated detectors for high-quality CTD",
        "x1": 1.0654772520065308,
        "x2": 2.588158369064331,
        "y1": 6.926097869873047,
        "y2": 1.9245425462722778
      },
      {
        "r": 0,
        "text": "The data, code, and models will be released.",
        "trunc_text": "The data, code, and models will be released.",
        "x1": 7.123329162597656,
        "x2": 7.707740783691406,
        "y1": 7.301939487457275,
        "y2": 2.5685949325561523
      },
      {
        "r": 0,
        "text": "Tuberculosis (TB) is a major global health threat, causing millions of deaths annually.",
        "trunc_text": "Tuberculosis (TB) is a major global health threat, causing millions of deaths annually.",
        "x1": 1.4175807237625122,
        "x2": 5.339626312255859,
        "y1": 7.071163654327393,
        "y2": 1.9199906587600708
      },
      {
        "r": 0,
        "text": "Although early diagnosis and treatment can greatly improve the chances of survival, it remains a major challenge, especially in developing countries.",
        "trunc_text": "Although early diagnosis and treatment can greatly improve the chances of survival, it remains a major challenge, especi",
        "x1": 3.915578842163086,
        "x2": 5.540384769439697,
        "y1": 7.90585994720459,
        "y2": 2.0392379760742188
      },
      {
        "r": 0,
        "text": "Recently, computer-aided tuberculosis diagnosis (CTD) using deep learning has shown promise, but progress is hindered by limited training data. .",
        "trunc_text": "Recently, computer-aided tuberculosis diagnosis (CTD) using deep learning has shown promise, but progress is hindered by",
        "x1": 1.3965017795562744,
        "x2": 2.8935792446136475,
        "y1": 6.899026870727539,
        "y2": 2.044262647628784
      },
      {
        "r": 0,
        "text": "This dataset enables the training of sophisticated detectors for high-quality CTD.",
        "trunc_text": "This dataset enables the training of sophisticated detectors for high-quality CTD.",
        "x1": 1.15656578540802,
        "x2": 2.603240728378296,
        "y1": 6.87890625,
        "y2": 1.824267029762268
      },
      {
        "r": 0,
        "text": "Furthermore, we propose a strong baseline, SymFormer, for simultaneous CXR image classification and TB infection area dete the bilateral symmetry property of CXR images for learning discriminative features.",
        "trunc_text": "Furthermore, we propose a strong baseline, SymFormer, for simultaneous CXR image classification and TB infection area de",
        "x1": 1.3353620767593384,
        "x2": 2.854840040206909,
        "y1": 7.05956506729126,
        "y2": 1.690889835357666
      },
      {
        "r": 0,
        "text": "Since CXR images may not strictly adhere to the bilateral symmetry property, we also propose Symmetric Positional Encoding (SPE) to facilitate SymAttention through feature recalibration.",
        "trunc_text": "Since CXR images may not strictly adhere to the bilateral symmetry property, we also propose Symmetric Positional Encodi",
        "x1": 1.3976831436157227,
        "x2": 3.21517014503479,
        "y1": 8.426580429077148,
        "y2": 0.6245120763778687
      },
      {
        "r": 0,
        "text": "To promote future research on CTD, we build a benchmark by introducing evaluation metrics, evaluating baseline models reformed from existing detectors, and running an online challenge.",
        "trunc_text": "To promote future research on CTD, we build a benchmark by introducing evaluation metrics, evaluating baseline models re",
        "x1": 1.0519886016845703,
        "x2": 2.4590299129486084,
        "y1": 6.929811954498291,
        "y2": 1.8714878559112549
      },
      {
        "r": 0,
        "text": "Experiments show that SymFormer achieves state-of-the-art performance on the TBX11K dataset.",
        "trunc_text": "Experiments show that SymFormer achieves state-of-the-art performance on the TBX11K dataset.",
        "x1": 4.588267803192139,
        "x2": 4.652578353881836,
        "y1": 6.473562717437744,
        "y2": 3.2109267711639404
      },
      {
        "r": 0,
        "text": "This paper presents an end-to-end methodology for collecting datasets to recognize handwritten English alphabets by utilizing Inertial Measurement Units (IMUs) and leveraging the diversity present in the Indian writing style",
        "trunc_text": "This paper presents an end-to-end methodology for collecting datasets to recognize handwritten English alphabets by util",
        "x1": 4.072930812835693,
        "x2": 1.707540512084961,
        "y1": 1.8521180152893066,
        "y2": 7.026750087738037
      },
      {
        "r": 0,
        "text": "The IMUs are utilized to capture the dynamic movement patterns associated with handwriting, enabling more accurate recognition of alphabets.",
        "trunc_text": "The IMUs are utilized to capture the dynamic movement patterns associated with handwriting, enabling more accurate recog",
        "x1": 4.025512218475342,
        "x2": 1.6953305006027222,
        "y1": 1.796771764755249,
        "y2": 7.072099685668945
      },
      {
        "r": 0,
        "text": "The Indian context introduces various challenges due to the heterogeneity in writing styles across different regions and languages.",
        "trunc_text": "The Indian context introduces various challenges due to the heterogeneity in writing styles across different regions and",
        "x1": 4.162572383880615,
        "x2": 1.9564790725708008,
        "y1": 2.0937299728393555,
        "y2": 6.93926477432251
      },
      {
        "r": 0,
        "text": "Some preliminary experimental results demonstrate the effectiveness of the dataset in accurately recogpattern recognition and offers valuable insights for developing improved systems for handwriting recognition, particularly in diverse linguistic and cultural contexts.",
        "trunc_text": "Some preliminary experimental results demonstrate the effectiveness of the dataset in accurately recogpattern recognitio",
        "x1": 3.9952902793884277,
        "x2": 1.6275699138641357,
        "y1": 1.7799994945526123,
        "y2": 7.007295608520508
      },
      {
        "r": 0,
        "text": "We fill this gap by semi-automatically creating an NLI dataset for spatial reasoning, called SpaceNLI",
        "trunc_text": "We fill this gap by semi-automatically creating an NLI dataset for spatial reasoning, called SpaceNLI",
        "x1": 3.5088698863983154,
        "x2": 3.1320180892944336,
        "y1": 4.355629920959473,
        "y2": 4.855451583862305
      },
      {
        "r": 0,
        "text": "While many natural language inference (NLI) datasets target certain semantic phenomena, e.g., negation, tense & aspect, monotonicity, and presupposition, to the best of our knowledge, there is no NLI dataset that involves diverse types of spatial expressions and reasoning. .",
        "trunc_text": "While many natural language inference (NLI) datasets target certain semantic phenomena, e.g., negation, tense & aspect, ",
        "x1": 3.5218281745910645,
        "x2": 2.968226432800293,
        "y1": 4.058465480804443,
        "y2": 5.257444858551025
      },
      {
        "r": 0,
        "text": "We test several SOTA NLI systems on SpaceNLI to gauge the complexity of the dataset and the system's capacity for spatial reasoning.",
        "trunc_text": "We test several SOTA NLI systems on SpaceNLI to gauge the complexity of the dataset and the system's capacity for spatia",
        "x1": 3.553602933883667,
        "x2": 3.0396766662597656,
        "y1": 4.293827056884766,
        "y2": 4.958014488220215
      },
      {
        "r": 0,
        "text": "Moreover, we introduce a Pattern Accuracy and argue that it is a more reliable and stricter measure than the accuracy for evaluating a system's performance on pattern-based generated data samples.",
        "trunc_text": "Moreover, we introduce a Pattern Accuracy and argue that it is a more reliable and stricter measure than the accuracy fo",
        "x1": 4.263798236846924,
        "x2": 4.588519096374512,
        "y1": 6.560878753662109,
        "y2": 3.369399070739746
      },
      {
        "r": 0,
        "text": "Based on the evaluation results we find that the systems obtain moderate results on the spatial NLI problems but lack consistency per inference pattern.",
        "trunc_text": "Based on the evaluation results we find that the systems obtain moderate results on the spatial NLI problems but lack co",
        "x1": 3.5026941299438477,
        "x2": 3.031736135482788,
        "y1": 4.28097677230835,
        "y2": 4.977473258972168
      },
      {
        "r": 0,
        "text": "The results also reveal that non-projective spatial inferences (especially due to the \"between\" preposition) are the most challenging ones.",
        "trunc_text": "The results also reveal that non-projective spatial inferences (especially due to the \"between\" preposition) are the mos",
        "x1": 3.4873409271240234,
        "x2": 3.088712215423584,
        "y1": 4.403146266937256,
        "y2": 4.806612014770508
      },
      {
        "r": 0,
        "text": "This paper presents the FormAI dataset, a large collection of 112,000 AI-generated compilable and independent C programs with vulnerability classification",
        "trunc_text": "This paper presents the FormAI dataset, a large collection of 112,000 AI-generated compilable and independent C programs",
        "x1": 7.730874538421631,
        "x2": 6.728065490722656,
        "y1": 5.118579864501953,
        "y2": 6.439441204071045
      },
      {
        "r": 0,
        "text": "We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabilities detected in each individual program",
        "trunc_text": "We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabil",
        "x1": 7.631961822509766,
        "x2": 6.740686893463135,
        "y1": 5.112801551818848,
        "y2": 6.4406585693359375
      },
      {
        "r": 0,
        "text": "We introduce a dynamic zero-shot prompting technique, constructed to spawn a diverse set of programs utilizing Large Language Models (LLMs).",
        "trunc_text": "We introduce a dynamic zero-shot prompting technique, constructed to spawn a diverse set of programs utilizing Large Lan",
        "x1": 5.3231401443481445,
        "x2": 3.9777584075927734,
        "y1": 3.840486526489258,
        "y2": 6.326622009277344
      },
      {
        "r": 0,
        "text": "Some programs handle complicated tasks such as networkion.",
        "trunc_text": "Some programs handle complicated tasks such as networkion.",
        "x1": 6.2200422286987305,
        "x2": 4.862695693969727,
        "y1": 4.3416748046875,
        "y2": 6.717992782592773
      },
      {
        "r": 0,
        "text": "This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which performs model checking, abstract interpretation, constraint programming, and satisfiability modulo theories, to reason over safety/security properties in programs.",
        "trunc_text": "This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBM",
        "x1": 7.1026716232299805,
        "x2": 5.997638702392578,
        "y1": 5.061254024505615,
        "y2": 6.045886993408203
      },
      {
        "r": 0,
        "text": "This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports.",
        "trunc_text": "This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating",
        "x1": 7.465071201324463,
        "x2": 6.583027362823486,
        "y1": 5.318514347076416,
        "y2": 6.077827453613281
      },
      {
        "r": 0,
        "text": "Furthermore, we have associated the identified vulnerabilities with relevant Common Weakness Enumeration (CWE) numbers.",
        "trunc_text": "Furthermore, we have associated the identified vulnerabilities with relevant Common Weakness Enumeration (CWE) numbers.",
        "x1": 7.691151142120361,
        "x2": 6.716738224029541,
        "y1": 5.231545448303223,
        "y2": 6.3128814697265625
      },
      {
        "r": 0,
        "text": "We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabilities detected in each individual program including location and function name, which makes the dataset ideal to train LLMs and machi",
        "trunc_text": "We make the source code available for the 112,000 programs, accompanied by a comprehensive list detailing the vulnerabil",
        "x1": 7.672560691833496,
        "x2": 6.706672191619873,
        "y1": 5.094233512878418,
        "y2": 6.399427890777588
      },
      {
        "r": 0,
        "text": "In this work, we construct two datasets to address this issue",
        "trunc_text": "In this work, we construct two datasets to address this issue",
        "x1": 5.380273342132568,
        "x2": 5.956177711486816,
        "y1": 7.462530136108398,
        "y2": 2.4149303436279297
      },
      {
        "r": 0,
        "text": "We introduce a new conversation head generation benchmark for synthesizing behaviors of a single interlocutor in a face-to-face conversation.",
        "trunc_text": "We introduce a new conversation head generation benchmark for synthesizing behaviors of a single interlocutor in a face-",
        "x1": 5.774288654327393,
        "x2": 3.801316499710083,
        "y1": 3.2853217124938965,
        "y2": 7.218942165374756
      },
      {
        "r": 0,
        "text": "The capability to automatically synthesize interlocutors which can participate in long and multi-turn conversations is vital and offer benefits for various applications, including digital humans, virtual agents, and social robots.",
        "trunc_text": "The capability to automatically synthesize interlocutors which can participate in long and multi-turn conversations is v",
        "x1": 5.883886814117432,
        "x2": 3.915440797805786,
        "y1": 3.4200947284698486,
        "y2": 7.316008567810059
      },
      {
        "r": 0,
        "text": "While existing research primarily focuses on talking head generation (one-way interaction), hindering the ability to create a digital human for conversation (two-way) interaction due to the absence of listening and interaction parts.",
        "trunc_text": "While existing research primarily focuses on talking head generation (one-way interaction), hindering the ability to cre",
        "x1": 5.6733903884887695,
        "x2": 3.800379753112793,
        "y1": 3.2526299953460693,
        "y2": 7.234498977661133
      },
      {
        "r": 0,
        "text": ", ``ViCo'' for independent talking and listening head generation tasks at the sentence level, and ``ViCo-X'', for synthesizing interlocutors in multi-turn conversational scenarios.",
        "trunc_text": ", ``ViCo'' for independent talking and listening head generation tasks at the sentence level, and ``ViCo-X'', for synthe",
        "x1": 5.732089519500732,
        "x2": 3.814722776412964,
        "y1": 3.281644582748413,
        "y2": 7.238851070404053
      },
      {
        "r": 0,
        "text": "Based on ViCo and ViCo-X, we define three novel tasks targeting the interaction modeling during the face-to-face conversation: 1) responsive listening head generation making listeners respond actively to the speaker with non-verbal signals, 2) expressive talking head generation guiding speakers to be aware of listeners' behaviors, and 3) conversational head generation to integrate the talking/listening ability in one interlocutor.",
        "trunc_text": "Based on ViCo and ViCo-X, we define three novel tasks targeting the interaction modeling during the face-to-face convers",
        "x1": 5.755110263824463,
        "x2": 3.7837073802948,
        "y1": 3.3148386478424072,
        "y2": 7.269449234008789
      },
      {
        "r": 0,
        "text": "Along with the datasets, we also propose corresponding baselierate responsive and vivid agents that can collaborate with real person to fulfil the whole conversation.",
        "trunc_text": "Along with the datasets, we also propose corresponding baselierate responsive and vivid agents that can collaborate with",
        "x1": 5.808959007263184,
        "x2": 3.876100778579712,
        "y1": 3.3675084114074707,
        "y2": 7.288443088531494
      },
      {
        "r": 0,
        "text": "Project page: https://vico.solutions/.",
        "trunc_text": "Project page: https://vico.solutions/.",
        "x1": 7.922462463378906,
        "x2": 8.483929634094238,
        "y1": 8.001629829406738,
        "y2": 1.8823243379592896
      },
      {
        "r": 0,
        "text": "The digitization of documents allows for wider accessibility and reproducibility.",
        "trunc_text": "The digitization of documents allows for wider accessibility and reproducibility.",
        "x1": 6.5073771476745605,
        "x2": 7.02070426940918,
        "y1": 6.641007423400879,
        "y2": 3.4868216514587402
      },
      {
        "r": 0,
        "text": "While automatic digitization of document layout and text content has been a long-standing focus of research, this problem in regard to graphical elements, such as statistical plots, has been under-explored.",
        "trunc_text": "While automatic digitization of document layout and text content has been a long-standing focus of research, this proble",
        "x1": 6.617859363555908,
        "x2": 7.212436676025391,
        "y1": 6.81374454498291,
        "y2": 2.8926596641540527
      },
      {
        "r": 0,
        "text": "In this paper, we introduce the task of fine-grained visual understanding of mathematical graphics and present the Line Graphics (LG) dataset, which includes pixel-wise annotations of 5 coarse and 10 fine-grained categories.  ",
        "trunc_text": "In this paper, we introduce the task of fine-grained visual understanding of mathematical graphics and present the Line ",
        "x1": 2.7162082195281982,
        "x2": 4.504722595214844,
        "y1": 7.8805742263793945,
        "y2": 1.5919413566589355
      },
      {
        "r": 0,
        "text": "To benchmark our LG dataset, we explore 7 state-of-the-art models.",
        "trunc_text": "To benchmark our LG dataset, we explore 7 state-of-the-art models.",
        "x1": 4.552409648895264,
        "x2": 4.343411445617676,
        "y1": 6.501176834106445,
        "y2": 3.3637609481811523
      },
      {
        "r": 0,
        "text": "To foster further research on the digitization of statistical graphs, we will make the dataset, code, and model",
        "trunc_text": "To foster further research on the digitization of statistical graphs, we will make the dataset, code, and model",
        "x1": 6.7392425537109375,
        "x2": 7.297250270843506,
        "y1": 7.0678911209106445,
        "y2": 2.692065715789795
      },
      {
        "r": 0,
        "text": "While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets.",
        "trunc_text": "While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of",
        "x1": 3.5348081588745117,
        "x2": 4.651506423950195,
        "y1": 7.482725620269775,
        "y2": 2.2363686561584473
      },
      {
        "r": 0,
        "text": "The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits.",
        "trunc_text": "The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained model",
        "x1": 3.375868320465088,
        "x2": 4.418979644775391,
        "y1": 7.3717193603515625,
        "y2": 2.275268077850342
      },
      {
        "r": 0,
        "text": "We help address these challenges through three contributions.  ",
        "trunc_text": "We help address these challenges through three contributions.  ",
        "x1": 4.870419979095459,
        "x2": 6.243988513946533,
        "y1": 8.357575416564941,
        "y2": 4.140227794647217
      },
      {
        "r": 0,
        "text": "Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients.",
        "trunc_text": "Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients.",
        "x1": 3.4484851360321045,
        "x2": 4.297072887420654,
        "y1": 7.317479610443115,
        "y2": 2.2742695808410645
      },
      {
        "r": 0,
        "text": "Second, we publish the weights of a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients.",
        "trunc_text": "Second, we publish the weights of a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.",
        "x1": 3.3446288108825684,
        "x2": 4.308964729309082,
        "y1": 7.367500305175781,
        "y2": 2.2491116523742676
      },
      {
        "r": 0,
        "text": "We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR.",
        "trunc_text": "We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for cl",
        "x1": 3.421351909637451,
        "x2": 4.282614231109619,
        "y1": 7.34058952331543,
        "y2": 2.2788357734680176
      },
      {
        "r": 0,
        "text": "We provide an end-to-end pipeline for the community to validate and build upon its performance.",
        "trunc_text": "We provide an end-to-end pipeline for the community to validate and build upon its performance.",
        "x1": 5.337462425231934,
        "x2": 5.780378818511963,
        "y1": 6.440948963165283,
        "y2": 3.556694269180298
      },
      {
        "r": 0,
        "text": "Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaption.",
        "trunc_text": "Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sam",
        "x1": 3.2813422679901123,
        "x2": 4.313907146453857,
        "y1": 7.355484485626221,
        "y2": 2.260117769241333
      },
      {
        "r": 0,
        "text": "The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available at our Github repo here: https://github.com/som",
        "trunc_text": "The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available a",
        "x1": 7.144733428955078,
        "x2": 7.631308078765869,
        "y1": 7.926665306091309,
        "y2": 1.879554271697998
      },
      {
        "r": 0,
        "text": "Generative latent diffusion models have been established as state-of-the-art in data generation.",
        "trunc_text": "Generative latent diffusion models have been established as state-of-the-art in data generation.",
        "x1": 2.3866233825683594,
        "x2": 3.2619903087615967,
        "y1": 6.506113529205322,
        "y2": 2.269502878189087
      },
      {
        "r": 0,
        "text": "One promising application is generation of realistic synthetic medical imaging data for open data sharing without compromising patient privacy.",
        "trunc_text": "One promising application is generation of realistic synthetic medical imaging data for open data sharing without compro",
        "x1": 7.278994083404541,
        "x2": 7.3078718185424805,
        "y1": 7.114285469055176,
        "y2": 2.306190252304077
      },
      {
        "r": 0,
        "text": "Despite the promise, the capacity of such models to memorize sensitive patient training data and synthesize samples showing high resemblance to training data samples is relatively unexplored.",
        "trunc_text": "Despite the promise, the capacity of such models to memorize sensitive patient training data and synthesize samples show",
        "x1": 2.678739547729492,
        "x2": 3.600245714187622,
        "y1": 6.361002445220947,
        "y2": 2.5625016689300537
      },
      {
        "r": 0,
        "text": "Here, we assess the memorization capacity of 3D latent diffusion models on photon-counting coronary computed tomography angiography and knee magnetic resonance imaging datasets.",
        "trunc_text": "Here, we assess the memorization capacity of 3D latent diffusion models on photon-counting coronary computed tomography ",
        "x1": 2.2663962841033936,
        "x2": 3.250777244567871,
        "y1": 6.499051094055176,
        "y2": 2.2553212642669678
      },
      {
        "r": 0,
        "text": "To detect potential memorization of training samples, we utilize self-supervised models based on contrastive learning.",
        "trunc_text": "To detect potential memorization of training samples, we utilize self-supervised models based on contrastive learning.",
        "x1": 2.6776554584503174,
        "x2": 3.486464262008667,
        "y1": 6.2601823806762695,
        "y2": 2.6990673542022705
      },
      {
        "r": 0,
        "text": "Our results suggest that such latent diffusion models indeed memorize training data, and there is a dire need for devising strategies to mitigate memorization.",
        "trunc_text": "Our results suggest that such latent diffusion models indeed memorize training data, and there is a dire need for devisi",
        "x1": 2.564281702041626,
        "x2": 3.4357903003692627,
        "y1": 6.294907093048096,
        "y2": 2.6052000522613525
      },
      {
        "r": 0,
        "text": "Visual Question Answering (VQA) models aim to answer natural language questions about given images.",
        "trunc_text": "Visual Question Answering (VQA) models aim to answer natural language questions about given images.",
        "x1": 4.220419883728027,
        "x2": 2.787858486175537,
        "y1": 3.737213134765625,
        "y2": 4.531156539916992
      },
      {
        "r": 0,
        "text": "Due to its ability to ask questions that differ from those used when training the model, medical VQA has received substantial attention in recent years.",
        "trunc_text": "Due to its ability to ask questions that differ from those used when training the model, medical VQA has received substa",
        "x1": 2.1695079803466797,
        "x2": 2.770066261291504,
        "y1": 6.329707145690918,
        "y2": 2.8732800483703613
      },
      {
        "r": 0,
        "text": "However, existing medical VQA models typically focus on answering questions that refer to an entire image rather than where the relevant content may be located in the image.",
        "trunc_text": "However, existing medical VQA models typically focus on answering questions that refer to an entire image rather than wh",
        "x1": 2.0812859535217285,
        "x2": 2.7488839626312256,
        "y1": 6.2877197265625,
        "y2": 2.8603246212005615
      },
      {
        "r": 0,
        "text": "Consequently, VQA models are limited in their interpretability power and the possibility to probe the model about specific image regions.",
        "trunc_text": "Consequently, VQA models are limited in their interpretability power and the possibility to probe the model about specif",
        "x1": 2.121786594390869,
        "x2": 2.7560131549835205,
        "y1": 6.277859687805176,
        "y2": 2.851083993911743
      },
      {
        "r": 0,
        "text": "This paper proposes a novel approach for medical VQA that addresses this limitation by developing a model that can answer questions about image regions while considering the context necessary to answer the questions.",
        "trunc_text": "This paper proposes a novel approach for medical VQA that addresses this limitation by developing a model that can answe",
        "x1": 1.9506810903549194,
        "x2": 2.698162317276001,
        "y1": 6.392800331115723,
        "y2": 2.7054154872894287
      },
      {
        "r": 0,
        "text": "Artificial intelligence applications enable farmers to optimize crop growth and production while reducing costs and environmental impact.",
        "trunc_text": "Artificial intelligence applications enable farmers to optimize crop growth and production while reducing costs and envi",
        "x1": 1.2821903228759766,
        "x2": 5.18967342376709,
        "y1": 7.61219596862793,
        "y2": 1.5771863460540771
      },
      {
        "r": 0,
        "text": "Computer vision-based algorithms in particular, are commonly used for fruit segmentation, enabling in-depth analysis of the harvest quality and accurate yield estimation.",
        "trunc_text": "Computer vision-based algorithms in particular, are commonly used for fruit segmentation, enabling in-depth analysis of ",
        "x1": 1.3347841501235962,
        "x2": 4.945609092712402,
        "y1": 7.558743000030518,
        "y2": 1.5508707761764526
      },
      {
        "r": 0,
        "text": "In this paper, we propose TomatoDIFF, a novel diffusion-based model for semantic segmentation of on-plant tomatoes.",
        "trunc_text": "In this paper, we propose TomatoDIFF, a novel diffusion-based model for semantic segmentation of on-plant tomatoes.",
        "x1": 1.250632882118225,
        "x2": 4.903903007507324,
        "y1": 7.510688781738281,
        "y2": 1.5282649993896484
      },
      {
        "r": 0,
        "text": "When evaluated against other competitive methods, our model demonstrates state-of-the-art (SOTA) performance, even in challenging environments with highly occluded fruits.  ",
        "trunc_text": "When evaluated against other competitive methods, our model demonstrates state-of-the-art (SOTA) performance, even in ch",
        "x1": 1.4006574153900146,
        "x2": 4.98581075668335,
        "y1": 7.648910045623779,
        "y2": 1.6381422281265259
      },
      {
        "r": 0,
        "text": "Large language models~(LLMs) obtain instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data.",
        "trunc_text": "Large language models~(LLMs) obtain instruction-following capability through instruction-finetuning (IFT) on supervised ",
        "x1": 5.375064373016357,
        "x2": 4.199896812438965,
        "y1": 4.12339973449707,
        "y2": 6.198453426361084
      },
      {
        "r": 0,
        "text": "However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT.",
        "trunc_text": "However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorre",
        "x1": 5.089545726776123,
        "x2": 5.548809051513672,
        "y1": 5.969465255737305,
        "y2": 3.730926036834717
      },
      {
        "r": 0,
        "text": "In this paper, we propose a simple and effective data selection strategy that automatically identifies and removes low-quality data using a strong LLM (e.g., ChatGPT).",
        "trunc_text": "In this paper, we propose a simple and effective data selection strategy that automatically identifies and removes low-q",
        "x1": 5.265100479125977,
        "x2": 4.728235244750977,
        "y1": 5.90817403793335,
        "y2": 4.14997673034668
      },
      {
        "r": 0,
        "text": "To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data.",
        "trunc_text": "To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data.",
        "x1": 5.145954608917236,
        "x2": 5.451711654663086,
        "y1": 6.36643123626709,
        "y2": 3.515016794204712
      },
      {
        "r": 0,
        "text": "AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and its 13B variant matches $>90\\%$ performance of its teacher LLM (i.e., Text-Davinci-003) on test tasks.",
        "trunc_text": "AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and its 13B variant ",
        "x1": 5.0023193359375,
        "x2": 5.280747413635254,
        "y1": 6.170044422149658,
        "y2": 3.731945037841797
      },
      {
        "r": 0,
        "text": "It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes \\footnote{We apply IFT for the same number of epochs as Alpaca(7B)",
        "trunc_text": "It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 mi",
        "x1": 4.855781078338623,
        "x2": 5.16284704208374,
        "y1": 6.140185832977295,
        "y2": 3.698768377304077
      },
      {
        "r": 0,
        "text": "but on fewer data, using 4$\\times$NVIDIA A100 (80GB) GPUs and following the original Alpaca setting and hyperparameters.}.",
        "trunc_text": "but on fewer data, using 4$\\times$NVIDIA A100 (80GB) GPUs and following the original Alpaca setting and hyperparameters.",
        "x1": 4.950221538543701,
        "x2": 5.2927069664001465,
        "y1": 6.255039215087891,
        "y2": 3.687464475631714
      },
      {
        "r": 0,
        "text": "Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models.",
        "trunc_text": "Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning da",
        "x1": 5.426019668579102,
        "x2": 4.439186096191406,
        "y1": 4.21668815612793,
        "y2": 6.119098663330078
      },
      {
        "r": 0,
        "text": "Our project page is available at: \\url{https://lichang-chen.github.io/AlpaGasus/}.",
        "trunc_text": "Our project page is available at: \\url{https://lichang-chen.github.io/AlpaGasus/}.",
        "x1": 7.944809436798096,
        "x2": 8.57148551940918,
        "y1": 8.020720481872559,
        "y2": 1.8805896043777466
      },
      {
        "r": 0,
        "text": "We also introduce a large-scale dataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with over two million frames, making it the largest natural-scene video depth dataset to our knowledge.",
        "trunc_text": "We also introduce a large-scale dataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with over two mi",
        "x1": 1.5225661993026733,
        "x2": 3.0633580684661865,
        "y1": 8.471489906311035,
        "y2": 0.8360002040863037
      },
      {
        "r": 0,
        "text": "Video depth estimation aims to infer temporally consistent depth.",
        "trunc_text": "Video depth estimation aims to infer temporally consistent depth.",
        "x1": 1.0141900777816772,
        "x2": 2.674539804458618,
        "y1": 8.672402381896973,
        "y2": 0.42280977964401245
      },
      {
        "r": 0,
        "text": "Some methods achieve temporal consistency by finetuning a single-image depth model during test time using geometry and re-projection constraints, which is inefficient and not robust.",
        "trunc_text": "Some methods achieve temporal consistency by finetuning a single-image depth model during test time using geometry and r",
        "x1": 1.0590647459030151,
        "x2": 2.788465976715088,
        "y1": 8.701070785522461,
        "y2": 0.43205299973487854
      },
      {
        "r": 0,
        "text": "An alternative approach is to learn how to enforce temporal consistency from data, but this requires well-designed models and sufficient video depth data.",
        "trunc_text": "An alternative approach is to learn how to enforce temporal consistency from data, but this requires well-designed model",
        "x1": 1.149207353591919,
        "x2": 2.771376848220825,
        "y1": 8.589994430541992,
        "y2": 0.5094084739685059
      },
      {
        "r": 0,
        "text": "To address these challenges, we propose a plug-and-play framework called Neural Video Depth Stabilizer (NVDS) that stabilizes inconsistent depth estimations and can be applied to different single-image depth models without extra effort.  ",
        "trunc_text": "To address these challenges, we propose a plug-and-play framework called Neural Video Depth Stabilizer (NVDS) that stabi",
        "x1": 0.9698337912559509,
        "x2": 2.7217824459075928,
        "y1": 8.642663955688477,
        "y2": 0.4730677902698517
      },
      {
        "r": 0,
        "text": "We evaluate our method on the VDW dataset as well as two public benchmarks and demonstrate significant improvements in consistency, accuracy, and efficiency compared to previous approaches.",
        "trunc_text": "We evaluate our method on the VDW dataset as well as two public benchmarks and demonstrate significant improvements in c",
        "x1": 4.2490925788879395,
        "x2": 4.6053786277771,
        "y1": 6.835235118865967,
        "y2": 3.0259063243865967
      },
      {
        "r": 0,
        "text": "Our work serves as a solid baseline and provides a data foundation for learning-based video depth models.",
        "trunc_text": "Our work serves as a solid baseline and provides a data foundation for learning-based video depth models.",
        "x1": 1.044324278831482,
        "x2": 2.6905553340911865,
        "y1": 8.593106269836426,
        "y2": 0.5480901002883911
      },
      {
        "r": 0,
        "text": "Using COLLIE, we compile the COLLIE-v1 dataset with 2080 instances comprising 13 constraint structures.",
        "trunc_text": "Using COLLIE, we compile the COLLIE-v1 dataset with 2080 instances comprising 13 constraint structures.",
        "x1": 5.079038619995117,
        "x2": 3.505904197692871,
        "y1": 4.414444446563721,
        "y2": 5.736364364624023
      },
      {
        "r": 0,
        "text": "Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models.",
        "trunc_text": "Text generation under constraints have seen increasing interests in natural language processing, especially with the rap",
        "x1": 5.136589050292969,
        "x2": 3.339815378189087,
        "y1": 3.893902540206909,
        "y2": 5.941202640533447
      },
      {
        "r": 0,
        "text": "However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g.,generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4.",
        "trunc_text": "However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g.,generate a sentenc",
        "x1": 5.19085693359375,
        "x2": 3.410108804702759,
        "y1": 3.9940552711486816,
        "y2": 5.9860734939575195
      },
      {
        "r": 0,
        "text": "We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning).",
        "trunc_text": "We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diver",
        "x1": 5.121771812438965,
        "x2": 3.393028497695923,
        "y1": 3.992466926574707,
        "y2": 5.949221134185791
      },
      {
        "r": 0,
        "text": "We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus.  ",
        "trunc_text": "We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus.  ",
        "x1": 3.4940857887268066,
        "x2": 2.5621607303619385,
        "y1": 3.246870279312134,
        "y2": 6.017458915710449
      },
      {
        "r": 0,
        "text": "We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings.",
        "trunc_text": "We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their perfo",
        "x1": 5.208579063415527,
        "x2": 3.956465721130371,
        "y1": 4.046819686889648,
        "y2": 6.135443210601807
      },
      {
        "r": 0,
        "text": "COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.",
        "trunc_text": "COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex c",
        "x1": 5.020595550537109,
        "x2": 3.4870424270629883,
        "y1": 4.524657726287842,
        "y2": 5.69446325302124
      },
      {
        "r": 0,
        "text": "In this paper, we focus on interactions between humans and small indoor robots and introduce a new human-robot interaction (HRI) dataset",
        "trunc_text": "In this paper, we focus on interactions between humans and small indoor robots and introduce a new human-robot interacti",
        "x1": 3.058748722076416,
        "x2": 4.519083499908447,
        "y1": 9.403459548950195,
        "y2": -0.5503602027893066
      },
      {
        "r": 0,
        "text": "The dataset used in this analysis is available at: https://github.com/AlexanderDavid/ZuckerDataset.",
        "trunc_text": "The dataset used in this analysis is available at: https://github.com/AlexanderDavid/ZuckerDataset.",
        "x1": 7.128958225250244,
        "x2": 7.4956440925598145,
        "y1": 8.033175468444824,
        "y2": 1.7377893924713135
      },
      {
        "r": 0,
        "text": "In recent years there has been a large focus on how robots can operate in human populated environments. .",
        "trunc_text": "In recent years there has been a large focus on how robots can operate in human populated environments. .",
        "x1": 3.110797882080078,
        "x2": 4.5481367111206055,
        "y1": 9.333810806274414,
        "y2": -0.47365808486938477
      },
      {
        "r": 0,
        "text": "The analysis of the recorded experiments shows that anticipatory and non-reactive robot controllers impose similar constraints to humans' safety and efficiency.",
        "trunc_text": "The analysis of the recorded experiments shows that anticipatory and non-reactive robot controllers impose similar const",
        "x1": 3.090909242630005,
        "x2": 4.452142715454102,
        "y1": 9.301782608032227,
        "y2": -0.5129478573799133
      },
      {
        "r": 0,
        "text": "Additionally, we found that current state-of-the-art models for human trajectory prediction can adequately extend to indoor HRI settings.",
        "trunc_text": "Additionally, we found that current state-of-the-art models for human trajectory prediction can adequately extend to ind",
        "x1": 3.208812713623047,
        "x2": 4.819315433502197,
        "y1": 9.719339370727539,
        "y2": -0.49738892912864685
      },
      {
        "r": 0,
        "text": "Finally, we show that humans respond differently in shared and homogeneous environments when collisions are imminent, since interacting with small differential drives can only cause a finite level of social discomfort as compared to human-human interactions.",
        "trunc_text": "Finally, we show that humans respond differently in shared and homogeneous environments when collisions are imminent, si",
        "x1": 3.177901029586792,
        "x2": 4.421417236328125,
        "y1": 9.253079414367676,
        "y2": -0.4594179093837738
      },
      {
        "r": 0,
        "text": "To validate our model, we built a new dataset based on the well-known Matterport3D and REVERIE datasets.",
        "trunc_text": "To validate our model, we built a new dataset based on the well-known Matterport3D and REVERIE datasets.",
        "x1": 2.1279587745666504,
        "x2": 5.0075249671936035,
        "y1": 8.73226261138916,
        "y2": 1.8995529413223267
      },
      {
        "r": 0,
        "text": "This dataset consists of instructions with complex referring expressions accompanied by real indoor environmental images that feature various target objects, in addition to pixel-wise segmentation masks.",
        "trunc_text": "This dataset consists of instructions with complex referring expressions accompanied by real indoor environmental images",
        "x1": 1.811152458190918,
        "x2": 3.8778295516967773,
        "y1": 7.738554954528809,
        "y2": 1.5464425086975098
      },
      {
        "r": 0,
        "text": "In this study, we aim to develop a model that comprehends a natural language instruction (e.g., \"Go to the living room and get the nearest pillow to the radio art on the wall\") and generates a segmentation mask for the target everyday object.",
        "trunc_text": "In this study, we aim to develop a model that comprehends a natural language instruction (e.g., \"Go to the living room a",
        "x1": 4.5838494300842285,
        "x2": 3.048084259033203,
        "y1": 3.7536263465881348,
        "y2": 5.384434700012207
      },
      {
        "r": 0,
        "text": "The task is challenging because it requires (1) the understanding of the referring expressions for multiple objects in the instruction, (2) the prediction of the target phrase of the sentence among the multiple phrases, and (3) the generation of pixel-wise segmentation masks rather than bounding boxes.",
        "trunc_text": "The task is challenging because it requires (1) the understanding of the referring expressions for multiple objects in t",
        "x1": 4.574141979217529,
        "x2": 3.1454520225524902,
        "y1": 3.707315683364868,
        "y2": 5.2981085777282715
      },
      {
        "r": 0,
        "text": "Studies have been conducted on languagebased segmentation methods; however, they sometimes mask irrelevant regions for complex sentences.",
        "trunc_text": "Studies have been conducted on languagebased segmentation methods; however, they sometimes mask irrelevant regions for c",
        "x1": 3.977231502532959,
        "x2": 2.749075174331665,
        "y1": 3.0710723400115967,
        "y2": 5.882655143737793
      },
      {
        "r": 0,
        "text": "In this paper, we propose the Multimodal Diffusion Segmentation Model (MDSM), which generates a mask in the first stage and refines it in the second stage.",
        "trunc_text": "In this paper, we propose the Multimodal Diffusion Segmentation Model (MDSM), which generates a mask in the first stage ",
        "x1": 1.9913365840911865,
        "x2": 3.164405345916748,
        "y1": 7.001135349273682,
        "y2": 1.8793367147445679
      },
      {
        "r": 0,
        "text": "We introduce a crossmodal parallel feature extraction mechanism and extend diffusion probabilistic models to handle crossmodal features.  ",
        "trunc_text": "We introduce a crossmodal parallel feature extraction mechanism and extend diffusion probabilistic models to handle cros",
        "x1": 2.389962673187256,
        "x2": 3.310027599334717,
        "y1": 6.822522163391113,
        "y2": 2.173419952392578
      },
      {
        "r": 0,
        "text": "This dataset consists of instructions with complex referring expressions accompanied by real indoor envi",
        "trunc_text": "This dataset consists of instructions with complex referring expressions accompanied by real indoor envi",
        "x1": 4.213223934173584,
        "x2": 3.213728904724121,
        "y1": 4.055107116699219,
        "y2": 4.735095500946045
      },
      {
        "r": 0,
        "text": "LLMs have demonstrated remarkable abilities at interacting with humans through language, especially with the usage of instruction-following data.",
        "trunc_text": "LLMs have demonstrated remarkable abilities at interacting with humans through language, especially with the usage of in",
        "x1": 5.56893253326416,
        "x2": 4.130063533782959,
        "y1": 3.74985671043396,
        "y2": 6.547136306762695
      },
      {
        "r": 0,
        "text": "Recent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further enlarge their abilities by incorporating multi-modal inputs, including image, video, and speech.",
        "trunc_text": "Recent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further enlarge their abilities by incorporating multi",
        "x1": 5.297497749328613,
        "x2": 4.052502632141113,
        "y1": 3.575197696685791,
        "y2": 6.408668041229248
      },
      {
        "r": 0,
        "text": "Despite their effectiveness at generating precise and detailed language understanding of the given modality signal, these LLMs give up the ability to ground specific parts of inputs, thus only constructing a coarse-grained mapping.",
        "trunc_text": "Despite their effectiveness at generating precise and detailed language understanding of the given modality signal, thes",
        "x1": 5.315908908843994,
        "x2": 4.111127853393555,
        "y1": 3.5747475624084473,
        "y2": 6.437588214874268
      },
      {
        "r": 0,
        "text": "However, explicit and informative correspondence between text and other modalities will not only improve the user experience but also help to expand the application scenario of multi-modal LLMs.",
        "trunc_text": "However, explicit and informative correspondence between text and other modalities will not only improve the user experi",
        "x1": 5.286890506744385,
        "x2": 4.082200050354004,
        "y1": 3.5236148834228516,
        "y2": 6.419492244720459
      },
      {
        "r": 0,
        "text": "Therefore, we propose BuboGPT, a multi-modal LLM with visual grounding that can perform cross-modal interaction between vision, audio and language, providing fine-grained understanding of visual objects and other given modalities.",
        "trunc_text": "Therefore, we propose BuboGPT, a multi-modal LLM with visual grounding that can perform cross-modal interaction between ",
        "x1": 4.401109218597412,
        "x2": 2.9131929874420166,
        "y1": 3.4873900413513184,
        "y2": 4.899556636810303
      },
      {
        "r": 0,
        "text": "As a result, BuboGPT is able to point out the specific location of an object in the image, when it is generating response or description for that object.",
        "trunc_text": "As a result, BuboGPT is able to point out the specific location of an object in the image, when it is generating respons",
        "x1": 4.380995750427246,
        "x2": 2.9135522842407227,
        "y1": 3.6901028156280518,
        "y2": 4.612313747406006
      },
      {
        "r": 0,
        "text": "Our contributions are two-fold: 1) An off-the-shelf visual grounding module based on SAM that extracts entities in a sentence and find corresponding masks in the image.",
        "trunc_text": "Our contributions are two-fold: 1) An off-the-shelf visual grounding module based on SAM that extracts entities in a sen",
        "x1": 4.375809669494629,
        "x2": 2.9000561237335205,
        "y1": 3.5680646896362305,
        "y2": 4.894937038421631
      },
      {
        "r": 0,
        "text": "2) A two-stage training scheme and instruction dataset to endow joint text-image-audio understanding.",
        "trunc_text": "2) A two-stage training scheme and instruction dataset to endow joint text-image-audio understanding.",
        "x1": 4.260606288909912,
        "x2": 2.6023097038269043,
        "y1": 3.667025566101074,
        "y2": 5.026232719421387
      },
      {
        "r": 0,
        "text": "Our experiments show that BuboGPT achieves impressive multi-modality understanding and visual grounding abilities during the interaction with human.",
        "trunc_text": "Our experiments show that BuboGPT achieves impressive multi-modality understanding and visual grounding abilities during",
        "x1": 4.569346904754639,
        "x2": 2.9826836585998535,
        "y1": 3.5896921157836914,
        "y2": 4.80358362197876
      },
      {
        "r": 0,
        "text": "It performs consistently well when provided by arbitrary modality combinations (either aligned or unaligned).",
        "trunc_text": "It performs consistently well when provided by arbitrary modality combinations (either aligned or unaligned).",
        "x1": 3.8478097915649414,
        "x2": 3.9715347290039062,
        "y1": 6.301455020904541,
        "y2": 3.9434502124786377
      },
      {
        "r": 0,
        "text": "Our code, model and dataset are available at https://bubo-gpt.github.io .",
        "trunc_text": "Our code, model and dataset are available at https://bubo-gpt.github.io .",
        "x1": 7.2907586097717285,
        "x2": 7.796464920043945,
        "y1": 7.786821365356445,
        "y2": 1.9328348636627197
      },
      {
        "r": 0,
        "text": "To this end, we present G-Scan, the first end-to-end fine-grained line-level vulnerability detection system evaluated on the first-of-its-kind real world dataset.",
        "trunc_text": "To this end, we present G-Scan, the first end-to-end fine-grained line-level vulnerability detection system evaluated on",
        "x1": 7.714566230773926,
        "x2": 6.743570327758789,
        "y1": 5.096972942352295,
        "y2": 6.454127788543701
      },
      {
        "r": 0,
        "text": "We train and evaluate G-Scan on a collected real world smart contracts dataset with line-level annotations on reentrancy vulnerability, one of the most common and severe types of smart contract vulnerabilities",
        "trunc_text": "We train and evaluate G-Scan on a collected real world smart contracts dataset with line-level annotations on reentrancy",
        "x1": 7.759024143218994,
        "x2": 6.712479114532471,
        "y1": 5.094512939453125,
        "y2": 6.441836357116699
      },
      {
        "r": 0,
        "text": "Due to the immutable and decentralized nature of Ethereum (ETH) platform, smart contracts are prone to security risks that can result in financial loss.",
        "trunc_text": "Due to the immutable and decentralized nature of Ethereum (ETH) platform, smart contracts are prone to security risks th",
        "x1": 7.690593719482422,
        "x2": 6.655864238739014,
        "y1": 5.109384059906006,
        "y2": 6.36011266708374
      },
      {
        "r": 0,
        "text": "While existing machine learning-based vulnerability detection algorithms achieve high accuracy at the contract level, they require developers to manually inspect source code to locate bugs.  ",
        "trunc_text": "While existing machine learning-based vulnerability detection algorithms achieve high accuracy at the contract level, th",
        "x1": 7.6514573097229,
        "x2": 6.697811603546143,
        "y1": 5.092430591583252,
        "y2": 6.457630634307861
      },
      {
        "r": 0,
        "text": "G-Scan first converts smart contracts to code graphs in a dependency and hierarchy preserving manner.",
        "trunc_text": "G-Scan first converts smart contracts to code graphs in a dependency and hierarchy preserving manner.",
        "x1": 7.739897727966309,
        "x2": 6.676659107208252,
        "y1": 5.05525541305542,
        "y2": 6.40204381942749
      },
      {
        "r": 0,
        "text": "Next, we train a graph neural network to identify vulnerable nodes and assess security risks.",
        "trunc_text": "Next, we train a graph neural network to identify vulnerable nodes and assess security risks.",
        "x1": 7.663163185119629,
        "x2": 6.7140607833862305,
        "y1": 5.1836748123168945,
        "y2": 6.339344501495361
      },
      {
        "r": 0,
        "text": "Finally, the code graphs with node vulnerability predictions are mapped back to the smart contracts for line-level localization.",
        "trunc_text": "Finally, the code graphs with node vulnerability predictions are mapped back to the smart contracts for line-level local",
        "x1": 7.750488758087158,
        "x2": 6.698680400848389,
        "y1": 5.101024150848389,
        "y2": 6.418078899383545
      },
      {
        "r": 0,
        "text": "We train and evaluate G-Scan on a collected real world smart contracts dataset with line-level annotations on reentrancy vulnerability, one of the most common andore in line-level vulnerability localization.",
        "trunc_text": "We train and evaluate G-Scan on a collected real world smart contracts dataset with line-level annotations on reentrancy",
        "x1": 7.757874011993408,
        "x2": 6.72381591796875,
        "y1": 5.0829081535339355,
        "y2": 6.466732501983643
      },
      {
        "r": 0,
        "text": "Additionally, the lightweight graph neural network enables G-Scan to localize vulnerabilities in 6.1k lines of code smart contract within 1.2 seconds.",
        "trunc_text": "Additionally, the lightweight graph neural network enables G-Scan to localize vulnerabilities in 6.1k lines of code smar",
        "x1": 7.737525463104248,
        "x2": 6.734318256378174,
        "y1": 5.0908203125,
        "y2": 6.447942733764648
      },
      {
        "r": 0,
        "text": "In this paper, I train a Bidirectional Long Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data from all 50 US states",
        "trunc_text": "In this paper, I train a Bidirectional Long Short-Term Memory (BiLSTM) model on a novel dataset of voter registration da",
        "x1": 4.562122344970703,
        "x2": 3.571119546890259,
        "y1": 4.783928871154785,
        "y2": 2.6941092014312744
      },
      {
        "r": 0,
        "text": "In the absence of sensitive race and ethnicity data, researchers, regulators, and firms alike turn to proxies.  ",
        "trunc_text": "In the absence of sensitive race and ethnicity data, researchers, regulators, and firms alike turn to proxies.  ",
        "x1": 5.0290398597717285,
        "x2": 6.478541851043701,
        "y1": 9.197967529296875,
        "y2": 1.2626069784164429
      },
      {
        "r": 0,
        "text": "and create an ensemble that achieves up to 36.8% higher out of sample (OOS) F1 scores than the best performing machine learning models in the literature.",
        "trunc_text": "and create an ensemble that achieves up to 36.8% higher out of sample (OOS) F1 scores than the best performing machine l",
        "x1": 4.147884845733643,
        "x2": 5.025412082672119,
        "y1": 6.151576042175293,
        "y2": 3.789777994155884
      },
      {
        "r": 0,
        "text": "Additionally, I construct the most comprehensive database of first and surname distributions in the US in order to improve the coverage and accuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved Firstname Surname Geocoding (BIFSG).",
        "trunc_text": "Additionally, I construct the most comprehensive database of first and surname distributions in the US in order to impro",
        "x1": 4.606963157653809,
        "x2": 5.899113655090332,
        "y1": 7.401909351348877,
        "y2": 2.815387487411499
      },
      {
        "r": 0,
        "text": "Researchers have invested considerable effort into ensuring that large language models (LLMs) align with human values, using various training techniques, such as instruction tuning and Reinforcement Learning from Human or AI Feedback (RLHF/RLAIF), to guard against text unsafety.",
        "trunc_text": "Researchers have invested considerable effort into ensuring that large language models (LLMs) align with human values, u",
        "x1": 5.4380903244018555,
        "x2": 4.076847076416016,
        "y1": 3.9909164905548096,
        "y2": 6.335829257965088
      },
      {
        "r": 0,
        "text": "However, these defenses remain incredibly vulnerable to some jailbreak attacks, which can cause the model to become overly defensive to sensitive topics or still generate harmful content, leaving the model performance particularly fragile.",
        "trunc_text": "However, these defenses remain incredibly vulnerable to some jailbreak attacks, which can cause the model to become over",
        "x1": 7.254390716552734,
        "x2": 6.26729679107666,
        "y1": 5.170536518096924,
        "y2": 6.2330498695373535
      },
      {
        "r": 0,
        "text": "Therefore, to comprehensively study text safety and output robustness, we propose a latent jailbreak prompt dataset, each involving malicious instruction embedding.",
        "trunc_text": "Therefore, to comprehensively study text safety and output robustness, we propose a latent jailbreak prompt dataset, eac",
        "x1": 7.266700744628906,
        "x2": 6.225555419921875,
        "y1": 5.163201332092285,
        "y2": 6.250186920166016
      },
      {
        "r": 0,
        "text": "Specifically, we instruct the model to complete a regular task, such as translation, where the text to be translated contains malicious instructions.",
        "trunc_text": "Specifically, we instruct the model to complete a regular task, such as translation, where the text to be translated con",
        "x1": 5.251622676849365,
        "x2": 4.170234680175781,
        "y1": 4.062755584716797,
        "y2": 6.2325310707092285
      },
      {
        "r": 0,
        "text": "To further analyze the safety and robustness, we design a hierarchical annotation framework.",
        "trunc_text": "To further analyze the safety and robustness, we design a hierarchical annotation framework.",
        "x1": 2.200181245803833,
        "x2": 1.3166003227233887,
        "y1": 3.763627529144287,
        "y2": 5.076022624969482
      },
      {
        "r": 0,
        "text": "We present a systematic analysis of the safety and robustness of LLMs concerning the position of explicit normal instructions, word replacement (verbs in explicit normal instructions, target groups in malicious instructions, cue words in malicious instructions), and instruction replacement (different explicit normal instructions).",
        "trunc_text": "We present a systematic analysis of the safety and robustness of LLMs concerning the position of explicit normal instruc",
        "x1": 5.640978813171387,
        "x2": 4.433072090148926,
        "y1": 4.1618804931640625,
        "y2": 6.2574238777160645
      },
      {
        "r": 0,
        "text": "Our results show that current LLMs not only have a preference for certain instruction verbs, but also exhibit different jailbreak rates for different instruction verbs in explicit normal instructions.",
        "trunc_text": "Our results show that current LLMs not only have a preference for certain instruction verbs, but also exhibit different ",
        "x1": 5.6409077644348145,
        "x2": 4.396338939666748,
        "y1": 4.108137607574463,
        "y2": 6.273751735687256
      },
      {
        "r": 0,
        "text": "In other words, the probability of generating unsafe content by the model will be reinforced to varying degrees depending on the instruction verb in explicit normal instructions.",
        "trunc_text": "In other words, the probability of generating unsafe content by the model will be reinforced to varying degrees dependin",
        "x1": 5.397907733917236,
        "x2": 4.210080623626709,
        "y1": 4.203449726104736,
        "y2": 6.129499912261963
      },
      {
        "r": 0,
        "text": "Code and data are available at https://github.com/qiuhuachuan/latent-jailbreak.",
        "trunc_text": "Code and data are available at https://github.com/qiuhuachuan/latent-jailbreak.",
        "x1": 7.488471031188965,
        "x2": 7.940636157989502,
        "y1": 7.491430759429932,
        "y2": 2.2218754291534424
      },
      {
        "r": 0,
        "text": "Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time.",
        "trunc_text": "Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over t",
        "x1": 3.252639055252075,
        "x2": 3.28322434425354,
        "y1": 5.382534980773926,
        "y2": 3.6024904251098633
      },
      {
        "r": 0,
        "text": "Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks.",
        "trunc_text": "Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks.",
        "x1": 3.972414016723633,
        "x2": 4.029248237609863,
        "y1": 5.365714073181152,
        "y2": 4.559328079223633
      },
      {
        "r": 0,
        "text": "Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search.",
        "trunc_text": "Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to bread",
        "x1": 2.9860146045684814,
        "x2": 2.918548822402954,
        "y1": 5.257502555847168,
        "y2": 3.6605401039123535
      },
      {
        "r": 0,
        "text": "However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs.",
        "trunc_text": "However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic grap",
        "x1": 3.223008871078491,
        "x2": 3.27946400642395,
        "y1": 5.405046463012695,
        "y2": 3.6258225440979004
      },
      {
        "r": 0,
        "text": "To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models.",
        "trunc_text": "To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-ti",
        "x1": 3.1587905883789062,
        "x2": 3.2447047233581543,
        "y1": 5.450387954711914,
        "y2": 3.535611391067505
      },
      {
        "r": 0,
        "text": "To achieve this, a streaming, low latency approximation to the random-walk based features is proposed.",
        "trunc_text": "To achieve this, a streaming, low latency approximation to the random-walk based features is proposed.",
        "x1": 3.0099146366119385,
        "x2": 3.2500882148742676,
        "y1": 5.768486976623535,
        "y2": 3.359003782272339
      },
      {
        "r": 0,
        "text": "In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop operations on the incoming edges.",
        "trunc_text": "In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop operat",
        "x1": 3.021005153656006,
        "x2": 2.9088306427001953,
        "y1": 5.1511688232421875,
        "y2": 3.7561240196228027
      },
      {
        "r": 0,
        "text": "We evaluate our proposed approach on three open-source datasets and two in-house datasets, and compare with three state-of-the-art algorithms (TGN-attn, TGN-ID, Jodie).",
        "trunc_text": "We evaluate our proposed approach on three open-source datasets and two in-house datasets, and compare with three state-",
        "x1": 4.398857116699219,
        "x2": 4.720609664916992,
        "y1": 7.059112548828125,
        "y2": 2.878941297531128
      },
      {
        "r": 0,
        "text": "We demonstrate that our graph-sprints features, combined with a machine learning classifier, achieve competitive performance (outperforming all baselines for the node classification tasks in five datasets).",
        "trunc_text": "We demonstrate that our graph-sprints features, combined with a machine learning classifier, achieve competitive perform",
        "x1": 3.173074960708618,
        "x2": 3.24582576751709,
        "y1": 5.47675085067749,
        "y2": 3.510235548019409
      },
      {
        "r": 0,
        "text": "Simultaneously, graph-sprints significantly reduce inference latencies, achieving close to an order of magnitude speed-up in our experimental setting.",
        "trunc_text": "Simultaneously, graph-sprints significantly reduce inference latencies, achieving close to an order of magnitude speed-u",
        "x1": 3.3213562965393066,
        "x2": 3.404758930206299,
        "y1": 5.543153285980225,
        "y2": 3.637091636657715
      },
      {
        "r": 0,
        "text": "We collected and published a dataset of 18,000 images in lab and hospital environments.",
        "trunc_text": "We collected and published a dataset of 18,000 images in lab and hospital environments.",
        "x1": 2.0513641834259033,
        "x2": 3.783088207244873,
        "y1": 7.19929313659668,
        "y2": 1.9026174545288086
      },
      {
        "r": 0,
        "text": "The contribution is to establish a baseline on this new dataset and to provide a proof of concept for the human emergency detection in this use case.",
        "trunc_text": "The contribution is to establish a baseline on this new dataset and to provide a proof of concept for the human emergenc",
        "x1": 3.2855567932128906,
        "x2": 4.487644672393799,
        "y1": 9.202414512634277,
        "y2": 0.057270828634500504
      },
      {
        "r": 0,
        "text": "Human transports in hospitals are labor-intensive and primarily performed in beds to save time.",
        "trunc_text": "Human transports in hospitals are labor-intensive and primarily performed in beds to save time.",
        "x1": 3.280418634414673,
        "x2": 4.580013275146484,
        "y1": 9.221014022827148,
        "y2": -0.22460080683231354
      },
      {
        "r": 0,
        "text": "This transfer method does not promote the mobility or autonomy of the patient.",
        "trunc_text": "This transfer method does not promote the mobility or autonomy of the patient.",
        "x1": 3.3580548763275146,
        "x2": 4.606754779815674,
        "y1": 9.12173843383789,
        "y2": -0.20164716243743896
      },
      {
        "r": 0,
        "text": "To relieve the caregivers from this time-consuming task, a mobile robot is developed to autonomously transport humans around the hospital.",
        "trunc_text": "To relieve the caregivers from this time-consuming task, a mobile robot is developed to autonomously transport humans ar",
        "x1": 3.2203259468078613,
        "x2": 4.590658187866211,
        "y1": 9.241908073425293,
        "y2": -0.27578383684158325
      },
      {
        "r": 0,
        "text": "It provides different transfer modes including walking and sitting in a wheelchair.",
        "trunc_text": "It provides different transfer modes including walking and sitting in a wheelchair.",
        "x1": 3.2602033615112305,
        "x2": 4.615517616271973,
        "y1": 9.2655029296875,
        "y2": -0.1919066458940506
      },
      {
        "r": 0,
        "text": "The problem that this paper focuses on is to detect emergencies and ensure the well-being of the patient during the transport.",
        "trunc_text": "The problem that this paper focuses on is to detect emergencies and ensure the well-being of the patient during the tran",
        "x1": 3.2617366313934326,
        "x2": 4.5265350341796875,
        "y1": 9.193877220153809,
        "y2": -0.17116372287273407
      },
      {
        "r": 0,
        "text": "For this purpose, the patient is tracked and monitored with a camera system.",
        "trunc_text": "For this purpose, the patient is tracked and monitored with a camera system.",
        "x1": 3.0381054878234863,
        "x2": 4.33817195892334,
        "y1": 9.195160865783691,
        "y2": -0.14519430696964264
      },
      {
        "r": 0,
        "text": "OpenPose is used for Human Pose Estimation and a trained classifier for emergency detection.  ",
        "trunc_text": "OpenPose is used for Human Pose Estimation and a trained classifier for emergency detection.  ",
        "x1": 2.761507749557495,
        "x2": 4.252194404602051,
        "y1": 9.366220474243164,
        "y2": -0.17411337792873383
      },
      {
        "r": 0,
        "text": "It differs from related work because we have a moving robot with different transfer modes in a highly dynamic environment with multiple people in the scene using only RGB-D data.",
        "trunc_text": "It differs from related work because we have a moving robot with different transfer modes in a highly dynamic environmen",
        "x1": 2.846569061279297,
        "x2": 4.345654487609863,
        "y1": 9.338330268859863,
        "y2": -0.3081490099430084
      },
      {
        "r": 0,
        "text": "To improve the critical recall metric, we apply threshold moving and a time delay.",
        "trunc_text": "To improve the critical recall metric, we apply threshold moving and a time delay.",
        "x1": 3.154916763305664,
        "x2": 4.0585713386535645,
        "y1": 6.287741661071777,
        "y2": 2.9466938972473145
      },
      {
        "r": 0,
        "text": "We compare different models with an AutoML approach.",
        "trunc_text": "We compare different models with an AutoML approach.",
        "x1": 4.349594593048096,
        "x2": 4.663631916046143,
        "y1": 5.969520568847656,
        "y2": 4.153691291809082
      },
      {
        "r": 0,
        "text": "This paper shows that emergencies while walking are best detected by a SVM with a recall of 95.8% on single frames.",
        "trunc_text": "This paper shows that emergencies while walking are best detected by a SVM with a recall of 95.8% on single frames.",
        "x1": 3.1955013275146484,
        "x2": 4.51315450668335,
        "y1": 9.169610023498535,
        "y2": -0.030023880302906036
      },
      {
        "r": 0,
        "text": "In the case of sitting transport, the best model achieves a recall of 62.2%.",
        "trunc_text": "In the case of sitting transport, the best model achieves a recall of 62.2%.",
        "x1": 3.3769102096557617,
        "x2": 4.55847692489624,
        "y1": 8.942770957946777,
        "y2": 0.026333652436733246
      },
      {
        "r": 0,
        "text": "The contribution is to establish a baseline on this new dataset and to provide a proof",
        "trunc_text": "The contribution is to establish a baseline on this new dataset and to provide a proof",
        "x1": 5.239556789398193,
        "x2": 5.881051540374756,
        "y1": 7.430190086364746,
        "y2": 2.46632719039917
      },
      {
        "r": 0,
        "text": "End-to-end model, especially Recurrent Neural Network Transducer (RNN-T), has achieved great success in speech recognition.",
        "trunc_text": "End-to-end model, especially Recurrent Neural Network Transducer (RNN-T), has achieved great success in speech recogniti",
        "x1": 4.508080959320068,
        "x2": 3.7143051624298096,
        "y1": 4.0164313316345215,
        "y2": 5.5200581550598145
      },
      {
        "r": 0,
        "text": "However, transducer requires a great memory footprint and computing time when processing a long decoding sequence.",
        "trunc_text": "However, transducer requires a great memory footprint and computing time when processing a long decoding sequence.",
        "x1": 4.41780948638916,
        "x2": 3.903104066848755,
        "y1": 4.814936637878418,
        "y2": 5.371396064758301
      },
      {
        "r": 0,
        "text": "To solve this problem, we propose a model named time-sparse transducer, which introduces a time-sparse mechanism into transducer.",
        "trunc_text": "To solve this problem, we propose a model named time-sparse transducer, which introduces a time-sparse mechanism into tr",
        "x1": 4.328946590423584,
        "x2": 3.891737461090088,
        "y1": 4.863180160522461,
        "y2": 5.242049694061279
      },
      {
        "r": 0,
        "text": "In this mechanism, we obtain the intermediate representations by reducing the time resolution of the hidden states.",
        "trunc_text": "In this mechanism, we obtain the intermediate representations by reducing the time resolution of the hidden states.",
        "x1": 2.5734500885009766,
        "x2": 2.5737903118133545,
        "y1": 5.781879425048828,
        "y2": 3.176562547683716
      },
      {
        "r": 0,
        "text": "Then the weighted average algorithm is used to combine these representations into sparse hidden states followed by the decoder.",
        "trunc_text": "Then the weighted average algorithm is used to combine these representations into sparse hidden states followed by the d",
        "x1": 2.628661632537842,
        "x2": 2.5833847522735596,
        "y1": 5.732096195220947,
        "y2": 3.340944528579712
      },
      {
        "r": 0,
        "text": "All the experiments are conducted on a Mandarin dataset AISHELL-1.",
        "trunc_text": "All the experiments are conducted on a Mandarin dataset AISHELL-1.",
        "x1": 4.373405456542969,
        "x2": 2.5232338905334473,
        "y1": 2.7082512378692627,
        "y2": 6.4282402992248535
      },
      {
        "r": 0,
        "text": "Compared with RNN-T, the character error rate of the time-sparse transducer is close to RNN-T and the real-time factor is 50.00% of the original.",
        "trunc_text": "Compared with RNN-T, the character error rate of the time-sparse transducer is close to RNN-T and the real-time factor i",
        "x1": 4.355329990386963,
        "x2": 3.9665069580078125,
        "y1": 4.908956527709961,
        "y2": 5.2967848777771
      },
      {
        "r": 0,
        "text": "By adjusting the time resolution, the time-sparse transducer can also reduce the real-time factor to 16.54% of the original at the expense of a 4.94% loss of precision.",
        "trunc_text": "By adjusting the time resolution, the time-sparse transducer can also reduce the real-time factor to 16.54% of the origi",
        "x1": 4.33950138092041,
        "x2": 3.9480040073394775,
        "y1": 4.865241050720215,
        "y2": 5.255940914154053
      },
      {
        "r": 0,
        "text": "To address this abstraction gap and provide a fair evaluation of the proposed method, we develop our method on a large-scale synthetic dataset covering 500k+ buildings with well-defined ground truths of polyhedral class labels.",
        "trunc_text": "To address this abstraction gap and provide a fair evaluation of the proposed method, we develop our method on a large-s",
        "x1": 2.837775468826294,
        "x2": 4.044806480407715,
        "y1": 8.069721221923828,
        "y2": 2.457364797592163
      },
      {
        "r": 0,
        "text": "We present PolyGNN, a polyhedron-based graph neural network for 3D building reconstruction from point clouds.",
        "trunc_text": "We present PolyGNN, a polyhedron-based graph neural network for 3D building reconstruction from point clouds.",
        "x1": 2.46400785446167,
        "x2": 3.2324559688568115,
        "y1": 8.27696418762207,
        "y2": 2.8536016941070557
      },
      {
        "r": 0,
        "text": "PolyGNN learns to assemble primitives obtained by polyhedral decomposition via graph node classification, achieving a watertight, compact, and weakly semantic reconstruction.",
        "trunc_text": "PolyGNN learns to assemble primitives obtained by polyhedral decomposition via graph node classification, achieving a wa",
        "x1": 3.0814032554626465,
        "x2": 3.1416127681732178,
        "y1": 5.236393451690674,
        "y2": 3.3364756107330322
      },
      {
        "r": 0,
        "text": "To effectively represent arbitrary-shaped polyhedra in the neural network, we propose three different sampling strategies to select representative points as polyhedron-wise queries, enabling efficient occupancy inference.",
        "trunc_text": "To effectively represent arbitrary-shaped polyhedra in the neural network, we propose three different sampling strategie",
        "x1": 2.3391427993774414,
        "x2": 3.2881805896759033,
        "y1": 8.38320541381836,
        "y2": 2.7011353969573975
      },
      {
        "r": 0,
        "text": "Furthermore, we incorporate the inter-polyhedron adjacency to enhance the classification of the graph nodes.",
        "trunc_text": "Furthermore, we incorporate the inter-polyhedron adjacency to enhance the classification of the graph nodes.",
        "x1": 3.167801856994629,
        "x2": 3.1279196739196777,
        "y1": 5.340455532073975,
        "y2": 3.317617177963257
      },
      {
        "r": 0,
        "text": "We also observe that existing city-building models are abstractions of the underlying instances.  ",
        "trunc_text": "We also observe that existing city-building models are abstractions of the underlying instances.  ",
        "x1": 2.9579272270202637,
        "x2": 4.801455497741699,
        "y1": 8.258988380432129,
        "y2": 1.3039523363113403
      },
      {
        "r": 0,
        "text": "We further conduct a transferability analysis across cities and on real-world point clouds.",
        "trunc_text": "We further conduct a transferability analysis across cities and on real-world point clouds.",
        "x1": 2.1390278339385986,
        "x2": 4.770698547363281,
        "y1": 9.082695960998535,
        "y2": 1.2445892095565796
      },
      {
        "r": 0,
        "text": "Both qualitative and quantitative results demonstrate the effectiveness of our method, particularly its efficiency for large-scale reconstructions.",
        "trunc_text": "Both qualitative and quantitative results demonstrate the effectiveness of our method, particularly its efficiency for l",
        "x1": 2.110414981842041,
        "x2": 3.7484405040740967,
        "y1": 8.51771354675293,
        "y2": 0.78789883852005
      },
      {
        "r": 0,
        "text": "The source code and data of our work are available at https://github.com/chenzhaiyu/polygnn.",
        "trunc_text": "The source code and data of our work are available at https://github.com/chenzhaiyu/polygnn.",
        "x1": 7.597068786621094,
        "x2": 8.156744956970215,
        "y1": 7.646080493927002,
        "y2": 1.9754480123519897
      },
      {
        "r": 0,
        "text": "Curating an informative and representative dataset is essential for enhancing the performance of 2D object detectors.",
        "trunc_text": "Curating an informative and representative dataset is essential for enhancing the performance of 2D object detectors.",
        "x1": 0.7369475364685059,
        "x2": 2.3332839012145996,
        "y1": 7.509226322174072,
        "y2": 1.4756096601486206
      },
      {
        "r": 0,
        "text": "We present a novel active learning sampling strategy that addresses both the informativeness and diversity of the selections.",
        "trunc_text": "We present a novel active learning sampling strategy that addresses both the informativeness and diversity of the select",
        "x1": 3.521611213684082,
        "x2": 3.9632155895233154,
        "y1": 5.851121425628662,
        "y2": 3.898303270339966
      },
      {
        "r": 0,
        "text": "Our strategy integrates uncertainty and diversity-based selection principles into a joint selection objective by measuring the collective information score of the selected samples.",
        "trunc_text": "Our strategy integrates uncertainty and diversity-based selection principles into a joint selection objective by measuri",
        "x1": 3.556283473968506,
        "x2": 4.008376121520996,
        "y1": 5.826238632202148,
        "y2": 3.886401891708374
      },
      {
        "r": 0,
        "text": "Specifically, our proposed NORIS algorithm quantifies the impact of training with a sample on the informativeness of other similar samples.",
        "trunc_text": "Specifically, our proposed NORIS algorithm quantifies the impact of training with a sample on the informativeness of oth",
        "x1": 3.5121829509735107,
        "x2": 4.012988090515137,
        "y1": 5.805332183837891,
        "y2": 3.8935749530792236
      },
      {
        "r": 0,
        "text": "By exclusively selecting samples that are simultaneously informative and distant from other highly informative samples, we effectively avoid redundancy while maintaining a high level of informativeness.",
        "trunc_text": "By exclusively selecting samples that are simultaneously informative and distant from other highly informative samples, ",
        "x1": 3.5228748321533203,
        "x2": 3.982325553894043,
        "y1": 5.868955612182617,
        "y2": 3.9096622467041016
      },
      {
        "r": 0,
        "text": "Moreover, instead of utilizing whole image features to calculate distances between samples, we leverage features extracted from detected object regions within images to define object features.",
        "trunc_text": "Moreover, instead of utilizing whole image features to calculate distances between samples, we leverage features extract",
        "x1": 0.8114896416664124,
        "x2": 2.3458163738250732,
        "y1": 7.554863929748535,
        "y2": 1.3375059366226196
      },
      {
        "r": 0,
        "text": "This allows us to construct a dataset encompassing diverse object types, shapes, and angles.",
        "trunc_text": "This allows us to construct a dataset encompassing diverse object types, shapes, and angles.",
        "x1": 1.9468786716461182,
        "x2": 4.1419501304626465,
        "y1": 7.935943126678467,
        "y2": 1.4213861227035522
      },
      {
        "r": 0,
        "text": "Extensive experiments on object detection and image classification tasks demonstrate the effectiveness of our strategy over the state-of-the-art baselines.",
        "trunc_text": "Extensive experiments on object detection and image classification tasks demonstrate the effectiveness of our strategy o",
        "x1": 0.7016410231590271,
        "x2": 2.1449813842773438,
        "y1": 7.498077869415283,
        "y2": 1.351574420928955
      },
      {
        "r": 0,
        "text": "Specifically, our selection strategy achieves a 20% and 30% reduction in labeling costs compared to random selection for PASCAL-VOC and KITTI, respectively.",
        "trunc_text": "Specifically, our selection strategy achieves a 20% and 30% reduction in labeling costs compared to random selection for",
        "x1": 3.5066285133361816,
        "x2": 3.9369940757751465,
        "y1": 5.748623847961426,
        "y2": 3.901834011077881
      },
      {
        "r": 0,
        "text": "To provide a more robust evaluation of the proposed method, a large-scale clinical image dataset of skin diseases with significantly more cases than existing datasets has been established.",
        "trunc_text": "To provide a more robust evaluation of the proposed method, a large-scale clinical image dataset of skin diseases with s",
        "x1": 3.149214267730713,
        "x2": 4.297126293182373,
        "y1": 7.643558502197266,
        "y2": 1.9886270761489868
      },
      {
        "r": 0,
        "text": "Skin diseases are among the most prevalent health issues, and accurate computer-aided diagnosis methods are of importance for both dermatologists and patients.",
        "trunc_text": "Skin diseases are among the most prevalent health issues, and accurate computer-aided diagnosis methods are of importanc",
        "x1": 3.2280497550964355,
        "x2": 4.3733954429626465,
        "y1": 7.704735279083252,
        "y2": 1.9715802669525146
      },
      {
        "r": 0,
        "text": "However, most of the existing methods overlook the essential domain knowledge required for skin disease diagnosis.",
        "trunc_text": "However, most of the existing methods overlook the essential domain knowledge required for skin disease diagnosis.",
        "x1": 3.2067208290100098,
        "x2": 4.37231969833374,
        "y1": 7.6707963943481445,
        "y2": 1.9930049180984497
      },
      {
        "r": 0,
        "text": "A novel multi-task model, namely DermImitFormer, is proposed to fill this gap by imitating dermatologists' diagnostic procedures and strategies.",
        "trunc_text": "A novel multi-task model, namely DermImitFormer, is proposed to fill this gap by imitating dermatologists' diagnostic pr",
        "x1": 3.1928412914276123,
        "x2": 4.347733020782471,
        "y1": 7.708665370941162,
        "y2": 1.90021550655365
      },
      {
        "r": 0,
        "text": "Through multi-task learning, the model simultaneously predicts body parts and lesion attributes in addition to the disease itself, enhancing diagnosis accuracy and improving diagnosis interpretability.",
        "trunc_text": "Through multi-task learning, the model simultaneously predicts body parts and lesion attributes in addition to the disea",
        "x1": 3.270965576171875,
        "x2": 4.417003631591797,
        "y1": 7.706735134124756,
        "y2": 2.0224690437316895
      },
      {
        "r": 0,
        "text": "The designed lesion selection module mimics dermatologists' zoom-in action, effectively highlighting the local lesion features from noisy backgrounds.",
        "trunc_text": "The designed lesion selection module mimics dermatologists' zoom-in action, effectively highlighting the local lesion fe",
        "x1": 3.1426889896392822,
        "x2": 4.2288312911987305,
        "y1": 7.696037292480469,
        "y2": 1.910201072692871
      },
      {
        "r": 0,
        "text": "Additionally, the presented cross-interaction module explicitly models the complicated diagnostic reasoning between body parts, lesion attributes, and diseases.  ",
        "trunc_text": "Additionally, the presented cross-interaction module explicitly models the complicated diagnostic reasoning between body",
        "x1": 3.252855062484741,
        "x2": 4.387753009796143,
        "y1": 7.719786167144775,
        "y2": 1.982303500175476
      },
      {
        "r": 0,
        "text": "Extensive experiments on three different datasets consistently demonstrate the state-of-the-art recognition performance of the proposed approach.",
        "trunc_text": "Extensive experiments on three different datasets consistently demonstrate the state-of-the-art recognition performance ",
        "x1": 3.989227294921875,
        "x2": 4.919995307922363,
        "y1": 7.508460521697998,
        "y2": 2.770686149597168
      },
      {
        "r": 0,
        "text": "To stimulate research in this exciting area, we present LOAF, the first large-scale overhead fisheye dataset for person detection and localization.",
        "trunc_text": "To stimulate research in this exciting area, we present LOAF, the first large-scale overhead fisheye dataset for person ",
        "x1": 2.0184214115142822,
        "x2": 3.873070240020752,
        "y1": 9.517827987670898,
        "y2": -0.021589593961834908
      },
      {
        "r": 0,
        "text": "it contains currently the largest number of annotated pedestrian, i.e., 457K bounding boxes with groundtruth location information",
        "trunc_text": "it contains currently the largest number of annotated pedestrian, i.e., 457K bounding boxes with groundtruth location in",
        "x1": 2.325819969177246,
        "x2": 4.217332363128662,
        "y1": 9.263296127319336,
        "y2": 0.13579218089580536
      },
      {
        "r": 0,
        "text": "Location determination finds wide applications in daily life.",
        "trunc_text": "Location determination finds wide applications in daily life.",
        "x1": 3.577934980392456,
        "x2": 4.905535697937012,
        "y1": 9.556597709655762,
        "y2": 0.21133650839328766
      },
      {
        "r": 0,
        "text": "Instead of existing efforts devoted to localizing tourist photos captured by perspective cameras, in this article, we focus on devising person positioning solutions using overhead fisheye cameras.",
        "trunc_text": "Instead of existing efforts devoted to localizing tourist photos captured by perspective cameras, in this article, we fo",
        "x1": 1.9258990287780762,
        "x2": 3.8199594020843506,
        "y1": 9.510037422180176,
        "y2": -0.027654478326439857
      },
      {
        "r": 0,
        "text": "Such solutions are advantageous in large field of view (FOV), low cost, anti-occlusion, and unaggressive work mode (without the necessity of cameras carried by persons).",
        "trunc_text": "Such solutions are advantageous in large field of view (FOV), low cost, anti-occlusion, and unaggressive work mode (with",
        "x1": 1.4055761098861694,
        "x2": 3.499847412109375,
        "y1": 9.142552375793457,
        "y2": 0.2078382521867752
      },
      {
        "r": 0,
        "text": "However, related studies are quite scarce, due to the paucity of data.  ",
        "trunc_text": "However, related studies are quite scarce, due to the paucity of data.  ",
        "x1": 5.897631645202637,
        "x2": 6.445957660675049,
        "y1": 7.560527801513672,
        "y2": 2.8979439735412598
      },
      {
        "r": 0,
        "text": "LOAF is built with many essential features, e.g., i) the data cover abundant diversities in scenes, human pose, density, and location; ii) it contains currently the largest number of annotated pedestrian, i.e., 457K bounding boxes with groundtruth location information; iii) the body-boperson detection network, which exploits the fisheye distortions by a rotation-equivariant training strategy and predict radius-aligned human boxes end-to-end.",
        "trunc_text": "LOAF is built with many essential features, e.g., i) the data cover abundant diversities in scenes, human pose, density,",
        "x1": 2.3002872467041016,
        "x2": 4.1167192459106445,
        "y1": 9.432687759399414,
        "y2": -0.05037252977490425
      },
      {
        "r": 0,
        "text": "Then, the actual locations of the detected persons are calculated by a numerical solution on the fisheye model and camera altitude data.",
        "trunc_text": "Then, the actual locations of the detected persons are calculated by a numerical solution on the fisheye model and camer",
        "x1": 1.9942973852157593,
        "x2": 3.942882776260376,
        "y1": 9.496482849121094,
        "y2": -0.010880408808588982
      },
      {
        "r": 0,
        "text": "Extensive experiments on LOAF validate the superiority of our fisheye detector w.r.t.",
        "trunc_text": "Extensive experiments on LOAF validate the superiority of our fisheye detector w.r.t.",
        "x1": 1.9452906847000122,
        "x2": 3.7982354164123535,
        "y1": 9.521052360534668,
        "y2": -0.08107111603021622
      },
      {
        "r": 0,
        "text": "previous methods, and show that our whole fisheye positioning solution is able to locate all persons in FOV with an accuracy of 0.5 m, within 0.1 s.",
        "trunc_text": "previous methods, and show that our whole fisheye positioning solution is able to locate all persons in FOV with an accu",
        "x1": 1.7865393161773682,
        "x2": 3.776923894882202,
        "y1": 9.45557689666748,
        "y2": -0.03446931019425392
      },
      {
        "r": 0,
        "text": "Hand trajectory forecasting from egocentric views is crucial for enabling a prompt understanding of human intentions when interacting with AR/VR systems.",
        "trunc_text": "Hand trajectory forecasting from egocentric views is crucial for enabling a prompt understanding of human intentions whe",
        "x1": 3.018103837966919,
        "x2": 4.503331184387207,
        "y1": 9.485260009765625,
        "y2": -0.5769789814949036
      },
      {
        "r": 0,
        "text": "However, existing methods handle this problem in a 2D image space which is inadequate for 3D real-world applications.",
        "trunc_text": "However, existing methods handle this problem in a 2D image space which is inadequate for 3D real-world applications.",
        "x1": 1.5690171718597412,
        "x2": 3.422545909881592,
        "y1": 8.785399436950684,
        "y2": 0.5525158047676086
      },
      {
        "r": 0,
        "text": "In this paper, we set up an egocentric 3D hand trajectory forecasting task that aims to predict hand trajectories in a 3D space from early observed RGB videos in a first-person view.",
        "trunc_text": "In this paper, we set up an egocentric 3D hand trajectory forecasting task that aims to predict hand trajectories in a 3",
        "x1": 2.964500665664673,
        "x2": 4.3155412673950195,
        "y1": 9.533269882202148,
        "y2": -0.5936024188995361
      },
      {
        "r": 0,
        "text": "To fulfill this goal, we propose an uncertainty-aware state space Transformer (USST) that takes the merits of the attention mechanism and aleatoric uncertainty within the framework of the classical state-space model.",
        "trunc_text": "To fulfill this goal, we propose an uncertainty-aware state space Transformer (USST) that takes the merits of the attent",
        "x1": 2.8023998737335205,
        "x2": 2.3747973442077637,
        "y1": 8.32088565826416,
        "y2": 2.288459539413452
      },
      {
        "r": 0,
        "text": "The model can be further enhanced by the velocity constraint and visual prompt tuning (VPT) on large vision transformers.",
        "trunc_text": "The model can be further enhanced by the velocity constraint and visual prompt tuning (VPT) on large vision transformers",
        "x1": 2.7474710941314697,
        "x2": 2.4096906185150146,
        "y1": 8.297849655151367,
        "y2": 2.3805387020111084
      },
      {
        "r": 0,
        "text": "Moreover, we develop an annotation workflow to collect 3D hand trajectories with high quality.",
        "trunc_text": "Moreover, we develop an annotation workflow to collect 3D hand trajectories with high quality.",
        "x1": 2.5847649574279785,
        "x2": 4.091989994049072,
        "y1": 9.372088432312012,
        "y2": -0.5748306512832642
      },
      {
        "r": 0,
        "text": "Experimental results on H2O and EgoPAT3D datasets demonstrate the superiority of USST for both 2D and 3D trajectory forecasting.",
        "trunc_text": "Experimental results on H2O and EgoPAT3D datasets demonstrate the superiority of USST for both 2D and 3D trajectory fore",
        "x1": 3.286036968231201,
        "x2": 4.958934783935547,
        "y1": 9.782540321350098,
        "y2": -0.49169468879699707
      },
      {
        "r": 0,
        "text": "The code and datasets are publicly released: https://github.com/Cogito2012/USST.",
        "trunc_text": "The code and datasets are publicly released: https://github.com/Cogito2012/USST.",
        "x1": 7.46456241607666,
        "x2": 7.820495128631592,
        "y1": 7.683403491973877,
        "y2": 2.15449595451355
      },
      {
        "r": 0,
        "text": "We also build a stereo visual acquisition system composed of an event camera and an RGB-D camera to collect a new Stereo Event-Intensity Dataset (SEID) containing diverse scenes with complex motions and varying depths.",
        "trunc_text": "We also build a stereo visual acquisition system composed of an event camera and an RGB-D camera to collect a new Stereo",
        "x1": 1.0867900848388672,
        "x2": 2.964750289916992,
        "y1": 9.0274019241333,
        "y2": 0.13249453902244568
      },
      {
        "r": 0,
        "text": "The stereo event-intensity camera setup is widely applied to leverage the advantages of both event cameras with low latency and intensity cameras that capture accurate brightness and texture information.",
        "trunc_text": "The stereo event-intensity camera setup is widely applied to leverage the advantages of both event cameras with low late",
        "x1": 1.0802581310272217,
        "x2": 2.9481606483459473,
        "y1": 9.070813179016113,
        "y2": 0.14038361608982086
      },
      {
        "r": 0,
        "text": "However, such a setup commonly encounters cross-modality parallax that is difficult to be eliminated solely with stereo rectification especially for real-world scenes with complex motions and varying depths, posing artifacts and distortion for existing Event-based Video Frame Interpolation (E-VFI) approaches.",
        "trunc_text": "However, such a setup commonly encounters cross-modality parallax that is difficult to be eliminated solely with stereo ",
        "x1": 1.0582047700881958,
        "x2": 2.981926918029785,
        "y1": 8.96293830871582,
        "y2": 0.27524274587631226
      },
      {
        "r": 0,
        "text": "To tackle this problem, we propose a novel Stereo Event-based VFI (SE-VFI) network (SEVFI-Net) to generate high-quality intermediate frames and corresponding disparities from misaligned inputs consisting of two consecutive keyframes and event streams emitted between them.",
        "trunc_text": "To tackle this problem, we propose a novel Stereo Event-based VFI (SE-VFI) network (SEVFI-Net) to generate high-quality ",
        "x1": 1.0357081890106201,
        "x2": 2.911065101623535,
        "y1": 9.015974998474121,
        "y2": 0.1843307763338089
      },
      {
        "r": 0,
        "text": "Specifically, we propose a Feature Aggregation Module (FAM) to alleviate the parallax and achieve spatial alignment in the feature domain.",
        "trunc_text": "Specifically, we propose a Feature Aggregation Module (FAM) to alleviate the parallax and achieve spatial alignment in t",
        "x1": 1.8129997253417969,
        "x2": 3.102992296218872,
        "y1": 8.391413688659668,
        "y2": 0.6733863353729248
      },
      {
        "r": 0,
        "text": "We then exploit the fused features accomplishing accurate optical flow and disparity estimation, and achieving better interpolated results through flow-based and synthesis-based ways.  Experiments on public real-world stereo datasets, i.e., DSEC and MVSEC, and our SEID dataset demonstrate that our proposed SEVFI-Net outperforms state-of-the-art methods by a large margin.",
        "trunc_text": "We then exploit the fused features accomplishing accurate optical flow and disparity estimation, and achieving better in",
        "x1": 1.001095175743103,
        "x2": 2.868245840072632,
        "y1": 8.999939918518066,
        "y2": 0.14601005613803864
      },
      {
        "r": 0,
        "text": "We evaluated the framework with five Python and Java code generation models and six prompt datasets, including a newly created one in this work (SOEval).",
        "trunc_text": "We evaluated the framework with five Python and Java code generation models and six prompt datasets, including a newly c",
        "x1": 6.328703880310059,
        "x2": 5.441020488739014,
        "y1": 4.136227130889893,
        "y2": 5.936368942260742
      },
      {
        "r": 0,
        "text": "In recent years, the use of automated source code generation utilizing transformer-based generative models has expanded, and these models can generate functional code according to the requirements of the developers.",
        "trunc_text": "In recent years, the use of automated source code generation utilizing transformer-based generative models has expanded,",
        "x1": 6.925063133239746,
        "x2": 6.002933502197266,
        "y1": 5.098676681518555,
        "y2": 6.091067790985107
      },
      {
        "r": 0,
        "text": "However, recent research revealed that these automatically generated source codes can contain vulnerabilities and other quality issues.",
        "trunc_text": "However, recent research revealed that these automatically generated source codes can contain vulnerabilities and other ",
        "x1": 7.543432712554932,
        "x2": 6.574389934539795,
        "y1": 5.142337799072266,
        "y2": 6.37253999710083
      },
      {
        "r": 0,
        "text": "Despite researchers' and practitioners' attempts to enhance code generation models, retraining and fine-tuning large language models is time-consuming and resource-intensive.",
        "trunc_text": "Despite researchers' and practitioners' attempts to enhance code generation models, retraining and fine-tuning large lan",
        "x1": 5.449633598327637,
        "x2": 3.871628761291504,
        "y1": 3.94629168510437,
        "y2": 6.0885515213012695
      },
      {
        "r": 0,
        "text": "Thus, we describe FRANC, a lightweight framework for recommending more secure and high-quality source code derived from transformer-based code generation models.",
        "trunc_text": "Thus, we describe FRANC, a lightweight framework for recommending more secure and high-quality source code derived from ",
        "x1": 7.209928512573242,
        "x2": 6.182910442352295,
        "y1": 5.146539688110352,
        "y2": 6.1601338386535645
      },
      {
        "r": 0,
        "text": "FRANC includes a static filter to make the generated code compilable with heuristics and a quality-aware ranker to sort the code snippets based on a quality score.",
        "trunc_text": "FRANC includes a static filter to make the generated code compilable with heuristics and a quality-aware ranker to sort ",
        "x1": 6.875162124633789,
        "x2": 6.060779571533203,
        "y1": 5.153906345367432,
        "y2": 6.02606725692749
      },
      {
        "r": 0,
        "text": "Moreover, the framework uses prompt engineering to fix persistent quality issues.  ",
        "trunc_text": "Moreover, the framework uses prompt engineering to fix persistent quality issues.  ",
        "x1": 5.411219120025635,
        "x2": 5.868957042694092,
        "y1": 5.806736946105957,
        "y2": 4.346579551696777
      },
      {
        "r": 0,
        "text": "The static filter improves 9% to 46% Java suggestions and 10% to 43% Python suggestions regarding compilability.",
        "trunc_text": "The static filter improves 9% to 46% Java suggestions and 10% to 43% Python suggestions regarding compilability.",
        "x1": 6.434940814971924,
        "x2": 5.8432207107543945,
        "y1": 5.286254405975342,
        "y2": 5.734523296356201
      },
      {
        "r": 0,
        "text": "The average improvement over the NDCG@10 score for the ranking system is 0.0763, and the repairing techniques repair the highest 80% of prompts.",
        "trunc_text": "The average improvement over the NDCG@10 score for the ranking system is 0.0763, and the repairing techniques repair the",
        "x1": 4.573821067810059,
        "x2": 5.221662998199463,
        "y1": 5.945986270904541,
        "y2": 4.179450511932373
      },
      {
        "r": 0,
        "text": "FRANC takes, on average, 1.98 seconds for Java; for Python, it takes 0.08 seconds.",
        "trunc_text": "FRANC takes, on average, 1.98 seconds for Java; for Python, it takes 0.08 seconds.",
        "x1": 6.520002365112305,
        "x2": 5.792104244232178,
        "y1": 5.287096977233887,
        "y2": 5.766978740692139
      },
      {
        "r": 0,
        "text": "Lane detection plays a pivotal role in the field of autonomous vehicles and advanced driving assistant systems (ADAS).",
        "trunc_text": "Lane detection plays a pivotal role in the field of autonomous vehicles and advanced driving assistant systems (ADAS).",
        "x1": 3.598240375518799,
        "x2": 5.272070407867432,
        "y1": 10.173388481140137,
        "y2": -0.08136429637670517
      },
      {
        "r": 0,
        "text": "Over the years, numerous algorithms have emerged, spanning from rudimentary image processing techniques to sophisticated deep neural networks.",
        "trunc_text": "Over the years, numerous algorithms have emerged, spanning from rudimentary image processing techniques to sophisticated",
        "x1": 2.1122686862945557,
        "x2": 2.878610372543335,
        "y1": 6.431124210357666,
        "y2": 2.594881057739258
      },
      {
        "r": 0,
        "text": "The performance of deep learning-based models is highly dependent on the quality of their training data.",
        "trunc_text": "The performance of deep learning-based models is highly dependent on the quality of their training data.",
        "x1": 1.9352471828460693,
        "x2": 2.4454710483551025,
        "y1": 5.844263553619385,
        "y2": 2.9295449256896973
      },
      {
        "r": 0,
        "text": "Consequently, these models often experience a decline in performance when confronted with challenging scenarios such as extreme lighting conditions, partially visible lane markings, and sparse lane markings like Botts' dots.",
        "trunc_text": "Consequently, these models often experience a decline in performance when confronted with challenging scenarios such as ",
        "x1": 3.520988941192627,
        "x2": 5.2107319831848145,
        "y1": 10.088993072509766,
        "y2": -0.05013207346200943
      },
      {
        "r": 0,
        "text": "To address this, we present an end-to-end lane detection and classification system based on deep learning methodologies.",
        "trunc_text": "To address this, we present an end-to-end lane detection and classification system based on deep learning methodologies.",
        "x1": 3.569228172302246,
        "x2": 5.251977920532227,
        "y1": 10.234718322753906,
        "y2": -0.0954233705997467
      },
      {
        "r": 0,
        "text": "In our study, we introduce a unique dataset meticulously curated to encompass scenarios that pose significant challenges for state-of-the-art (SOTA) models.",
        "trunc_text": "In our study, we introduce a unique dataset meticulously curated to encompass scenarios that pose significant challenges",
        "x1": 4.558430194854736,
        "x2": 5.532092094421387,
        "y1": 7.554073810577393,
        "y2": 2.5348753929138184
      },
      {
        "r": 0,
        "text": "Through fine-tuning selected models, we aim to achieve enhanced localization accuracy.",
        "trunc_text": "Through fine-tuning selected models, we aim to achieve enhanced localization accuracy.",
        "x1": 0.7597715258598328,
        "x2": 2.3502895832061768,
        "y1": 8.13559627532959,
        "y2": 0.8242682814598083
      },
      {
        "r": 0,
        "text": "Moreover, we propose a CNN-based classification branch, seamlessly integrated with the detector, facilitating the identification of distinct lane types.",
        "trunc_text": "Moreover, we propose a CNN-based classification branch, seamlessly integrated with the detector, facilitating the identi",
        "x1": 3.5411057472229004,
        "x2": 5.243660926818848,
        "y1": 10.169707298278809,
        "y2": -0.11006248742341995
      },
      {
        "r": 0,
        "text": "This architecture enables informed lane-changing decisions and empowers more resilient ADAS capabilities.",
        "trunc_text": "This architecture enables informed lane-changing decisions and empowers more resilient ADAS capabilities.",
        "x1": 3.6283469200134277,
        "x2": 5.216756820678711,
        "y1": 10.145427703857422,
        "y2": -0.09846463054418564
      },
      {
        "r": 0,
        "text": "We also investigate the effect of using mixed precision training and testing on different models and batch sizes.",
        "trunc_text": "We also investigate the effect of using mixed precision training and testing on different models and batch sizes.",
        "x1": 3.946864128112793,
        "x2": 4.328980922698975,
        "y1": 6.436158657073975,
        "y2": 3.701611042022705
      },
      {
        "r": 0,
        "text": "Experimental evaluations conducted on the widely-used TuSimple dataset, Caltech lane dataset, and our LVLane dataset demonstrate the effectiveness of our model in accurately detecting and classifying lanes amidst challenging scenarios.",
        "trunc_text": "Experimental evaluations conducted on the widely-used TuSimple dataset, Caltech lane dataset, and our LVLane dataset dem",
        "x1": 3.5468826293945312,
        "x2": 5.242389678955078,
        "y1": 10.171305656433105,
        "y2": -0.10127180814743042
      },
      {
        "r": 0,
        "text": "Our method achieves state-of-the-art classification results on the TuSimple dataset.",
        "trunc_text": "Our method achieves state-of-the-art classification results on the TuSimple dataset.",
        "x1": 4.503909587860107,
        "x2": 5.026340007781982,
        "y1": 7.315896034240723,
        "y2": 2.791168451309204
      },
      {
        "r": 0,
        "text": "The code of the work will be published upon the acceptance of the paper.",
        "trunc_text": "The code of the work will be published upon the acceptance of the paper.",
        "x1": 8.373237609863281,
        "x2": 8.933235168457031,
        "y1": 7.301474571228027,
        "y2": 2.711907386779785
      },
      {
        "r": 0,
        "text": "In today's highly connected society, we are constantly asked to provide personal information to retailers, voter surveys, medical professionals, and other data collection efforts.",
        "trunc_text": "In today's highly connected society, we are constantly asked to provide personal information to retailers, voter surveys",
        "x1": 5.532469272613525,
        "x2": 6.460385322570801,
        "y1": 7.935790538787842,
        "y2": 1.7220243215560913
      },
      {
        "r": 0,
        "text": "The collected data is stored in large data warehouses.",
        "trunc_text": "The collected data is stored in large data warehouses.",
        "x1": 5.960940837860107,
        "x2": 6.6423821449279785,
        "y1": 7.249898433685303,
        "y2": 2.68737530708313
      },
      {
        "r": 0,
        "text": "Organisations and statistical agencies share and use this data to facilitate research in public health, economics, sociology, etc.",
        "trunc_text": "Organisations and statistical agencies share and use this data to facilitate research in public health, economics, socio",
        "x1": 5.846716403961182,
        "x2": 6.673190116882324,
        "y1": 7.701438903808594,
        "y2": 2.5329339504241943
      },
      {
        "r": 0,
        "text": "However, this data contains sensitive information about individuals, which can result in identity theft, financial loss, stress and depression, embarrassment, abuse, etc.",
        "trunc_text": "However, this data contains sensitive information about individuals, which can result in identity theft, financial loss,",
        "x1": 5.038698196411133,
        "x2": 6.4826741218566895,
        "y1": 9.20820140838623,
        "y2": 1.241044282913208
      },
      {
        "r": 0,
        "text": "Therefore, one must ensure rigorous management of individuals' privacy.",
        "trunc_text": "Therefore, one must ensure rigorous management of individuals' privacy.",
        "x1": 4.888869285583496,
        "x2": 6.488540172576904,
        "y1": 9.52468204498291,
        "y2": 0.8664268255233765
      },
      {
        "r": 0,
        "text": "We propose, an advanced data privacy management architecture composed of three layers.",
        "trunc_text": "We propose, an advanced data privacy management architecture composed of three layers.",
        "x1": 4.896130084991455,
        "x2": 6.517459392547607,
        "y1": 9.517876625061035,
        "y2": 0.9237111210823059
      },
      {
        "r": 0,
        "text": "The data management layer consists of de-identification and anonymisation, the access management layer for re-enforcing data access based on the concepts of Role-Based Access Control and the Chinese Wall Security Policy, and the roles layer for regulating different users.",
        "trunc_text": "The data management layer consists of de-identification and anonymisation, the access management layer for re-enforcing ",
        "x1": 4.955007553100586,
        "x2": 6.526695251464844,
        "y1": 9.448453903198242,
        "y2": 1.024115800857544
      },
      {
        "r": 0,
        "text": "The proposed system architecture is validated on healthcare datasets.",
        "trunc_text": "The proposed system architecture is validated on healthcare datasets.",
        "x1": 3.4940502643585205,
        "x2": 4.347841262817383,
        "y1": 7.280410289764404,
        "y2": 2.362067222595215
      },
      {
        "r": 0,
        "text": "Answer selection in open-domain dialogues aims to select an accurate answer from candidates.",
        "trunc_text": "Answer selection in open-domain dialogues aims to select an accurate answer from candidates.",
        "x1": 5.58346700668335,
        "x2": 3.2285706996917725,
        "y1": 3.1566994190216064,
        "y2": 6.646432399749756
      },
      {
        "r": 0,
        "text": "Recent success of answer selection models hinges on training with large amounts of labeled data.",
        "trunc_text": "Recent success of answer selection models hinges on training with large amounts of labeled data.",
        "x1": 4.0985870361328125,
        "x2": 2.818981885910034,
        "y1": 4.13424015045166,
        "y2": 5.942588806152344
      },
      {
        "r": 0,
        "text": "However, collecting large-scale labeled data is labor-intensive and time-consuming.",
        "trunc_text": "However, collecting large-scale labeled data is labor-intensive and time-consuming.",
        "x1": 5.630359649658203,
        "x2": 6.152010440826416,
        "y1": 6.889479637145996,
        "y2": 3.0137031078338623
      },
      {
        "r": 0,
        "text": "In this paper, we introduce the predicted intent labels to calibrate answer labels in a self-training paradigm.",
        "trunc_text": "In this paper, we introduce the predicted intent labels to calibrate answer labels in a self-training paradigm.",
        "x1": 4.04612398147583,
        "x2": 2.832862377166748,
        "y1": 4.341688632965088,
        "y2": 5.889617443084717
      },
      {
        "r": 0,
        "text": "Specifically, we propose the intent-calibrated self-training (ICAST) to improve the quality of pseudo answer labels through the intent-calibrated answer selection paradigm, in which we employ pseudo intent labels to help improve pseudo answer labels.",
        "trunc_text": "Specifically, we propose the intent-calibrated self-training (ICAST) to improve the quality of pseudo answer labels thro",
        "x1": 3.9432756900787354,
        "x2": 2.6933705806732178,
        "y1": 4.272190570831299,
        "y2": 5.950531959533691
      },
      {
        "r": 0,
        "text": "We carry out extensive experiments on two benchmark datasets with open-domain dialogues.",
        "trunc_text": "We carry out extensive experiments on two benchmark datasets with open-domain dialogues.",
        "x1": 5.712102890014648,
        "x2": 3.6466684341430664,
        "y1": 3.124326229095459,
        "y2": 6.945188522338867
      },
      {
        "r": 0,
        "text": "The experimental results show that ICAST outperforms baselines consistently with 1%, 5% and 10% labeled data.",
        "trunc_text": "The experimental results show that ICAST outperforms baselines consistently with 1%, 5% and 10% labeled data.",
        "x1": 3.7208948135375977,
        "x2": 4.8448967933654785,
        "y1": 6.47198486328125,
        "y2": 3.933953046798706
      },
      {
        "r": 0,
        "text": "Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets, compared with the strongest baseline with only 5% labeled data.",
        "trunc_text": "Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets, compared with the strongest baseline with onl",
        "x1": 4.155313968658447,
        "x2": 5.069582939147949,
        "y1": 6.169157981872559,
        "y2": 3.769559383392334
      },
      {
        "r": 0,
        "text": "Indirect surveys, in which respondents provide information about other people they know, have been proposed for scenarios where privacy is important or where the population to be surveyed is hard to reach.",
        "trunc_text": "Indirect surveys, in which respondents provide information about other people they know, have been proposed for scenario",
        "x1": 5.145711421966553,
        "x2": 6.324671745300293,
        "y1": 8.176405906677246,
        "y2": 1.6382615566253662
      },
      {
        "r": 0,
        "text": "As an example, during various stages of the COVID-19 pandemic surveys, including indirect surveys, have been used to estimate the number of cases or the level of vaccination.",
        "trunc_text": "As an example, during various stages of the COVID-19 pandemic surveys, including indirect surveys, have been used to est",
        "x1": 4.8689446449279785,
        "x2": 6.087749004364014,
        "y1": 7.911667346954346,
        "y2": 1.8059782981872559
      },
      {
        "r": 0,
        "text": "The Network Scale-up Method (NSUM) is the classical approach to developing such estimates but was designed with discrete, time-limited indirect surveys in mind.",
        "trunc_text": "The Network Scale-up Method (NSUM) is the classical approach to developing such estimates but was designed with discrete",
        "x1": 4.933719158172607,
        "x2": 6.013417720794678,
        "y1": 7.999961853027344,
        "y2": 1.7668818235397339
      },
      {
        "r": 0,
        "text": "Further, it requires asking for or estimating the number of individuals in each respondent's network.",
        "trunc_text": "Further, it requires asking for or estimating the number of individuals in each respondent's network.",
        "x1": 5.033140182495117,
        "x2": 6.188347816467285,
        "y1": 7.981626510620117,
        "y2": 1.8189507722854614
      },
      {
        "r": 0,
        "text": "In recent years, surveys are being increasingly deployed online and collecting data continuously (e.g., COVID-19 surveys on Facebook during much of the pandemic).",
        "trunc_text": "In recent years, surveys are being increasingly deployed online and collecting data continuously (e.g., COVID-19 surveys",
        "x1": 5.228017807006836,
        "x2": 6.327253818511963,
        "y1": 7.808739185333252,
        "y2": 1.8564770221710205
      },
      {
        "r": 0,
        "text": "Conventional NSUM can be applied to these scenarios by analyzing the data independently during each time interval, but this misses the opportunity of leveraging the temporal dimension.",
        "trunc_text": "Conventional NSUM can be applied to these scenarios by analyzing the data independently during each time interval, but t",
        "x1": 4.6960015296936035,
        "x2": 5.487736225128174,
        "y1": 7.767041206359863,
        "y2": 1.933727741241455
      },
      {
        "r": 0,
        "text": "Understanding the advantage of simply smoothing NSUM results to various degrees is not trivial.",
        "trunc_text": "Understanding the advantage of simply smoothing NSUM results to various degrees is not trivial.",
        "x1": 4.577425956726074,
        "x2": 4.926400184631348,
        "y1": 7.602731227874756,
        "y2": 4.002909183502197
      },
      {
        "r": 0,
        "text": "We propose to use the responses from indirect surveys collected over time and develop analytical tools (i) to prove that indirect surveys can be used to provide better estimates for the size of the hidden population compared to direct surveys, and (ii) to identify appropriate aggregations over time to further improve the estimates.",
        "trunc_text": "We propose to use the responses from indirect surveys collected over time and develop analytical tools (i) to prove that",
        "x1": 4.9142866134643555,
        "x2": 6.097804069519043,
        "y1": 7.9691972732543945,
        "y2": 1.7962960004806519
      },
      {
        "r": 0,
        "text": "We demonstrate through simulations that our approach outperforms traditional NSUM and direct surveying methods to estimate the size of a time-varying hidden population.",
        "trunc_text": "We demonstrate through simulations that our approach outperforms traditional NSUM and direct surveying methods to estima",
        "x1": 4.8002448081970215,
        "x2": 5.943843841552734,
        "y1": 7.931664943695068,
        "y2": 1.7079998254776
      },
      {
        "r": 0,
        "text": "We also demonstrate the superiority of our approach on an existing indirect survey dataset on COVID-19 confirmed cases.",
        "trunc_text": "We also demonstrate the superiority of our approach on an existing indirect survey dataset on COVID-19 confirmed cases.",
        "x1": 4.685732364654541,
        "x2": 5.975025177001953,
        "y1": 7.916776657104492,
        "y2": 1.8526710271835327
      },
      {
        "r": 0,
        "text": "[Context] Systematic Literature Review (SLR) has been a major type of study published in Software Engineering (SE) venues for about two decades.",
        "trunc_text": "[Context] Systematic Literature Review (SLR) has been a major type of study published in Software Engineering (SE) venue",
        "x1": 6.456454277038574,
        "x2": 7.245915412902832,
        "y1": 9.711947441101074,
        "y2": 4.152204513549805
      },
      {
        "r": 0,
        "text": "However, there is a lack of understanding of whether an SLR is really needed in comparison to a more conventional literature review.",
        "trunc_text": "However, there is a lack of understanding of whether an SLR is really needed in comparison to a more conventional litera",
        "x1": 6.462347030639648,
        "x2": 7.308644771575928,
        "y1": 9.720730781555176,
        "y2": 4.173634052276611
      },
      {
        "r": 0,
        "text": "Very often, SE researchers embark on an SLR with such doubts.",
        "trunc_text": "Very often, SE researchers embark on an SLR with such doubts.",
        "x1": 6.483438491821289,
        "x2": 7.311307907104492,
        "y1": 9.736285209655762,
        "y2": 4.181934833526611
      },
      {
        "r": 0,
        "text": "We aspire to provide more understanding of when an SLR in SE should be conducted.",
        "trunc_text": "We aspire to provide more understanding of when an SLR in SE should be conducted.",
        "x1": 6.48012638092041,
        "x2": 7.315329551696777,
        "y1": 9.732754707336426,
        "y2": 4.165106773376465
      },
      {
        "r": 0,
        "text": "[Objective] The first step of our investigation was focused on the dataset, i.e., the reviewed papers, in an SLR, which indicates the development of a research topic or area.",
        "trunc_text": "[Objective] The first step of our investigation was focused on the dataset, i.e., the reviewed papers, in an SLR, which ",
        "x1": 6.457155227661133,
        "x2": 7.258005142211914,
        "y1": 9.69900131225586,
        "y2": 4.09975004196167
      },
      {
        "r": 0,
        "text": "The objective of this step is to provide a better understanding of the characteristics of the datasets of SLRs in SE.",
        "trunc_text": "The objective of this step is to provide a better understanding of the characteristics of the datasets of SLRs in SE.",
        "x1": 6.471905708312988,
        "x2": 7.300695419311523,
        "y1": 9.717402458190918,
        "y2": 4.131035327911377
      },
      {
        "r": 0,
        "text": "[Method] A research synthesis was conducted on a sample of 170 SLRs published in top-tier SE journals.",
        "trunc_text": "[Method] A research synthesis was conducted on a sample of 170 SLRs published in top-tier SE journals.",
        "x1": 6.475981712341309,
        "x2": 7.312010288238525,
        "y1": 9.73451042175293,
        "y2": 4.1556220054626465
      },
      {
        "r": 0,
        "text": "We extracted and analysed the quantitative attributes of the datasets of these SLRs.",
        "trunc_text": "We extracted and analysed the quantitative attributes of the datasets of these SLRs.",
        "x1": 6.458882808685303,
        "x2": 7.28720235824585,
        "y1": 9.704061508178711,
        "y2": 4.100997447967529
      },
      {
        "r": 0,
        "text": "[Results]",
        "trunc_text": "[Results]",
        "x1": 5.4335808753967285,
        "x2": 6.084262847900391,
        "y1": 8.004790306091309,
        "y2": 4.251366138458252
      },
      {
        "r": 0,
        "text": "The findings show that the median size of the datasets in our sample is 57 reviewed papers, and the median review period covered is 14 years.",
        "trunc_text": "The findings show that the median size of the datasets in our sample is 57 reviewed papers, and the median review period",
        "x1": 4.362828254699707,
        "x2": 5.256724834442139,
        "y1": 7.030728816986084,
        "y2": 3.305506944656372
      },
      {
        "r": 0,
        "text": "The number of reviewed papers and review period have a very weak and non-significant positive correlation.",
        "trunc_text": "The number of reviewed papers and review period have a very weak and non-significant positive correlation.",
        "x1": 2.699435234069824,
        "x2": 5.761360168457031,
        "y1": 4.000241756439209,
        "y2": 4.305351734161377
      },
      {
        "r": 0,
        "text": "[Conclusions] The results of our study can be used by SE researchers as an indicator or benchmark to understand whether an SLR is conducted at a good time.",
        "trunc_text": "[Conclusions] The results of our study can be used by SE researchers as an indicator or benchmark to understand whether ",
        "x1": 6.493955612182617,
        "x2": 7.301348686218262,
        "y1": 9.751986503601074,
        "y2": 4.164295196533203
      },
      {
        "r": 0,
        "text": "OCR (Optical Character Recognition) is a technology that offers comprehensive alphanumeric recognition of handwritten and printed characters at electronic speed by merely scanning the document.",
        "trunc_text": "OCR (Optical Character Recognition) is a technology that offers comprehensive alphanumeric recognition of handwritten an",
        "x1": 4.008945941925049,
        "x2": 1.5768104791641235,
        "y1": 1.7597520351409912,
        "y2": 6.930793285369873
      },
      {
        "r": 0,
        "text": "Recently, the understanding of visual data has been termed Intelligent Character Recognition (ICR).",
        "trunc_text": "Recently, the understanding of visual data has been termed Intelligent Character Recognition (ICR).",
        "x1": 3.9449269771575928,
        "x2": 1.6909797191619873,
        "y1": 1.9330657720565796,
        "y2": 6.825443744659424
      },
      {
        "r": 0,
        "text": "Intelligent Character Recognition (ICR) is the OCR module that can convert scans of handwritten or printed characters into ASCII text.",
        "trunc_text": "Intelligent Character Recognition (ICR) is the OCR module that can convert scans of handwritten or printed characters in",
        "x1": 4.0364861488342285,
        "x2": 1.6189757585525513,
        "y1": 1.821751356124878,
        "y2": 6.917917728424072
      },
      {
        "r": 0,
        "text": "ASCII data is the standard format for data encoding in electronic communication.",
        "trunc_text": "ASCII data is the standard format for data encoding in electronic communication.",
        "x1": 6.359284400939941,
        "x2": 7.044748783111572,
        "y1": 6.566605567932129,
        "y2": 3.3951845169067383
      },
      {
        "r": 0,
        "text": "ASCII assigns standard numeric values to letters, numeral, symbols, white-spaces and other characters.",
        "trunc_text": "ASCII assigns standard numeric values to letters, numeral, symbols, white-spaces and other characters.",
        "x1": 6.416224956512451,
        "x2": 7.07073450088501,
        "y1": 6.495599269866943,
        "y2": 3.419588804244995
      },
      {
        "r": 0,
        "text": "In more technical terms, OCR is the process of using an electronic device to transform 2-Dimensional textual information into machine-encoded text.",
        "trunc_text": "In more technical terms, OCR is the process of using an electronic device to transform 2-Dimensional textual information",
        "x1": 4.011364936828613,
        "x2": 1.601036548614502,
        "y1": 1.7632982730865479,
        "y2": 6.914795398712158
      },
      {
        "r": 0,
        "text": "Anything that contains text both machine written or handwritten can be scanned either through a scanner or just simply a picture of the text is enough for the recognition system to distinguish the text.",
        "trunc_text": "Anything that contains text both machine written or handwritten can be scanned either through a scanner or just simply a",
        "x1": 3.9917733669281006,
        "x2": 1.5988343954086304,
        "y1": 1.7695978879928589,
        "y2": 6.9875006675720215
      },
      {
        "r": 0,
        "text": "The goal of this papers is to show the results of a Convolutional Neural Network model which has been trained on National Institute of Science and Technology (NIST) dataset containing over a 100,000 images.",
        "trunc_text": "The goal of this papers is to show the results of a Convolutional Neural Network model which has been trained on Nationa",
        "x1": 2.106227397918701,
        "x2": 2.919848680496216,
        "y1": 6.493239879608154,
        "y2": 2.5475950241088867
      },
      {
        "r": 0,
        "text": "The network learns from the features extracted from the images and use it to generate the probability of each class to which the picture belongs to.",
        "trunc_text": "The network learns from the features extracted from the images and use it to generate the probability of each class to w",
        "x1": 1.967919945716858,
        "x2": 2.74006724357605,
        "y1": 6.558952808380127,
        "y2": 2.6243226528167725
      },
      {
        "r": 0,
        "text": "We have achieved an accuracy of 90.54% with a loss of 2.53%.",
        "trunc_text": "We have achieved an accuracy of 90.54% with a loss of 2.53%.",
        "x1": 4.278424263000488,
        "x2": 5.329747676849365,
        "y1": 6.017181396484375,
        "y2": 4.190975666046143
      },
      {
        "r": 0,
        "text": "We compare the performance of three nearest neighbor search algorithms: the Orchard, ball tree, and VP-tree algorithms.",
        "trunc_text": "We compare the performance of three nearest neighbor search algorithms: the Orchard, ball tree, and VP-tree algorithms.",
        "x1": 3.8235690593719482,
        "x2": 4.77971076965332,
        "y1": 7.083062648773193,
        "y2": 3.6332132816314697
      },
      {
        "r": 0,
        "text": "These algorithms are commonly used for nearest-neighbor searches and are known for their efficiency in large datasets.",
        "trunc_text": "These algorithms are commonly used for nearest-neighbor searches and are known for their efficiency in large datasets.",
        "x1": 3.7902579307556152,
        "x2": 4.778179168701172,
        "y1": 7.00571346282959,
        "y2": 3.8374814987182617
      },
      {
        "r": 0,
        "text": "We analyze the fraction of distances computed in relation to the size of the dataset and its dimension.",
        "trunc_text": "We analyze the fraction of distances computed in relation to the size of the dataset and its dimension.",
        "x1": 3.945215940475464,
        "x2": 4.936413764953613,
        "y1": 6.638861179351807,
        "y2": 3.491359233856201
      },
      {
        "r": 0,
        "text": "For each algorithm we derive a fitting function for the efficiency as a function to set size and dimension.",
        "trunc_text": "For each algorithm we derive a fitting function for the efficiency as a function to set size and dimension.",
        "x1": 3.826186418533325,
        "x2": 4.74873685836792,
        "y1": 6.853079319000244,
        "y2": 3.9640398025512695
      },
      {
        "r": 0,
        "text": "The article aims to provide a comprehensive analysis of the performance of these algorithms and help researchers and practitioners choose the best algorithm for their specific application.",
        "trunc_text": "The article aims to provide a comprehensive analysis of the performance of these algorithms and help researchers and pra",
        "x1": 3.8649702072143555,
        "x2": 4.83063268661499,
        "y1": 6.957653522491455,
        "y2": 3.96401309967041
      },
      {
        "r": 0,
        "text": "In order to broaden the application and research of anomaly detection in unmanned supermarkets and smart manufacturing, we introduce the supermarket goods anomaly detection (GoodsAD) dataset",
        "trunc_text": "In order to broaden the application and research of anomaly detection in unmanned supermarkets and smart manufacturing, ",
        "x1": 1.4330435991287231,
        "x2": 2.3388707637786865,
        "y1": 6.479462623596191,
        "y2": 2.1545708179473877
      },
      {
        "r": 0,
        "text": "It contains 6124 high-resolution images of 484 different appearance goods divided into 6 categories",
        "trunc_text": "It contains 6124 high-resolution images of 484 different appearance goods divided into 6 categories",
        "x1": 1.667676329612732,
        "x2": 4.3080153465271,
        "y1": 7.948269844055176,
        "y2": 1.2856358289718628
      },
      {
        "r": 0,
        "text": "This is a comprehensive, multi-object dataset for supermarket goods anomaly detection that focuses on real-world applications.",
        "trunc_text": "This is a comprehensive, multi-object dataset for supermarket goods anomaly detection that focuses on real-world applica",
        "x1": 1.3751697540283203,
        "x2": 2.325876235961914,
        "y1": 6.53418493270874,
        "y2": 2.104936361312866
      },
      {
        "r": 0,
        "text": "Visual anomaly detection is essential and commonly used for many tasks in the field of computer vision.",
        "trunc_text": "Visual anomaly detection is essential and commonly used for many tasks in the field of computer vision.",
        "x1": 1.3196845054626465,
        "x2": 2.2356979846954346,
        "y1": 6.501821041107178,
        "y2": 2.1390445232391357
      },
      {
        "r": 0,
        "text": "Recent anomaly detection datasets mainly focus on industrial automated inspection, medical image analysis and video surveillance. .",
        "trunc_text": "Recent anomaly detection datasets mainly focus on industrial automated inspection, medical image analysis and video surv",
        "x1": 1.4486331939697266,
        "x2": 2.3589119911193848,
        "y1": 6.485287189483643,
        "y2": 2.1914241313934326
      },
      {
        "r": 0,
        "text": "It contains 6124 high-resolution images of 484 different appearance goods divided into 6 categories.",
        "trunc_text": "It contains 6124 high-resolution images of 484 different appearance goods divided into 6 categories.",
        "x1": 1.6618499755859375,
        "x2": 4.295076847076416,
        "y1": 7.881518363952637,
        "y2": 1.3453800678253174
      },
      {
        "r": 0,
        "text": "Each category contains several common different types of anomalies such as deformation, s the unsupervised setting and only normal (defect-free) images are used for training.",
        "trunc_text": "Each category contains several common different types of anomalies such as deformation, s the unsupervised setting and o",
        "x1": 1.42159903049469,
        "x2": 2.253920078277588,
        "y1": 6.4062886238098145,
        "y2": 2.2598824501037598
      },
      {
        "r": 0,
        "text": "Pixel-precise ground truth regions are provided for all anomalies.",
        "trunc_text": "Pixel-precise ground truth regions are provided for all anomalies.",
        "x1": 1.3488718271255493,
        "x2": 2.051326274871826,
        "y1": 6.2365875244140625,
        "y2": 2.334322929382324
      },
      {
        "r": 0,
        "text": "Moreover, we also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods.",
        "trunc_text": "Moreover, we also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods.",
        "x1": 1.430751919746399,
        "x2": 2.3425989151000977,
        "y1": 6.487494468688965,
        "y2": 2.2006561756134033
      },
      {
        "r": 0,
        "text": "This initial benchmark indicates that some methods which perform well on the industrial anomaly detection dataset (e.g., MVTec AD), show poor performance on our dataset.",
        "trunc_text": "This initial benchmark indicates that some methods which perform well on the industrial anomaly detection dataset (e.g.,",
        "x1": 1.4862526655197144,
        "x2": 2.568136692047119,
        "y1": 6.515384674072266,
        "y2": 2.3778154850006104
      },
      {
        "r": 0,
        "text": "We also made available all the synthetic data generated in this work for the 18 different datasets in the BEIR benchmark which took more than 2,000 GPU hours to be generated as well as the reranker models finetuned on the synthetic data.",
        "trunc_text": "We also made available all the synthetic data generated in this work for the 18 different datasets in the BEIR benchmark",
        "x1": 5.203125,
        "x2": 5.23906946182251,
        "y1": 6.973637104034424,
        "y2": 2.2574687004089355
      },
      {
        "r": 0,
        "text": "Recent work has explored Large Language Models (LLMs) to overcome the lack of training data for Information Retrieval (IR) tasks.",
        "trunc_text": "Recent work has explored Large Language Models (LLMs) to overcome the lack of training data for Information Retrieval (I",
        "x1": 5.0334296226501465,
        "x2": 3.5312793254852295,
        "y1": 3.794668197631836,
        "y2": 6.196717262268066
      },
      {
        "r": 0,
        "text": "The generalization abilities of these models have enabled the creation of synthetic in-domain data by providing instructions and a few examples on a prompt.",
        "trunc_text": "The generalization abilities of these models have enabled the creation of synthetic in-domain data by providing instruct",
        "x1": 5.174739360809326,
        "x2": 5.3228583335876465,
        "y1": 6.813791275024414,
        "y2": 2.303558349609375
      },
      {
        "r": 0,
        "text": "InPars and Promptagator have pioneered this approach and both methods have demonstrated the potential of using LLMs as synthetic data generators for IR tasks.",
        "trunc_text": "InPars and Promptagator have pioneered this approach and both methods have demonstrated the potential of using LLMs as s",
        "x1": 6.106087684631348,
        "x2": 4.8295063972473145,
        "y1": 4.675673484802246,
        "y2": 6.2362775802612305
      },
      {
        "r": 0,
        "text": "This makes them an attractive solution for IR tasks that suffer from a lack of annotated data.",
        "trunc_text": "This makes them an attractive solution for IR tasks that suffer from a lack of annotated data.",
        "x1": 2.4703164100646973,
        "x2": 1.4491021633148193,
        "y1": 3.5817482471466064,
        "y2": 5.1884331703186035
      },
      {
        "r": 0,
        "text": "However, the reproducibility of these methods was limited, because InPars' training scripts are based on TPUs -- which are not widely accessible -- and because the code for Promptagator was not released and its proprietary LLM is not publicly accessible.",
        "trunc_text": "However, the reproducibility of these methods was limited, because InPars' training scripts are based on TPUs -- which a",
        "x1": 6.114365100860596,
        "x2": 4.838885307312012,
        "y1": 4.774125576019287,
        "y2": 6.168070316314697
      },
      {
        "r": 0,
        "text": "To fully realize the potential of these methods and make their impact more widespread in the research community, the resources need to be accessible and easy to reproduce by researchers and practitioners.",
        "trunc_text": "To fully realize the potential of these methods and make their impact more widespread in the research community, the res",
        "x1": 5.67893123626709,
        "x2": 6.435826301574707,
        "y1": 8.045398712158203,
        "y2": 4.317510604858398
      },
      {
        "r": 0,
        "text": "Our main contribution is a unified toolkit for end-to-end reproducible synthetic data generation research, which includes generation, filtering, training and evaluation.",
        "trunc_text": "Our main contribution is a unified toolkit for end-to-end reproducible synthetic data generation research, which include",
        "x1": 5.145767688751221,
        "x2": 5.279255390167236,
        "y1": 6.879846096038818,
        "y2": 2.2473063468933105
      },
      {
        "r": 0,
        "text": "Additionally, we provide an interface to IR libraries widely used by the community and support for GPU.",
        "trunc_text": "Additionally, we provide an interface to IR libraries widely used by the community and support for GPU.",
        "x1": 7.605177879333496,
        "x2": 7.816136360168457,
        "y1": 6.467250347137451,
        "y2": 3.1950185298919678
      },
      {
        "r": 0,
        "text": "Our toolkit not only reproduces the InPars method and partially reproduces Promptagator, but also provides a plug-and-play functionality allowing the use of different LLMs, exploring filtering methods and finetuning various reranker models on the generated data.  ",
        "trunc_text": "Our toolkit not only reproduces the InPars method and partially reproduces Promptagator, but also provides a plug-and-pl",
        "x1": 6.096353054046631,
        "x2": 4.818714618682861,
        "y1": 4.775050163269043,
        "y2": 6.20233154296875
      },
      {
        "r": 0,
        "text": "Code and data are available at https://github.com/zetaalphavector/InPars",
        "trunc_text": "Code and data are available at https://github.com/zetaalphavector/InPars",
        "x1": 7.426562309265137,
        "x2": 7.9413886070251465,
        "y1": 7.9158430099487305,
        "y2": 1.9519768953323364
      },
      {
        "r": 0,
        "text": "Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations.",
        "trunc_text": "Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations.",
        "x1": 7.236896991729736,
        "x2": 6.499499320983887,
        "y1": 5.560149192810059,
        "y2": 5.828525066375732
      },
      {
        "r": 0,
        "text": "Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence.",
        "trunc_text": "Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial ",
        "x1": 7.341246604919434,
        "x2": 6.58208703994751,
        "y1": 5.528634548187256,
        "y2": 5.8885626792907715
      },
      {
        "r": 0,
        "text": "Recent studies have suggested that score-based diffusion models are effective in adversarial defenses.",
        "trunc_text": "Recent studies have suggested that score-based diffusion models are effective in adversarial defenses.",
        "x1": 7.238950729370117,
        "x2": 6.500539302825928,
        "y1": 5.558024883270264,
        "y2": 5.840810298919678
      },
      {
        "r": 0,
        "text": "However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results.",
        "trunc_text": "However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equ",
        "x1": 2.4426705837249756,
        "x2": 3.3324615955352783,
        "y1": 6.404407024383545,
        "y2": 2.3313510417938232
      },
      {
        "r": 0,
        "text": "In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors.",
        "trunc_text": "In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at te",
        "x1": 7.233596324920654,
        "x2": 6.469668388366699,
        "y1": 5.552353382110596,
        "y2": 5.817280292510986
      },
      {
        "r": 0,
        "text": "Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.",
        "trunc_text": "Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robust",
        "x1": 7.263512134552002,
        "x2": 6.513138294219971,
        "y1": 5.555522441864014,
        "y2": 5.847731113433838
      },
      {
        "r": 0,
        "text": "To overcome these limitations, we build a new challenging benchmark named KoRc in this paper",
        "trunc_text": "To overcome these limitations, we build a new challenging benchmark named KoRc in this paper",
        "x1": 4.595799922943115,
        "x2": 4.551711559295654,
        "y1": 6.459461688995361,
        "y2": 3.861602544784546
      },
      {
        "r": 0,
        "text": "Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years.",
        "trunc_text": "Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, ha",
        "x1": 3.780433416366577,
        "x2": 2.9667160511016846,
        "y1": 3.9597229957580566,
        "y2": 5.2824482917785645
      },
      {
        "r": 0,
        "text": "However, these benchmarks have encountered two major limitations.",
        "trunc_text": "However, these benchmarks have encountered two major limitations.",
        "x1": 4.817590713500977,
        "x2": 4.958253860473633,
        "y1": 6.337972164154053,
        "y2": 4.067714214324951
      },
      {
        "r": 0,
        "text": "On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage.",
        "trunc_text": "On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage.",
        "x1": 3.288511276245117,
        "x2": 1.8467905521392822,
        "y1": 4.156961917877197,
        "y2": 4.95490026473999
      },
      {
        "r": 0,
        "text": "On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. .",
        "trunc_text": "On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. ",
        "x1": 3.5871336460113525,
        "x2": 2.3796236515045166,
        "y1": 3.7859935760498047,
        "y2": 5.772853374481201
      },
      {
        "r": 0,
        "text": "Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format.",
        "trunc_text": "Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format.",
        "x1": 4.679412364959717,
        "x2": 4.527580738067627,
        "y1": 6.330556392669678,
        "y2": 3.8802413940429688
      },
      {
        "r": 0,
        "text": "Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowledgable questions.",
        "trunc_text": "Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowle",
        "x1": 5.194815635681152,
        "x2": 3.764937400817871,
        "y1": 4.000965118408203,
        "y2": 6.103297233581543
      },
      {
        "r": 0,
        "text": "Moreover, we use labels in knowledge bases rather than spans or choices as the final answers.",
        "trunc_text": "Moreover, we use labels in knowledge bases rather than spans or choices as the final answers.",
        "x1": 3.4440836906433105,
        "x2": 2.4005463123321533,
        "y1": 4.176704406738281,
        "y2": 5.530219078063965
      },
      {
        "r": 0,
        "text": "We test state-of-the-art models on KoRC and the experimental results show that the strongest baseline only achieves 68.3% and 30.0% F1 measure in the in-distribution and out-of-distribution test set, respectively.",
        "trunc_text": "We test state-of-the-art models on KoRC and the experimental results show that the strongest baseline only achieves 68.3",
        "x1": 4.467154026031494,
        "x2": 4.617245674133301,
        "y1": 6.395841121673584,
        "y2": 3.876392126083374
      },
      {
        "r": 0,
        "text": "These results indicate that deep text understanding is still an unsolved challenge.",
        "trunc_text": "These results indicate that deep text understanding is still an unsolved challenge.",
        "x1": 3.732274055480957,
        "x2": 2.9280757904052734,
        "y1": 4.063935279846191,
        "y2": 5.239592552185059
      },
      {
        "r": 0,
        "text": "The benchmark dataset, leaderboard, and baseline methods are released in https://github.com/",
        "trunc_text": "The benchmark dataset, leaderboard, and baseline methods are released in https://github.com/",
        "x1": 4.673472881317139,
        "x2": 4.490618705749512,
        "y1": 6.45912504196167,
        "y2": 3.595843553543091
      },
      {
        "r": 0,
        "text": "Schema discovery is an important aspect to working with data in formats such as JSON.",
        "trunc_text": "Schema discovery is an important aspect to working with data in formats such as JSON.",
        "x1": 6.012623310089111,
        "x2": 6.623881816864014,
        "y1": 6.659794330596924,
        "y2": 3.169990062713623
      },
      {
        "r": 0,
        "text": "Unlike relational databases, JSON data sets often do not have associated structural information.",
        "trunc_text": "Unlike relational databases, JSON data sets often do not have associated structural information.",
        "x1": 5.973067283630371,
        "x2": 6.612128734588623,
        "y1": 6.741600036621094,
        "y2": 3.1250386238098145
      },
      {
        "r": 0,
        "text": "Consumers of such datasets are often left to browse through data in an attempt to observe commonalities in structure across documents to construct suitable code for data processing.",
        "trunc_text": "Consumers of such datasets are often left to browse through data in an attempt to observe commonalities in structure acr",
        "x1": 5.9954752922058105,
        "x2": 6.603752613067627,
        "y1": 6.946518898010254,
        "y2": 3.011955976486206
      },
      {
        "r": 0,
        "text": "However, this process is time-consuming and error-prone.",
        "trunc_text": "However, this process is time-consuming and error-prone.",
        "x1": 5.497608661651611,
        "x2": 6.144177436828613,
        "y1": 6.895326137542725,
        "y2": 3.0167829990386963
      },
      {
        "r": 0,
        "text": "Existing distributed approaches to mining schemas present a significant usability advantage as they provide useful metadata for large data sources.",
        "trunc_text": "Existing distributed approaches to mining schemas present a significant usability advantage as they provide useful metad",
        "x1": 5.9758453369140625,
        "x2": 6.6161298751831055,
        "y1": 6.747457981109619,
        "y2": 3.1105308532714844
      },
      {
        "r": 0,
        "text": "However, depending on the data source, ad hoc queries for estimating other properties to help with crafting an efficient data pipeline can be expensive.",
        "trunc_text": "However, depending on the data source, ad hoc queries for estimating other properties to help with crafting an efficient",
        "x1": 5.774891376495361,
        "x2": 6.344812393188477,
        "y1": 6.818273067474365,
        "y2": 3.0388433933258057
      },
      {
        "r": 0,
        "text": "We propose JSONoid, a distributed schema discovery process augmented with additional metadata in the form of monoid data structures that are easily maintainable in a distributed setting.",
        "trunc_text": "We propose JSONoid, a distributed schema discovery process augmented with additional metadata in the form of monoid data",
        "x1": 6.02682638168335,
        "x2": 6.733677387237549,
        "y1": 6.837681770324707,
        "y2": 3.02540922164917
      },
      {
        "r": 0,
        "text": "JSONoid subsumes several existing approaches to distributed schema discovery with similar performance.",
        "trunc_text": "JSONoid subsumes several existing approaches to distributed schema discovery with similar performance.",
        "x1": 6.073534965515137,
        "x2": 6.722373008728027,
        "y1": 6.776005744934082,
        "y2": 3.0644500255584717
      },
      {
        "r": 0,
        "text": "Our approach also adds significant useful additional information about data values to discovered schemas with linear scalability.",
        "trunc_text": "Our approach also adds significant useful additional information about data values to discovered schemas with linear sca",
        "x1": 5.867484092712402,
        "x2": 6.455984592437744,
        "y1": 6.722425937652588,
        "y2": 3.2135632038116455
      },
      {
        "r": 0,
        "text": "Scientific publications follow conventionalized rhetorical structures.",
        "trunc_text": "Scientific publications follow conventionalized rhetorical structures.",
        "x1": 2.838392972946167,
        "x2": 1.751272439956665,
        "y1": 3.5306901931762695,
        "y2": 5.441390514373779
      },
      {
        "r": 0,
        "text": "Classifying the Argumentative Zone (AZ), e.g., identifying whether a sentence states a Motivation, a Result or Background information, has been proposed to improve processing of scholarly documents.",
        "trunc_text": "Classifying the Argumentative Zone (AZ), e.g., identifying whether a sentence states a Motivation, a Result or Backgroun",
        "x1": 2.8331568241119385,
        "x2": 1.8367528915405273,
        "y1": 3.516890525817871,
        "y2": 5.522096633911133
      },
      {
        "r": 0,
        "text": "In this work, we adapt and extend this idea to the domain of materials science research.  ",
        "trunc_text": "In this work, we adapt and extend this idea to the domain of materials science research.  ",
        "x1": 5.277031898498535,
        "x2": 6.392264366149902,
        "y1": 8.485898971557617,
        "y2": 4.523048400878906
      },
      {
        "r": 0,
        "text": "We detail corpus statistics and demonstrate high inter-annotator agreement.",
        "trunc_text": "We detail corpus statistics and demonstrate high inter-annotator agreement.",
        "x1": 2.8275346755981445,
        "x2": 1.6450159549713135,
        "y1": 3.3941702842712402,
        "y2": 5.595704555511475
      },
      {
        "r": 0,
        "text": "Our computational experiments show that using domain-specific pre-trained transformer-based text encoders is key to high classification performance.",
        "trunc_text": "Our computational experiments show that using domain-specific pre-trained transformer-based text encoders is key to high",
        "x1": 4.0954179763793945,
        "x2": 3.4131321907043457,
        "y1": 4.593390464782715,
        "y2": 5.017570495605469
      },
      {
        "r": 0,
        "text": "We also find that AZ categories from existing datasets in other domains are transferable to varying degrees.",
        "trunc_text": "We also find that AZ categories from existing datasets in other domains are transferable to varying degrees.",
        "x1": 3.069837808609009,
        "x2": 4.594460487365723,
        "y1": 5.892373561859131,
        "y2": 2.8519527912139893
      },
      {
        "r": 0,
        "text": "One of the key problems in 3D object detection is to reduce the accuracy gap between methods based on LiDAR sensors and those based on monocular cameras.",
        "trunc_text": "One of the key problems in 3D object detection is to reduce the accuracy gap between methods based on LiDAR sensors and ",
        "x1": 1.0339882373809814,
        "x2": 2.868971586227417,
        "y1": 9.163411140441895,
        "y2": 0.006439009681344032
      },
      {
        "r": 0,
        "text": "A recently proposed framework for monocular 3D detection based on Pseudo-Stereo has received considerable attention in the community.",
        "trunc_text": "A recently proposed framework for monocular 3D detection based on Pseudo-Stereo has received considerable attention in t",
        "x1": 0.9977653622627258,
        "x2": 2.797513246536255,
        "y1": 8.980530738830566,
        "y2": 0.07164131104946136
      },
      {
        "r": 0,
        "text": "However, so far these two problems are discovered in existing practices, including (1) monocular depth estimation and Pseudo-Stereo detector must be trained separately, (2) Difficult to be compatible with different stereo detectors and (3) the overall calculation is large, which affects the reasoning speed.",
        "trunc_text": "However, so far these two problems are discovered in existing practices, including (1) monocular depth estimation and Ps",
        "x1": 0.9297178387641907,
        "x2": 2.761624336242676,
        "y1": 8.973413467407227,
        "y2": 0.10321130603551865
      },
      {
        "r": 0,
        "text": "In this work, we propose an end-to-end, efficient pseudo-stereo 3D detection framework by introducing a Single-View Diffusion Model (SVDM) that uses a few iterations to gradually deliver right informative pixels to the left image.",
        "trunc_text": "In this work, we propose an end-to-end, efficient pseudo-stereo 3D detection framework by introducing a Single-View Diff",
        "x1": 1.0894535779953003,
        "x2": 2.81851863861084,
        "y1": 8.957168579101562,
        "y2": 0.1670820266008377
      },
      {
        "r": 0,
        "text": "SVDM allows the entire pseudo-stereo 3D detection pipeline to be trained end-to-end and can benefit from the training of stereo detectors.",
        "trunc_text": "SVDM allows the entire pseudo-stereo 3D detection pipeline to be trained end-to-end and can benefit from the training of",
        "x1": 1.0258269309997559,
        "x2": 2.8248496055603027,
        "y1": 9.014002799987793,
        "y2": 0.11772359162569046
      },
      {
        "r": 0,
        "text": "Afterwards, we further explore the application of SVDM in depth-free stereo 3D detection, and the final framework is compatible with most stereo detectors.",
        "trunc_text": "Afterwards, we further explore the application of SVDM in depth-free stereo 3D detection, and the final framework is com",
        "x1": 0.9596092700958252,
        "x2": 2.8096516132354736,
        "y1": 8.993107795715332,
        "y2": 0.12085392326116562
      },
      {
        "r": 0,
        "text": "We consider the problem of learning a sparse graph underlying an undirected Gaussian graphical model, a key problem in statistical machine learning.",
        "trunc_text": "We consider the problem of learning a sparse graph underlying an undirected Gaussian graphical model, a key problem in s",
        "x1": 2.9645252227783203,
        "x2": 3.113345146179199,
        "y1": 5.3186774253845215,
        "y2": 3.705221652984619
      },
      {
        "r": 0,
        "text": "Given $n$ samples from a multivariate Gaussian distribution with $p$ variables, the goal is to estimate the $p \\times p$ inverse covariance matrix (aka precision matrix), assuming it is sparse (i.e., has a few nonzero entries).",
        "trunc_text": "Given $n$ samples from a multivariate Gaussian distribution with $p$ variables, the goal is to estimate the $p \\times p$",
        "x1": 3.099957227706909,
        "x2": 3.6055655479431152,
        "y1": 5.794504165649414,
        "y2": 3.619044303894043
      },
      {
        "r": 0,
        "text": "We propose GraphL0BnB, a new estimator based on an $\\ell_0$-penalized version of the pseudolikelihood function, while most earlier approaches are based on the $\\ell_1$-relaxation.",
        "trunc_text": "We propose GraphL0BnB, a new estimator based on an $\\ell_0$-penalized version of the pseudolikelihood function, while mo",
        "x1": 2.574071168899536,
        "x2": 3.394120931625366,
        "y1": 5.205403804779053,
        "y2": 3.8974545001983643
      },
      {
        "r": 0,
        "text": "Our estimator can be formulated as a convex mixed integer program (MIP) which can be difficult to compute at scale using off-the-shelf commercial solvers.",
        "trunc_text": "Our estimator can be formulated as a convex mixed integer program (MIP) which can be difficult to compute at scale using",
        "x1": 3.398204803466797,
        "x2": 3.7226462364196777,
        "y1": 5.655627250671387,
        "y2": 3.993436336517334
      },
      {
        "r": 0,
        "text": "To solve the MIP, we propose a custom nonlinear branch-and-bound (BnB) framework that solves node relaxations with tailored first-order methods.",
        "trunc_text": "To solve the MIP, we propose a custom nonlinear branch-and-bound (BnB) framework that solves node relaxations with tailo",
        "x1": 4.011876583099365,
        "x2": 4.878799915313721,
        "y1": 5.5461859703063965,
        "y2": 4.58335018157959
      },
      {
        "r": 0,
        "text": "As a by-product of our BnB framework, we propose large-scale solvers for obtaining good primal solutions that are of independent interest.",
        "trunc_text": "As a by-product of our BnB framework, we propose large-scale solvers for obtaining good primal solutions that are of ind",
        "x1": 4.517521381378174,
        "x2": 4.970724105834961,
        "y1": 5.677140712738037,
        "y2": 4.627255916595459
      },
      {
        "r": 0,
        "text": "We derive novel statistical guarantees (estimation and variable selection) for our estimator and discuss how our approach improves upon existing estimators.",
        "trunc_text": "We derive novel statistical guarantees (estimation and variable selection) for our estimator and discuss how our approac",
        "x1": 3.38161039352417,
        "x2": 3.8397719860076904,
        "y1": 5.746992111206055,
        "y2": 3.90212345123291
      },
      {
        "r": 0,
        "text": "Our numerical experiments on real/synthetic datasets suggest that our method can solve, to near-optimality, problem instances with $p = 10^4$ -- corresponding to a symmetric matrix of size $p \\times p$ with $p^2/2$ binary variables.",
        "trunc_text": "Our numerical experiments on real/synthetic datasets suggest that our method can solve, to near-optimality, problem inst",
        "x1": 3.959327220916748,
        "x2": 4.658352375030518,
        "y1": 6.9629597663879395,
        "y2": 2.915114402770996
      },
      {
        "r": 0,
        "text": "We demonstrate the usefulness of GraphL0BnB versus various state-of-the-art approaches on a range of datasets.",
        "trunc_text": "We demonstrate the usefulness of GraphL0BnB versus various state-of-the-art approaches on a range of datasets.",
        "x1": 3.227881908416748,
        "x2": 3.414496660232544,
        "y1": 5.501194000244141,
        "y2": 3.382833242416382
      },
      {
        "r": 0,
        "text": "Trajectory data collection is a common task with many applications in our daily lives.",
        "trunc_text": "Trajectory data collection is a common task with many applications in our daily lives.",
        "x1": 3.9430477619171143,
        "x2": 5.393984317779541,
        "y1": 9.725319862365723,
        "y2": -0.24420355260372162
      },
      {
        "r": 0,
        "text": "Analyzing trajectory data enables service providers to enhance their services, which ultimately benefits users.",
        "trunc_text": "Analyzing trajectory data enables service providers to enhance their services, which ultimately benefits users.",
        "x1": 4.396032810211182,
        "x2": 6.009946346282959,
        "y1": 9.708955764770508,
        "y2": 0.2150852233171463
      },
      {
        "r": 0,
        "text": "However, directly collecting trajectory data may give rise to privacy-related issues that cannot be ignored.",
        "trunc_text": "However, directly collecting trajectory data may give rise to privacy-related issues that cannot be ignored.",
        "x1": 4.736145973205566,
        "x2": 6.4157490730285645,
        "y1": 9.655503273010254,
        "y2": 0.6352332830429077
      },
      {
        "r": 0,
        "text": "Local differential privacy (LDP), as the de facto privacy protection standard in a decentralized setting, enables users to perturb their trajectories locally and provides a provable privacy guarantee.",
        "trunc_text": "Local differential privacy (LDP), as the de facto privacy protection standard in a decentralized setting, enables users ",
        "x1": 4.704958438873291,
        "x2": 6.4024152755737305,
        "y1": 9.699514389038086,
        "y2": 0.5854421257972717
      },
      {
        "r": 0,
        "text": "Existing approaches to private trajectory data collection in a local setting typically use relaxed versions of LDP, which cannot provide a strict privacy guarantee, or require some external knowledge that is impractical to obtain and update in a timely manner.",
        "trunc_text": "Existing approaches to private trajectory data collection in a local setting typically use relaxed versions of LDP, whic",
        "x1": 4.681116580963135,
        "x2": 6.384681224822998,
        "y1": 9.730813980102539,
        "y2": 0.5363422632217407
      },
      {
        "r": 0,
        "text": "To tackle these problems, we propose a novel trajectory perturbation mechanism that relies solely on an underlying location set and satisfies pure $\\epsilon$-LDP to provide a stringent privacy guarantee.",
        "trunc_text": "To tackle these problems, we propose a novel trajectory perturbation mechanism that relies solely on an underlying locat",
        "x1": 4.6575140953063965,
        "x2": 6.39457893371582,
        "y1": 9.75389575958252,
        "y2": 0.5472049713134766
      },
      {
        "r": 0,
        "text": "In the proposed mechanism, each point's adjacent direction information in the trajectory is used in its perturbation process.",
        "trunc_text": "In the proposed mechanism, each point's adjacent direction information in the trajectory is used in its perturbation pro",
        "x1": 4.472811222076416,
        "x2": 6.136075973510742,
        "y1": 9.806258201599121,
        "y2": 0.3329099118709564
      },
      {
        "r": 0,
        "text": "Such information serves as an effective clue to connect neighboring points and can be used to restrict the possible region of a perturbed point in order to enhance utility.",
        "trunc_text": "Such information serves as an effective clue to connect neighboring points and can be used to restrict the possible regi",
        "x1": 4.474182605743408,
        "x2": 6.243739128112793,
        "y1": 9.765117645263672,
        "y2": 0.45729315280914307
      },
      {
        "r": 0,
        "text": "To the best of our knowledge, our study is the first to use direction information for trajectory perturbation under LDP.",
        "trunc_text": "To the best of our knowledge, our study is the first to use direction information for trajectory perturbation under LDP.",
        "x1": 4.518655776977539,
        "x2": 6.253419399261475,
        "y1": 9.77026081085205,
        "y2": 0.39330676198005676
      },
      {
        "r": 0,
        "text": "Furthermore, based on this mechanism, we present an anchor-based method that adaptively restricts the region of each perturbed trajectory, thereby significantly boosting performance without violating the privacy constraint.",
        "trunc_text": "Furthermore, based on this mechanism, we present an anchor-based method that adaptively restricts the region of each per",
        "x1": 4.665691375732422,
        "x2": 6.381566524505615,
        "y1": 9.735980033874512,
        "y2": 0.5270758271217346
      },
      {
        "r": 0,
        "text": "Extensive experiments on both real-world and synthetic datasets demonstrate the effectiveness of the proposed mechanisms.",
        "trunc_text": "Extensive experiments on both real-world and synthetic datasets demonstrate the effectiveness of the proposed mechanisms",
        "x1": 4.597856044769287,
        "x2": 5.172609806060791,
        "y1": 7.257658004760742,
        "y2": 2.446000099182129
      },
      {
        "r": 0,
        "text": "In safety-critical classification tasks, conformal prediction allows to perform rigorous uncertainty quantification by providing confidence sets including the true class with a user-specified probability.",
        "trunc_text": "In safety-critical classification tasks, conformal prediction allows to perform rigorous uncertainty quantification by p",
        "x1": 1.9827181100845337,
        "x2": 1.4990472793579102,
        "y1": 4.593800067901611,
        "y2": 4.0358428955078125
      },
      {
        "r": 0,
        "text": "This generally assumes the availability of a held-out calibration set with access to ground truth labels.",
        "trunc_text": "This generally assumes the availability of a held-out calibration set with access to ground truth labels.",
        "x1": 1.7562274932861328,
        "x2": 1.3311827182769775,
        "y1": 4.37632417678833,
        "y2": 4.143826007843018
      },
      {
        "r": 0,
        "text": "Unfortunately, in many domains, such labels are difficult to obtain and usually approximated by aggregating expert opinions.",
        "trunc_text": "Unfortunately, in many domains, such labels are difficult to obtain and usually approximated by aggregating expert opini",
        "x1": 2.2923178672790527,
        "x2": 1.4138460159301758,
        "y1": 3.9552578926086426,
        "y2": 4.754079341888428
      },
      {
        "r": 0,
        "text": "In fact, this holds true for almost all datasets, including well-known ones such as CIFAR and ImageNet.",
        "trunc_text": "In fact, this holds true for almost all datasets, including well-known ones such as CIFAR and ImageNet.",
        "x1": 2.8443503379821777,
        "x2": 4.594817638397217,
        "y1": 7.028239727020264,
        "y2": 2.4658219814300537
      },
      {
        "r": 0,
        "text": "Applying conformal prediction using such labels underestimates uncertainty.",
        "trunc_text": "Applying conformal prediction using such labels underestimates uncertainty.",
        "x1": 1.7417188882827759,
        "x2": 1.3174055814743042,
        "y1": 4.539830684661865,
        "y2": 4.138033390045166
      },
      {
        "r": 0,
        "text": "Indeed, when expert opinions are not resolvable, there is inherent ambiguity present in the labels.",
        "trunc_text": "Indeed, when expert opinions are not resolvable, there is inherent ambiguity present in the labels.",
        "x1": 2.2475996017456055,
        "x2": 1.3196420669555664,
        "y1": 4.041403293609619,
        "y2": 4.6328020095825195
      },
      {
        "r": 0,
        "text": "That is, we do not have ``crisp'', definitive ground truth labels and this uncertainty should be taken into account during calibration.",
        "trunc_text": "That is, we do not have ``crisp'', definitive ground truth labels and this uncertainty should be taken into account duri",
        "x1": 1.981229305267334,
        "x2": 1.3725159168243408,
        "y1": 4.277517795562744,
        "y2": 4.353987693786621
      },
      {
        "r": 0,
        "text": "In this paper, we develop a conformal prediction framework for such ambiguous ground truth settings which relies on an approximation of the underlying posterior distribution of labels given inputs.",
        "trunc_text": "In this paper, we develop a conformal prediction framework for such ambiguous ground truth settings which relies on an a",
        "x1": 1.9939848184585571,
        "x2": 1.4487719535827637,
        "y1": 4.394720554351807,
        "y2": 4.2216386795043945
      },
      {
        "r": 0,
        "text": "We demonstrate our methodology on synthetic and real datasets, including a case study of skin condition classification in dermatology.",
        "trunc_text": "We demonstrate our methodology on synthetic and real datasets, including a case study of skin condition classification i",
        "x1": 3.221928596496582,
        "x2": 4.370260238647461,
        "y1": 7.647236347198486,
        "y2": 2.022995710372925
      },
      {
        "r": 0,
        "text": "In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13 billion samples, meticulously gathered from the Kaggle platform.",
        "trunc_text": "In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13 billion",
        "x1": 5.577910423278809,
        "x2": 6.131430149078369,
        "y1": 6.515905380249023,
        "y2": 3.118830680847168
      },
      {
        "r": 0,
        "text": "Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks.",
        "trunc_text": "Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, ",
        "x1": 3.765721082687378,
        "x2": 3.1143484115600586,
        "y1": 3.7475852966308594,
        "y2": 5.653622627258301
      },
      {
        "r": 0,
        "text": "This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks.",
        "trunc_text": "This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, ye",
        "x1": 5.869234085083008,
        "x2": 6.415315628051758,
        "y1": 6.465653896331787,
        "y2": 3.3824734687805176
      },
      {
        "r": 0,
        "text": "The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time.",
        "trunc_text": "The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, t",
        "x1": 5.806868553161621,
        "x2": 6.3627471923828125,
        "y1": 6.340925693511963,
        "y2": 3.395268678665161
      },
      {
        "r": 0,
        "text": "In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures.",
        "trunc_text": "In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manne",
        "x1": 5.90676736831665,
        "x2": 6.434201717376709,
        "y1": 6.461694717407227,
        "y2": 3.3968303203582764
      },
      {
        "r": 0,
        "text": "UniTabE's core concept relies on representing each basic table element with a module, termed TabUnit.",
        "trunc_text": "UniTabE's core concept relies on representing each basic table element with a module, termed TabUnit.",
        "x1": 5.924223899841309,
        "x2": 6.441769123077393,
        "y1": 6.4744977951049805,
        "y2": 3.4280593395233154
      },
      {
        "r": 0,
        "text": "This is subsequently followed by a Transformer encoder to refine the representation.",
        "trunc_text": "This is subsequently followed by a Transformer encoder to refine the representation.",
        "x1": 2.529067039489746,
        "x2": 2.517137050628662,
        "y1": 5.8378987312316895,
        "y2": 3.1133434772491455
      },
      {
        "r": 0,
        "text": "Moreover, our model is designed to facilitate pretraining and finetuning through the utilization of free-form prompts.  ",
        "trunc_text": "Moreover, our model is designed to facilitate pretraining and finetuning through the utilization of free-form prompts.  ",
        "x1": 5.0781331062316895,
        "x2": 3.9149973392486572,
        "y1": 4.093428134918213,
        "y2": 5.886939525604248
      },
      {
        "r": 0,
        "text": "Rigorous experimental testing and analyses were performed under a myriad of scenarios to validate the effectiveness of our methodology.",
        "trunc_text": "Rigorous experimental testing and analyses were performed under a myriad of scenarios to validate the effectiveness of o",
        "x1": 5.3917317390441895,
        "x2": 6.537362098693848,
        "y1": 8.915193557739258,
        "y2": 4.831149578094482
      },
      {
        "r": 0,
        "text": "The experimental results demonstrate UniTabE's superior performance against several baseline models across a multitude of benchmark datasets.",
        "trunc_text": "The experimental results demonstrate UniTabE's superior performance against several baseline models across a multitude o",
        "x1": 3.9212522506713867,
        "x2": 4.526387691497803,
        "y1": 6.347711563110352,
        "y2": 3.6348822116851807
      },
      {
        "r": 0,
        "text": "This, therefore, underscores UniTabE's potential to significantly enhance the semantic representation of tabular data, thereby marking a significant stride in the field of tabular data analysis.",
        "trunc_text": "This, therefore, underscores UniTabE's potential to significantly enhance the semantic representation of tabular data, t",
        "x1": 5.94074821472168,
        "x2": 6.532109260559082,
        "y1": 6.4894914627075195,
        "y2": 3.3914577960968018
      },
      {
        "r": 0,
        "text": "Large-scale online recommender system spreads all over the Internet being in charge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion Rate (CVR) estimations.",
        "trunc_text": "Large-scale online recommender system spreads all over the Internet being in charge of two basic tasks: Click-Through Ra",
        "x1": 4.630069255828857,
        "x2": 4.695270538330078,
        "y1": 5.185059547424316,
        "y2": 4.988433837890625
      },
      {
        "r": 0,
        "text": "However, traditional CVR estimators suffer from well-known Sample Selection Bias and Data Sparsity issues.",
        "trunc_text": "However, traditional CVR estimators suffer from well-known Sample Selection Bias and Data Sparsity issues.",
        "x1": 3.466721534729004,
        "x2": 4.021218299865723,
        "y1": 5.804251670837402,
        "y2": 3.8708443641662598
      },
      {
        "r": 0,
        "text": "Entire space models were proposed to address the two issues via tracing the decision-making path of \"exposure_click_purchase\".",
        "trunc_text": "Entire space models were proposed to address the two issues via tracing the decision-making path of \"exposure_click_purc",
        "x1": 4.225154399871826,
        "x2": 4.485015869140625,
        "y1": 5.239440441131592,
        "y2": 4.850125312805176
      },
      {
        "r": 0,
        "text": "Further, some researchers observed that there are purchase-related behaviors between click and purchase, which can better draw the user's decision-making intention and improve the recommendation performance.",
        "trunc_text": "Further, some researchers observed that there are purchase-related behaviors between click and purchase, which can bette",
        "x1": 4.452947616577148,
        "x2": 4.693962097167969,
        "y1": 5.17946195602417,
        "y2": 4.965104579925537
      },
      {
        "r": 0,
        "text": "Thus, the decision-making path has been extended to \"exposure_click_in-shop action_purchase\" and can be modeled with conditional probability approach.",
        "trunc_text": "Thus, the decision-making path has been extended to \"exposure_click_in-shop action_purchase\" and can be modeled with con",
        "x1": 4.2597832679748535,
        "x2": 4.365176677703857,
        "y1": 5.1910176277160645,
        "y2": 4.886448383331299
      },
      {
        "r": 0,
        "text": "Nevertheless, we observe that the chain rule of conditional probability does not always hold.",
        "trunc_text": "Nevertheless, we observe that the chain rule of conditional probability does not always hold.",
        "x1": 3.7648022174835205,
        "x2": 4.004426002502441,
        "y1": 5.2206830978393555,
        "y2": 4.706000804901123
      },
      {
        "r": 0,
        "text": "We report Probability Space Confusion (PSC) issue and give a derivation of difference between ground-truth and estimation mathematically.",
        "trunc_text": "We report Probability Space Confusion (PSC) issue and give a derivation of difference between ground-truth and estimatio",
        "x1": 2.022240400314331,
        "x2": 1.480953574180603,
        "y1": 4.310506820678711,
        "y2": 4.385159492492676
      },
      {
        "r": 0,
        "text": "We propose a novel Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint (ESMC) and two alternatives: Entire Space Multi-Task Model with Siamese Network (ESMS) and Entire Space Multi-Task Model in Global Domain (ESMG) to address the PSC issue.",
        "trunc_text": "We propose a novel Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint (ESMC) and two ",
        "x1": 4.728030681610107,
        "x2": 4.116725921630859,
        "y1": 4.378048419952393,
        "y2": 5.461175441741943
      },
      {
        "r": 0,
        "text": "Specifically, we handle \"exposure_click_in-shop action\" and \"in-shop action_purchase\" separately in the light of characteristics of in-shop action.",
        "trunc_text": "Specifically, we handle \"exposure_click_in-shop action\" and \"in-shop action_purchase\" separately in the light of charact",
        "x1": 4.283878326416016,
        "x2": 4.553131103515625,
        "y1": 5.251986026763916,
        "y2": 4.898957252502441
      },
      {
        "r": 0,
        "text": "The first path is still treated with conditional probability while the second one is treated with parameter constraint strategy.",
        "trunc_text": "The first path is still treated with conditional probability while the second one is treated with parameter constraint s",
        "x1": 3.7938365936279297,
        "x2": 4.145171165466309,
        "y1": 5.23642635345459,
        "y2": 4.796238899230957
      },
      {
        "r": 0,
        "text": "Experiments on both offline and online environments in a large-scale recommendation system illustrate the superiority of our proposed methods over state-of-the-art models.",
        "trunc_text": "Experiments on both offline and online environments in a large-scale recommendation system illustrate the superiority of",
        "x1": 4.5627007484436035,
        "x2": 4.772839069366455,
        "y1": 5.185609817504883,
        "y2": 4.9667744636535645
      },
      {
        "r": 0,
        "text": "This paper proposes a new SOD dataset consisting of 39,070 images including 137,121 bird instances, which is called the Small Object Detection for Spotting Birds (SOD4SB) dataset",
        "trunc_text": "This paper proposes a new SOD dataset consisting of 39,070 images including 137,121 bird instances, which is called the ",
        "x1": 0.6812712550163269,
        "x2": 2.1991589069366455,
        "y1": 7.474150657653809,
        "y2": 1.3839116096496582
      },
      {
        "r": 0,
        "text": "The dataset, the baseline code, and the website for evaluation on the public testset are publicly available.",
        "trunc_text": "The dataset, the baseline code, and the website for evaluation on the public testset are publicly available.",
        "x1": 7.142136573791504,
        "x2": 7.713898181915283,
        "y1": 7.518990993499756,
        "y2": 2.4687371253967285
      },
      {
        "r": 0,
        "text": "Small Object Detection (SOD) is an important machine vision topic because (i) a variety of real-world applications require object detection for distant objects and (ii) SOD is a challenging task due to the noisy, blurred, and less-informative image appearances of small objects. .",
        "trunc_text": "Small Object Detection (SOD) is an important machine vision topic because (i) a variety of real-world applications requi",
        "x1": 0.6046254634857178,
        "x2": 2.1364290714263916,
        "y1": 7.6128830909729,
        "y2": 1.297027587890625
      },
      {
        "r": 0,
        "text": "The detail of the challenge with the SOD4SB dataset is introduced in this paper.",
        "trunc_text": "The detail of the challenge with the SOD4SB dataset is introduced in this paper.",
        "x1": 4.566805839538574,
        "x2": 5.302064895629883,
        "y1": 7.406980037689209,
        "y2": 2.607422351837158
      },
      {
        "r": 0,
        "text": "In total, 223 participants joined this challenge.",
        "trunc_text": "In total, 223 participants joined this challenge.",
        "x1": 4.753406524658203,
        "x2": 6.120185375213623,
        "y1": 8.188153266906738,
        "y2": 1.868630290031433
      },
      {
        "r": 0,
        "text": "This paper briefly introduces the award-winning methods.",
        "trunc_text": "This paper briefly introduces the award-winning methods.",
        "x1": 4.917050361633301,
        "x2": 5.929220676422119,
        "y1": 8.21039867401123,
        "y2": 4.332735061645508
      },
      {
        "r": 0,
        "text": "The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions.",
        "trunc_text": "The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textua",
        "x1": 4.0863142013549805,
        "x2": 2.575873613357544,
        "y1": 3.5469632148742676,
        "y2": 4.731695175170898
      },
      {
        "r": 0,
        "text": "The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities.",
        "trunc_text": "The main challenge of this task lies in the significant differences in information representation between the visual and",
        "x1": 4.082945346832275,
        "x2": 2.6875360012054443,
        "y1": 3.5032496452331543,
        "y2": 4.941736221313477
      },
      {
        "r": 0,
        "text": "The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images.",
        "trunc_text": "The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the v",
        "x1": 4.175634860992432,
        "x2": 2.711322784423828,
        "y1": 3.5465304851531982,
        "y2": 4.8544392585754395
      },
      {
        "r": 0,
        "text": "To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.   ",
        "trunc_text": "To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual de",
        "x1": 4.103350639343262,
        "x2": 2.6333582401275635,
        "y1": 3.5371501445770264,
        "y2": 4.768527030944824
      },
      {
        "r": 0,
        "text": "To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences.",
        "trunc_text": "To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person ret",
        "x1": 4.218642711639404,
        "x2": 2.5720908641815186,
        "y1": 3.5064072608947754,
        "y2": 4.84569787979126
      },
      {
        "r": 0,
        "text": "Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge.",
        "trunc_text": "Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking ",
        "x1": 4.0690507888793945,
        "x2": 2.6428959369659424,
        "y1": 3.6713922023773193,
        "y2": 4.6159443855285645
      },
      {
        "r": 0,
        "text": "The Text-guided Image Restoration auxiliary task is proposed with the aim of implicitly mapping abstract textual entities to specific image regions, facilitating alignment between textual and visual embeddings.",
        "trunc_text": "The Text-guided Image Restoration auxiliary task is proposed with the aim of implicitly mapping abstract textual entitie",
        "x1": 4.054001331329346,
        "x2": 2.6113545894622803,
        "y1": 3.5718271732330322,
        "y2": 4.706099510192871
      },
      {
        "r": 0,
        "text": "Additionally, we introduce a cross-modal triplet loss tailored for handling hard samples, enhancing the model's ability to distinguish minor differences.   ",
        "trunc_text": "Additionally, we introduce a cross-modal triplet loss tailored for handling hard samples, enhancing the model's ability ",
        "x1": 2.907726287841797,
        "x2": 3.248854875564575,
        "y1": 6.20799446105957,
        "y2": 3.556770086288452
      },
      {
        "r": 0,
        "text": "To focus the model on the key components within sentences, we propose a novel text data augmentation technique.",
        "trunc_text": "To focus the model on the key components within sentences, we propose a novel text data augmentation technique.",
        "x1": 3.9142749309539795,
        "x2": 2.831761598587036,
        "y1": 3.4567182064056396,
        "y2": 5.826580047607422
      },
      {
        "r": 0,
        "text": "Diffusion MRI tractography parcellation classifies streamlines into anatomical fiber tracts to enable quantification and visualization for clinical and scientific applications.",
        "trunc_text": "Diffusion MRI tractography parcellation classifies streamlines into anatomical fiber tracts to enable quantification and",
        "x1": 1.062098741531372,
        "x2": 2.882274866104126,
        "y1": 6.53164529800415,
        "y2": 1.8221704959869385
      },
      {
        "r": 0,
        "text": "Current tractography parcellation methods rely heavily on registration, but registration inaccuracies can affect parcellation and the computational cost of registration is high for large-scale datasets.",
        "trunc_text": "Current tractography parcellation methods rely heavily on registration, but registration inaccuracies can affect parcell",
        "x1": 0.9908000230789185,
        "x2": 2.8480803966522217,
        "y1": 6.507661819458008,
        "y2": 1.827834129333496
      },
      {
        "r": 0,
        "text": "Recently, deep-learning-based methods have been proposed for tractography parcellation using various types of representations for streamlines.",
        "trunc_text": "Recently, deep-learning-based methods have been proposed for tractography parcellation using various types of representa",
        "x1": 1.0468751192092896,
        "x2": 2.853175401687622,
        "y1": 6.510106086730957,
        "y2": 1.8328876495361328
      },
      {
        "r": 0,
        "text": "However, these methods only focus on the information from a single streamline, ignoring geometric relationships between the streamlines in the brain.",
        "trunc_text": "However, these methods only focus on the information from a single streamline, ignoring geometric relationships between ",
        "x1": 3.267843723297119,
        "x2": 3.7015559673309326,
        "y1": 6.9819817543029785,
        "y2": 2.2659249305725098
      },
      {
        "r": 0,
        "text": "We propose TractCloud, a registration-free framework that performs whole-brain tractography parcellation directly in individual subject space.",
        "trunc_text": "We propose TractCloud, a registration-free framework that performs whole-brain tractography parcellation directly in ind",
        "x1": 1.0175116062164307,
        "x2": 2.829714775085449,
        "y1": 6.5294928550720215,
        "y2": 1.8859245777130127
      },
      {
        "r": 0,
        "text": "We propose a novel, learnable, local-global streamline representation that leverages information from neighboring and whole-brain streamlines to describe the local anatomy and global pose of the brain.",
        "trunc_text": "We propose a novel, learnable, local-global streamline representation that leverages information from neighboring and wh",
        "x1": 1.7756786346435547,
        "x2": 3.229321241378784,
        "y1": 6.889374256134033,
        "y2": 2.016054153442383
      },
      {
        "r": 0,
        "text": "We train our framework on a large-scale labeled tractography dataset, which we augment by applying synthetic transforms including rotation, scaling, and translations.",
        "trunc_text": "We train our framework on a large-scale labeled tractography dataset, which we augment by applying synthetic transforms ",
        "x1": 1.0245665311813354,
        "x2": 2.9059767723083496,
        "y1": 6.489449977874756,
        "y2": 1.9057285785675049
      },
      {
        "r": 0,
        "text": "We test our framework on five independently acquired datasets across populations and health conditions.",
        "trunc_text": "We test our framework on five independently acquired datasets across populations and health conditions.",
        "x1": 3.8898158073425293,
        "x2": 5.035813808441162,
        "y1": 7.5616021156311035,
        "y2": 2.316774606704712
      },
      {
        "r": 0,
        "text": "TractCloud significantly outperforms several state-of-the-art methods on all testing datasets.",
        "trunc_text": "TractCloud significantly outperforms several state-of-the-art methods on all testing datasets.",
        "x1": 4.362772464752197,
        "x2": 4.4741315841674805,
        "y1": 7.049785614013672,
        "y2": 2.95192551612854
      },
      {
        "r": 0,
        "text": "TractCloud achieves efficient and consistent whole-brain white matter parcellation across the lifespan (from neonates to elderly subjects, including brain tumor patients) without the need for registration.",
        "trunc_text": "TractCloud achieves efficient and consistent whole-brain white matter parcellation across the lifespan (from neonates to",
        "x1": 1.0526357889175415,
        "x2": 2.8833494186401367,
        "y1": 6.535017013549805,
        "y2": 1.7995975017547607
      },
      {
        "r": 0,
        "text": "The robustness and high inference speed of TractCloud make it suitable for large-scale tractography data analysis.",
        "trunc_text": "The robustness and high inference speed of TractCloud make it suitable for large-scale tractography data analysis.",
        "x1": 1.0152658224105835,
        "x2": 2.89373517036438,
        "y1": 6.519113540649414,
        "y2": 1.8573578596115112
      },
      {
        "r": 0,
        "text": "Our project page is available at https://tractcloud.github.io/.",
        "trunc_text": "Our project page is available at https://tractcloud.github.io/.",
        "x1": 7.907959461212158,
        "x2": 8.527958869934082,
        "y1": 8.020137786865234,
        "y2": 1.8802698850631714
      },
      {
        "r": 0,
        "text": "The AgBioData Consortium (https://www.agbiodata.org/) currently represents 44 databases and resources covering model or crop plant and animal GGB data, ontologies, pathways, genetic variation and breeding platforms (referred to as 'databases' throughout).",
        "trunc_text": "The AgBioData Consortium (https://www.agbiodata.org/) currently represents 44 databases and resources covering model or ",
        "x1": 6.072878360748291,
        "x2": 6.5904998779296875,
        "y1": 7.7500481605529785,
        "y2": 2.3843350410461426
      },
      {
        "r": 0,
        "text": "Over the last several decades, there has been rapid growth in the number and scope of agricultural genetics, genomics and breeding (GGB) databases and resources.  ",
        "trunc_text": "Over the last several decades, there has been rapid growth in the number and scope of agricultural genetics, genomics an",
        "x1": 1.3025171756744385,
        "x2": 5.360513210296631,
        "y1": 7.657537937164307,
        "y2": 1.6363625526428223
      },
      {
        "r": 0,
        "text": "One of the goals of the Consortium is to facilitate FAIR (Findable, Accessible, Interoperable, and Reusable) data management and the integration of datasets which requires data sharing, along with structured vocabularies and/or ontologies.",
        "trunc_text": "One of the goals of the Consortium is to facilitate FAIR (Findable, Accessible, Interoperable, and Reusable) data manage",
        "x1": 5.987636566162109,
        "x2": 6.551542282104492,
        "y1": 7.38895845413208,
        "y2": 2.5674257278442383
      },
      {
        "r": 0,
        "text": "Two AgBioData working groups, focused on Data Sharing and Ontologies, conducted a survey to assess the status and future needs of the members in those areas.",
        "trunc_text": "Two AgBioData working groups, focused on Data Sharing and Ontologies, conducted a survey to assess the status and future",
        "x1": 6.084682464599609,
        "x2": 6.621017932891846,
        "y1": 7.747931480407715,
        "y2": 2.419116497039795
      },
      {
        "r": 0,
        "text": "A total of 33 researchers responded to the survey, representing 37 databases.",
        "trunc_text": "A total of 33 researchers responded to the survey, representing 37 databases.",
        "x1": 5.18733024597168,
        "x2": 6.281986236572266,
        "y1": 7.863976001739502,
        "y2": 2.0619990825653076
      },
      {
        "r": 0,
        "text": "Results suggest that data sharing practices by AgBioData databases are in a healthy state, but it is not clear whether this is true for all metadata and data types across all databases; and that ontology use has not substantially changed since a similar survey was conducted in 2017.",
        "trunc_text": "Results suggest that data sharing practices by AgBioData databases are in a healthy state, but it is not clear whether t",
        "x1": 6.067150592803955,
        "x2": 6.567557334899902,
        "y1": 7.728360652923584,
        "y2": 2.4896817207336426
      },
      {
        "r": 0,
        "text": "We recommend 1) providing training for database personnel in specific data sharing techniques, as well as in ontology use; 2) further study on what metadata is shared, and how well it is shared among databases; 3) promoting an understanding of data sharing and ontologies in the stakeholder community; 4) improving data sharing and ontologies for specific phenotypic data types and formats; and 5) lowering specific barriers to data sharing and ontology use, by identifying sustainability solutions, and the identification, promotion, or development of data standards.",
        "trunc_text": "We recommend 1) providing training for database personnel in specific data sharing techniques, as well as in ontology us",
        "x1": 6.037601947784424,
        "x2": 6.621008396148682,
        "y1": 7.6896820068359375,
        "y2": 2.493286371231079
      },
      {
        "r": 0,
        "text": "Combined, these improvements are likely to help AgBioData databases increase development efforts towards improved ontology use, and data sharing via programmatic means.",
        "trunc_text": "Combined, these improvements are likely to help AgBioData databases increase development efforts towards improved ontolo",
        "x1": 6.0556182861328125,
        "x2": 6.580531120300293,
        "y1": 7.704252243041992,
        "y2": 2.4642374515533447
      },
      {
        "r": 0,
        "text": "To feed the data-hungry models, we first elaborate interference generators and conduct comprehensive co-location experiments on a testbed to build Alioth-dataset which reflects the complexity and dynamicity in real-world scenarios",
        "trunc_text": "To feed the data-hungry models, we first elaborate interference generators and conduct comprehensive co-location experim",
        "x1": 4.2885637283325195,
        "x2": 5.154033184051514,
        "y1": 7.373409271240234,
        "y2": 2.558814287185669
      },
      {
        "r": 0,
        "text": "The dataset and code of Alioth have been released on GitHub.",
        "trunc_text": "The dataset and code of Alioth have been released on GitHub.",
        "x1": 7.386879920959473,
        "x2": 7.871296405792236,
        "y1": 7.83442497253418,
        "y2": 1.8868968486785889
      },
      {
        "r": 0,
        "text": "Multi-tenancy in public clouds may lead to co-location interference on shared resources, which possibly results in performance degradation of cloud applications.",
        "trunc_text": "Multi-tenancy in public clouds may lead to co-location interference on shared resources, which possibly results in perfo",
        "x1": 5.791406154632568,
        "x2": 5.836045265197754,
        "y1": 6.017083644866943,
        "y2": 3.9457011222839355
      },
      {
        "r": 0,
        "text": "Cloud providers want to know when such events happen and how serious the degradation is, to perform interference-aware migrations and alleviate the problem.",
        "trunc_text": "Cloud providers want to know when such events happen and how serious the degradation is, to perform interference-aware m",
        "x1": 5.781625747680664,
        "x2": 5.8457136154174805,
        "y1": 5.9836015701293945,
        "y2": 3.989328145980835
      },
      {
        "r": 0,
        "text": "However, virtual machines (VM) in Infrastructure-as-a-Service public clouds are black-boxes to providers, where application-level performance information cannot be acquired.",
        "trunc_text": "However, virtual machines (VM) in Infrastructure-as-a-Service public clouds are black-boxes to providers, where applicat",
        "x1": 5.816910266876221,
        "x2": 5.826536178588867,
        "y1": 6.0238447189331055,
        "y2": 3.9800572395324707
      },
      {
        "r": 0,
        "text": "This makes performance monitoring intensely challenging as cloud providers can only rely on low-level metrics such as CPU usage and hardware counters.   ",
        "trunc_text": "This makes performance monitoring intensely challenging as cloud providers can only rely on low-level metrics such as CP",
        "x1": 5.722811222076416,
        "x2": 5.7568745613098145,
        "y1": 6.01580286026001,
        "y2": 3.9957714080810547
      },
      {
        "r": 0,
        "text": "We propose a novel machine learning framework, Alioth, to monitor the performance degradation of cloud applications. .",
        "trunc_text": "We propose a novel machine learning framework, Alioth, to monitor the performance degradation of cloud applications. .",
        "x1": 5.786114692687988,
        "x2": 5.8165602684021,
        "y1": 6.004328727722168,
        "y2": 3.9921486377716064
      },
      {
        "r": 0,
        "text": "Then we construct Alioth by (1) augmenting features via recovering low-level metrics under no interference using denoising auto-encoders, (2) devising a transfer learning model based on domain adaptation neural network to make models generalize on test cases unseen in offline training, and (3) developing a SHAP explainer to automate feature selection and enhance model interpretability.",
        "trunc_text": "Then we construct Alioth by (1) augmenting features via recovering low-level metrics under no interference using denoisi",
        "x1": 2.2871885299682617,
        "x2": 2.509784460067749,
        "y1": 5.756927490234375,
        "y2": 3.315645933151245
      },
      {
        "r": 0,
        "text": "Experiments show that Alioth achieves an average mean absolute error of 5.29% offline and 10.8% when testing on applications unseen in the training stage, outperforming the baseline methods.",
        "trunc_text": "Experiments show that Alioth achieves an average mean absolute error of 5.29% offline and 10.8% when testing on applicat",
        "x1": 3.81280779838562,
        "x2": 4.895578384399414,
        "y1": 6.474364757537842,
        "y2": 3.657304048538208
      },
      {
        "r": 0,
        "text": "Alioth is also robust in signaling quality-of-service violation under dynamicity.",
        "trunc_text": "Alioth is also robust in signaling quality-of-service violation under dynamicity.",
        "x1": 5.83214807510376,
        "x2": 5.851531028747559,
        "y1": 5.926921844482422,
        "y2": 4.1716437339782715
      },
      {
        "r": 0,
        "text": "Finally, we demonstrate a possible application of Alioth's interpretability, providing insights to benefit the decision-making of cloud operators.",
        "trunc_text": "Finally, we demonstrate a possible application of Alioth's interpretability, providing insights to benefit the decision-",
        "x1": 5.787254333496094,
        "x2": 5.8017449378967285,
        "y1": 5.974391460418701,
        "y2": 4.043299198150635
      },
      {
        "r": 0,
        "text": "We introduce the problem of knot-based inverse perceptual art.",
        "trunc_text": "We introduce the problem of knot-based inverse perceptual art.",
        "x1": 1.5989280939102173,
        "x2": 3.2917354106903076,
        "y1": 8.53825855255127,
        "y2": 0.6446273326873779
      },
      {
        "r": 0,
        "text": "Given multiple target images and their corresponding viewing configurations, the objective is to find a 3D knot-based tubular structure whose appearance resembles the target images when viewed from the specified viewing configurations.",
        "trunc_text": "Given multiple target images and their corresponding viewing configurations, the objective is to find a 3D knot-based tu",
        "x1": 1.5565574169158936,
        "x2": 3.2660436630249023,
        "y1": 8.649284362792969,
        "y2": 0.5168730616569519
      },
      {
        "r": 0,
        "text": "To solve this problem, we first design a differentiable rendering algorithm for rendering tubular knots embedded in 3D for arbitrary perspective camera configurations.",
        "trunc_text": "To solve this problem, we first design a differentiable rendering algorithm for rendering tubular knots embedded in 3D f",
        "x1": 1.5915285348892212,
        "x2": 3.2777273654937744,
        "y1": 8.684103965759277,
        "y2": 0.5697106122970581
      },
      {
        "r": 0,
        "text": "Utilizing this differentiable rendering algorithm, we search over the space of knot configurations to find the ideal knot embedding.",
        "trunc_text": "Utilizing this differentiable rendering algorithm, we search over the space of knot configurations to find the ideal kno",
        "x1": 1.5333424806594849,
        "x2": 3.247788429260254,
        "y1": 8.684976577758789,
        "y2": 0.5516334772109985
      },
      {
        "r": 0,
        "text": "We represent the knot embeddings via homeomorphisms of the desired template knot, where the homeomorphisms are parametrized by the weights of an invertible neural network.",
        "trunc_text": "We represent the knot embeddings via homeomorphisms of the desired template knot, where the homeomorphisms are parametri",
        "x1": 1.49880850315094,
        "x2": 3.2311313152313232,
        "y1": 8.593844413757324,
        "y2": 0.5428608655929565
      },
      {
        "r": 0,
        "text": "Our approach is fully differentiable, making it possible to find the ideal 3D tubular structure for the desired perceptual art using gradient-based optimization.",
        "trunc_text": "Our approach is fully differentiable, making it possible to find the ideal 3D tubular structure for the desired perceptu",
        "x1": 1.6829606294631958,
        "x2": 3.271271228790283,
        "y1": 8.670029640197754,
        "y2": 0.5774526000022888
      },
      {
        "r": 0,
        "text": "We propose several loss functions that impose additional physical constraints, ensuring that the tube is free of self-intersection, lies within a predefined region in space, satisfies the physical bending limits of the tube material and the material cost is within a specified budget.",
        "trunc_text": "We propose several loss functions that impose additional physical constraints, ensuring that the tube is free of self-in",
        "x1": 1.76084566116333,
        "x2": 3.2905845642089844,
        "y1": 8.706864356994629,
        "y2": 0.5379778742790222
      },
      {
        "r": 0,
        "text": "We demonstrate through results that our knot representation is highly expressive and gives impressive results even for challenging target images in both single view as well as multiple view constraints.",
        "trunc_text": "We demonstrate through results that our knot representation is highly expressive and gives impressive results even for c",
        "x1": 1.5167537927627563,
        "x2": 3.2328855991363525,
        "y1": 8.669797897338867,
        "y2": 0.4996252954006195
      },
      {
        "r": 0,
        "text": "Through extensive ablation study we show that each of the proposed loss function is effective in ensuring physical realizability.",
        "trunc_text": "Through extensive ablation study we show that each of the proposed loss function is effective in ensuring physical reali",
        "x1": 5.834759712219238,
        "x2": 6.6653947830200195,
        "y1": 8.690083503723145,
        "y2": 5.272960186004639
      },
      {
        "r": 0,
        "text": "To the best of our knowledge, we are the first to propose a fully differentiable optimization framework for knot-based inverse perceptual art.",
        "trunc_text": "To the best of our knowledge, we are the first to propose a fully differentiable optimization framework for knot-based i",
        "x1": 1.6607677936553955,
        "x2": 3.26473069190979,
        "y1": 8.528965950012207,
        "y2": 0.6118759512901306
      },
      {
        "r": 0,
        "text": "Both the code and data will be made publicly available.",
        "trunc_text": "Both the code and data will be made publicly available.",
        "x1": 8.02557373046875,
        "x2": 8.405065536499023,
        "y1": 7.245002269744873,
        "y2": 2.724005937576294
      },
      {
        "r": 0,
        "text": "To do so, a comparable big dataset of metadata of apps has been collected for learning and evaluation in this work.",
        "trunc_text": "To do so, a comparable big dataset of metadata of apps has been collected for learning and evaluation in this work.",
        "x1": 6.541277885437012,
        "x2": 6.615030288696289,
        "y1": 6.250662326812744,
        "y2": 3.588414192199707
      },
      {
        "r": 0,
        "text": "In the digitized world, smartphones and their apps play an important role.",
        "trunc_text": "In the digitized world, smartphones and their apps play an important role.",
        "x1": 6.541139602661133,
        "x2": 6.883489608764648,
        "y1": 6.237663745880127,
        "y2": 3.6835687160491943
      },
      {
        "r": 0,
        "text": "To name just a few examples, some apps offer possibilities for entertainment, others for online banking, and others offer support for two-factor authentication.",
        "trunc_text": "To name just a few examples, some apps offer possibilities for entertainment, others for online banking, and others offe",
        "x1": 6.434938907623291,
        "x2": 6.723875522613525,
        "y1": 6.2034101486206055,
        "y2": 3.71647310256958
      },
      {
        "r": 0,
        "text": "Therefore, with smartphones also, sensitive information is shared; thus, they are a desirable target for malware.",
        "trunc_text": "Therefore, with smartphones also, sensitive information is shared; thus, they are a desirable target for malware.",
        "x1": 6.670144557952881,
        "x2": 6.836226940155029,
        "y1": 6.151897430419922,
        "y2": 3.7570910453796387
      },
      {
        "r": 0,
        "text": "The following technical report gives an overview of how machine learning, especially neural networks, can be employed to detect malicious Android apps based on their metadata.",
        "trunc_text": "The following technical report gives an overview of how machine learning, especially neural networks, can be employed to",
        "x1": 6.703725814819336,
        "x2": 6.729684829711914,
        "y1": 6.090243339538574,
        "y2": 3.860252618789673
      },
      {
        "r": 0,
        "text": "Detection based on the metadata is necessary since not all of an app's information is readable from another app due to the security layout of Android.  ",
        "trunc_text": "Detection based on the metadata is necessary since not all of an app's information is readable from another app due to t",
        "x1": 6.6603102684021,
        "x2": 6.814395427703857,
        "y1": 6.162209510803223,
        "y2": 3.7716879844665527
      },
      {
        "r": 0,
        "text": "The first section, after the introduction, presents the related work, followed by the description of the sources of the dataset and the selection of the features used for machine learning, in this case, only the app permissions.",
        "trunc_text": "The first section, after the introduction, presents the related work, followed by the description of the sources of the ",
        "x1": 7.1786394119262695,
        "x2": 7.620909214019775,
        "y1": 7.510461330413818,
        "y2": 2.2656502723693848
      },
      {
        "r": 0,
        "text": "Afterward, a free available dataset is used to find an efficient and effective neural network model for learning and evaluation.",
        "trunc_text": "Afterward, a free available dataset is used to find an efficient and effective neural network model for learning and eva",
        "x1": 4.7303056716918945,
        "x2": 5.4588704109191895,
        "y1": 6.725690841674805,
        "y2": 3.193175792694092
      },
      {
        "r": 0,
        "text": "Here, the fully connected network type consisting of dense layers is chosen.",
        "trunc_text": "Here, the fully connected network type consisting of dense layers is chosen.",
        "x1": 2.0945088863372803,
        "x2": 2.5396251678466797,
        "y1": 5.983624458312988,
        "y2": 3.0382144451141357
      },
      {
        "r": 0,
        "text": "It turns out that this model detects malware with an accuracy of 92.93% based on an app's permissions.",
        "trunc_text": "It turns out that this model detects malware with an accuracy of 92.93% based on an app's permissions.",
        "x1": 6.66985559463501,
        "x2": 6.58665132522583,
        "y1": 6.058147430419922,
        "y2": 3.9596476554870605
      },
      {
        "r": 0,
        "text": "To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information.",
        "trunc_text": "To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified",
        "x1": 5.646960735321045,
        "x2": 3.7441482543945312,
        "y1": 3.135676622390747,
        "y2": 6.987769603729248
      },
      {
        "r": 0,
        "text": "Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training.",
        "trunc_text": "Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, con",
        "x1": 5.663351058959961,
        "x2": 3.696686029434204,
        "y1": 3.1303811073303223,
        "y2": 6.920929908752441
      },
      {
        "r": 0,
        "text": "To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning",
        "trunc_text": "To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompt",
        "x1": 5.674344539642334,
        "x2": 3.757439136505127,
        "y1": 3.2096610069274902,
        "y2": 6.991535186767578
      },
      {
        "r": 0,
        "text": "Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness.  ",
        "trunc_text": "Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, ",
        "x1": 5.71713399887085,
        "x2": 3.6379144191741943,
        "y1": 3.1990253925323486,
        "y2": 7.061403751373291
      },
      {
        "r": 0,
        "text": "Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounFurthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-shot and few-shot learning scenarios demonstrate the superiority of DialogStudio.",
        "trunc_text": "Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, con",
        "x1": 5.6373796463012695,
        "x2": 3.6677045822143555,
        "y1": 3.118180513381958,
        "y2": 6.976084232330322
      },
      {
        "r": 0,
        "text": "To improve transparency and support dataset and task-based research, as well as language model pre-train",
        "trunc_text": "To improve transparency and support dataset and task-based research, as well as language model pre-train",
        "x1": 4.781590938568115,
        "x2": 3.385481357574463,
        "y1": 3.89319109916687,
        "y2": 5.9083099365234375
      },
      {
        "r": 0,
        "text": "Movement paths are used widely in intelligent transportation and smart city applications.",
        "trunc_text": "Movement paths are used widely in intelligent transportation and smart city applications.",
        "x1": 3.7181034088134766,
        "x2": 5.135180473327637,
        "y1": 9.691438674926758,
        "y2": -0.22490085661411285
      },
      {
        "r": 0,
        "text": "To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation.",
        "trunc_text": "To serve such applications, path representation learning aims to provide compact representations of paths that enable ef",
        "x1": 2.4898390769958496,
        "x2": 2.7321696281433105,
        "y1": 5.769650936126709,
        "y2": 3.3099186420440674
      },
      {
        "r": 0,
        "text": "In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential.",
        "trunc_text": "In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited e",
        "x1": 2.4684245586395264,
        "x2": 2.762561798095703,
        "y1": 5.754174709320068,
        "y2": 3.368014335632324
      },
      {
        "r": 0,
        "text": "Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability.   ",
        "trunc_text": "Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource con",
        "x1": 2.485924243927002,
        "x2": 2.753709316253662,
        "y1": 5.723149299621582,
        "y2": 3.354365587234497
      },
      {
        "r": 0,
        "text": "We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability.",
        "trunc_text": "We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce reso",
        "x1": 2.5004453659057617,
        "x2": 2.7931015491485596,
        "y1": 5.68877649307251,
        "y2": 3.332904815673828
      },
      {
        "r": 0,
        "text": "More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good scalability with respect to path length.",
        "trunc_text": "More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good scalability with",
        "x1": 2.4999845027923584,
        "x2": 2.7522380352020264,
        "y1": 5.705631732940674,
        "y2": 3.414350986480713
      },
      {
        "r": 0,
        "text": "Next, we propose a relational reasoning framework to enable faster training of more robust sparse path encoders.",
        "trunc_text": "Next, we propose a relational reasoning framework to enable faster training of more robust sparse path encoders.",
        "x1": 2.5563793182373047,
        "x2": 2.7500572204589844,
        "y1": 5.640305519104004,
        "y2": 3.3862531185150146
      },
      {
        "r": 0,
        "text": "We also propose global-local knowledge distillation to further reduce the size and improve the performance of sparse path encoders.",
        "trunc_text": "We also propose global-local knowledge distillation to further reduce the size and improve the performance of sparse pat",
        "x1": 2.4630215167999268,
        "x2": 2.7561588287353516,
        "y1": 5.76692533493042,
        "y2": 3.3098437786102295
      },
      {
        "r": 0,
        "text": "Visual (re)localization is critical for various applications in computer vision and robotics.",
        "trunc_text": "Visual (re)localization is critical for various applications in computer vision and robotics.",
        "x1": 0.7505611181259155,
        "x2": 2.365530252456665,
        "y1": 8.085562705993652,
        "y2": 0.8970248699188232
      },
      {
        "r": 0,
        "text": "Its goal is to estimate the 6 degrees of freedom (DoF) camera pose for each query image, based on a set of posed database images.",
        "trunc_text": "Its goal is to estimate the 6 degrees of freedom (DoF) camera pose for each query image, based on a set of posed databas",
        "x1": 1.5012588500976562,
        "x2": 3.6575357913970947,
        "y1": 9.180679321289062,
        "y2": 0.10935454815626144
      },
      {
        "r": 0,
        "text": "Currently, all leading solutions are structure-based that either explicitly construct 3D metric maps from the database with structure-from-motion, or implicitly encode the 3D information with scene coordinate regression models.",
        "trunc_text": "Currently, all leading solutions are structure-based that either explicitly construct 3D metric maps from the database w",
        "x1": 1.7672772407531738,
        "x2": 3.2422916889190674,
        "y1": 9.133622169494629,
        "y2": 0.05397342890501022
      },
      {
        "r": 0,
        "text": "On the contrary, visual localization without reconstructing the scene in 3D offers clear benefits.",
        "trunc_text": "On the contrary, visual localization without reconstructing the scene in 3D offers clear benefits.",
        "x1": 0.8250846862792969,
        "x2": 2.461620330810547,
        "y1": 8.197099685668945,
        "y2": 0.7421314716339111
      },
      {
        "r": 0,
        "text": "It makes deployment more convenient by reducing database pre-processing time, releasing storage requirements, and remaining unaffected by imperfect reconstruction, etc.",
        "trunc_text": "It makes deployment more convenient by reducing database pre-processing time, releasing storage requirements, and remain",
        "x1": 6.076107025146484,
        "x2": 6.565735816955566,
        "y1": 6.508318901062012,
        "y2": 3.578397512435913
      },
      {
        "r": 0,
        "text": "In this technical report, we demonstrate that it is possible to achieve high localization accuracy without reconstructing the scene from the database.",
        "trunc_text": "In this technical report, we demonstrate that it is possible to achieve high localization accuracy without reconstructin",
        "x1": 0.7606786489486694,
        "x2": 2.3957533836364746,
        "y1": 8.161187171936035,
        "y2": 0.7666885852813721
      },
      {
        "r": 0,
        "text": "The key to achieving this owes to a tailored motion averaging over database-query pairs.",
        "trunc_text": "The key to achieving this owes to a tailored motion averaging over database-query pairs.",
        "x1": 2.792638063430786,
        "x2": 4.105401515960693,
        "y1": 9.459753036499023,
        "y2": -0.4203781187534332
      },
      {
        "r": 0,
        "text": "Experiments show that our visual localization proposal, LazyLoc, achieves comparable performance against state-of-the-art structure-based methods.",
        "trunc_text": "Experiments show that our visual localization proposal, LazyLoc, achieves comparable performance against state-of-the-ar",
        "x1": 0.8187644481658936,
        "x2": 2.450359344482422,
        "y1": 8.21242904663086,
        "y2": 0.7624136209487915
      },
      {
        "r": 0,
        "text": "Furthermore, we showcase the versatility of LazyLoc, which can be easily extended to handle complex configurations such as multi-query co-localization and camera rigs.",
        "trunc_text": "Furthermore, we showcase the versatility of LazyLoc, which can be easily extended to handle complex configurations such ",
        "x1": 0.8685877919197083,
        "x2": 2.563573122024536,
        "y1": 8.280345916748047,
        "y2": 0.7068518996238708
      },
      {
        "r": 0,
        "text": "This paper focuses on motion prediction for point cloud sequences in the challenging case of deformable 3D objects, such as human body motion.",
        "trunc_text": "This paper focuses on motion prediction for point cloud sequences in the challenging case of deformable 3D objects, such",
        "x1": 1.9783153533935547,
        "x2": 3.440938949584961,
        "y1": 9.316909790039062,
        "y2": -0.27738747000694275
      },
      {
        "r": 0,
        "text": "First, we investigate the challenges caused by deformable shapes and complex motions present in this type of representation, with the ultimate goal of understanding the technical limitations of state-of-the-art models.",
        "trunc_text": "First, we investigate the challenges caused by deformable shapes and complex motions present in this type of representat",
        "x1": 2.037290096282959,
        "x2": 3.3950178623199463,
        "y1": 9.11017894744873,
        "y2": 0.040674418210983276
      },
      {
        "r": 0,
        "text": "From this understanding, we propose an improved architecture for point cloud prediction of deformable 3D objects.",
        "trunc_text": "From this understanding, we propose an improved architecture for point cloud prediction of deformable 3D objects.",
        "x1": 1.8306479454040527,
        "x2": 3.1810474395751953,
        "y1": 9.223361015319824,
        "y2": -0.11800354719161987
      },
      {
        "r": 0,
        "text": "Specifically, to handle deformable shapes, we propose a graph-based approach that learns and exploits the spatial structure of point clouds to extract more representative features.",
        "trunc_text": "Specifically, to handle deformable shapes, we propose a graph-based approach that learns and exploits the spatial struct",
        "x1": 1.8600510358810425,
        "x2": 3.2151098251342773,
        "y1": 9.105001449584961,
        "y2": -0.03257845714688301
      },
      {
        "r": 0,
        "text": "Then we propose a module able to combine the learned features in an adaptative manner according to the point cloud movements.",
        "trunc_text": "Then we propose a module able to combine the learned features in an adaptative manner according to the point cloud movem",
        "x1": 1.8380286693572998,
        "x2": 3.2149159908294678,
        "y1": 9.17324161529541,
        "y2": 0.03192560747265816
      },
      {
        "r": 0,
        "text": "The proposed adaptative module controls the composition of local and global motions for each point, enabling the network to model complex motions in deformable 3D objects more effectively.",
        "trunc_text": "The proposed adaptative module controls the composition of local and global motions for each point, enabling the network",
        "x1": 1.9375622272491455,
        "x2": 3.2924768924713135,
        "y1": 9.219464302062988,
        "y2": 0.004137927200645208
      },
      {
        "r": 0,
        "text": "We tested the proposed method on the following datasets: MNIST moving digits, the Mixamo human bodies motions, JPEG and CWIPC-SXR real-world dynamic bodies.",
        "trunc_text": "We tested the proposed method on the following datasets: MNIST moving digits, the Mixamo human bodies motions, JPEG and ",
        "x1": 2.591223955154419,
        "x2": 4.147466659545898,
        "y1": 9.449841499328613,
        "y2": -0.4861534535884857
      },
      {
        "r": 0,
        "text": "Simulation results demonstrate that our method outperforms the current baseline methods given its improved ability to model complex movements as well as preserve point cloud shape.",
        "trunc_text": "Simulation results demonstrate that our method outperforms the current baseline methods given its improved ability to mo",
        "x1": 2.086608648300171,
        "x2": 3.4212546348571777,
        "y1": 9.289508819580078,
        "y2": -0.28040677309036255
      },
      {
        "r": 0,
        "text": "Furthermore, we demonstrate the generalizability of the proposed framework for dynamic feature learning, by testing the framework for action recognition on the MSRAction3D dataset and achieving results on-par with state-of-the-art methods",
        "trunc_text": "Furthermore, we demonstrate the generalizability of the proposed framework for dynamic feature learning, by testing the ",
        "x1": 2.557800769805908,
        "x2": 3.840721845626831,
        "y1": 8.965229988098145,
        "y2": -0.6399584412574768
      },
      {
        "r": 0,
        "text": "Multimodal image registration is a challenging but essential step for numerous image-guided procedures.",
        "trunc_text": "Multimodal image registration is a challenging but essential step for numerous image-guided procedures.",
        "x1": 1.6912609338760376,
        "x2": 3.151569128036499,
        "y1": 9.136979103088379,
        "y2": 0.03471909463405609
      },
      {
        "r": 0,
        "text": "Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to deal with the appearance discrepancy of anatomical structures between imaging modalities.",
        "trunc_text": "Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to dea",
        "x1": 1.683535099029541,
        "x2": 3.1187708377838135,
        "y1": 9.105918884277344,
        "y2": 0.1888638287782669
      },
      {
        "r": 0,
        "text": "Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to new settings.",
        "trunc_text": "Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to ",
        "x1": 2.5676233768463135,
        "x2": 3.951347827911377,
        "y1": 6.748680591583252,
        "y2": 2.2845659255981445
      },
      {
        "r": 0,
        "text": "We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration.",
        "trunc_text": "We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global regist",
        "x1": 1.754216194152832,
        "x2": 3.2170779705047607,
        "y1": 9.217854499816895,
        "y2": -0.07330513000488281
      },
      {
        "r": 0,
        "text": "We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN) which is inherently differentiable can be trained without registered data.",
        "trunc_text": "We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neura",
        "x1": 2.2016966342926025,
        "x2": 2.4376108646392822,
        "y1": 5.615231513977051,
        "y2": 3.3061599731445312
      },
      {
        "r": 0,
        "text": "Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical settings by replacing the similarity measure with the proposed one.",
        "trunc_text": "Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical ",
        "x1": 3.5996015071868896,
        "x2": 4.25744104385376,
        "y1": 7.153390884399414,
        "y2": 2.7339909076690674
      },
      {
        "r": 0,
        "text": "We make our training code and data publicly available.",
        "trunc_text": "We make our training code and data publicly available.",
        "x1": 7.8996453285217285,
        "x2": 8.301318168640137,
        "y1": 7.057596206665039,
        "y2": 2.798123359680176
      },
      {
        "r": 0,
        "text": "Cross-lingual image captioning is confronted with both cross-lingual and cross-modal challenges for multimedia analysis.",
        "trunc_text": "Cross-lingual image captioning is confronted with both cross-lingual and cross-modal challenges for multimedia analysis.",
        "x1": 3.9700241088867188,
        "x2": 2.414811134338379,
        "y1": 3.355304718017578,
        "y2": 4.989590167999268
      },
      {
        "r": 0,
        "text": "The crucial issue in this task is to model the global and local matching between the image and different languages.",
        "trunc_text": "The crucial issue in this task is to model the global and local matching between the image and different languages.",
        "x1": 4.046250820159912,
        "x2": 2.548478126525879,
        "y1": 3.6492881774902344,
        "y2": 4.680744171142578
      },
      {
        "r": 0,
        "text": "Existing cross-modal embedding methods based on Transformer architecture oversight the local matching between the image region and monolingual words, not to mention in the face of a variety of differentiated languages.",
        "trunc_text": "Existing cross-modal embedding methods based on Transformer architecture oversight the local matching between the image ",
        "x1": 3.8854124546051025,
        "x2": 2.4821932315826416,
        "y1": 3.5149128437042236,
        "y2": 4.828726768493652
      },
      {
        "r": 0,
        "text": "Due to the heterogeneous property of the cross-modal and cross-lingual task, we utilize the heterogeneous network to establish cross-domain relationships and the local correspondences between the image and different languages.",
        "trunc_text": "Due to the heterogeneous property of the cross-modal and cross-lingual task, we utilize the heterogeneous network to est",
        "x1": 4.0490312576293945,
        "x2": 2.487462282180786,
        "y1": 3.4729490280151367,
        "y2": 4.802388668060303
      },
      {
        "r": 0,
        "text": "In this paper, we propose an Embedded Heterogeneous Attention Transformer (EHAT) to build reasoning paths bridging cross-domain for cross-lingual image captioning and integrate into transformer.",
        "trunc_text": "In this paper, we propose an Embedded Heterogeneous Attention Transformer (EHAT) to build reasoning paths bridging cross",
        "x1": 3.9580836296081543,
        "x2": 2.500955581665039,
        "y1": 3.5746548175811768,
        "y2": 4.899405479431152
      },
      {
        "r": 0,
        "text": "The proposed EHAT consists of a Masked Heterogeneous Cross-attention (MHCA), Heterogeneous Attention Reasoning Network (HARN) and Heterogeneous Co-attention (HCA).",
        "trunc_text": "The proposed EHAT consists of a Masked Heterogeneous Cross-attention (MHCA), Heterogeneous Attention Reasoning Network (",
        "x1": 3.9918410778045654,
        "x2": 2.480996608734131,
        "y1": 3.7936320304870605,
        "y2": 4.82160758972168
      },
      {
        "r": 0,
        "text": "HARN as the core network, models and infers cross-domain relationship anchored by vision bounding box representation features to connect two languages word features and learn the heterogeneous maps.",
        "trunc_text": "HARN as the core network, models and infers cross-domain relationship anchored by vision bounding box representation fea",
        "x1": 3.9510133266448975,
        "x2": 2.520916700363159,
        "y1": 3.768399238586426,
        "y2": 4.694563388824463
      },
      {
        "r": 0,
        "text": "MHCA and HCA implement cross-domain integration in the encoder through the special heterogeneous attention and enable single model to generate two language captioning.",
        "trunc_text": "MHCA and HCA implement cross-domain integration in the encoder through the special heterogeneous attention and enable si",
        "x1": 4.001960754394531,
        "x2": 2.510444402694702,
        "y1": 3.546311616897583,
        "y2": 5.035135746002197
      },
      {
        "r": 0,
        "text": "We test on MSCOCO dataset to generate English and Chinese, which are most widely used and have obvious difference between their language families.",
        "trunc_text": "We test on MSCOCO dataset to generate English and Chinese, which are most widely used and have obvious difference betwee",
        "x1": 4.216442584991455,
        "x2": 2.413433790206909,
        "y1": 2.7306690216064453,
        "y2": 6.45314884185791
      },
      {
        "r": 0,
        "text": "Our experiments show that our method even achieve better than advanced monolingual methods.",
        "trunc_text": "Our experiments show that our method even achieve better than advanced monolingual methods.",
        "x1": 4.269265174865723,
        "x2": 2.517759323120117,
        "y1": 2.7944860458374023,
        "y2": 6.655038833618164
      },
      {
        "r": 0,
        "text": "In this paper, we study the Greek wiretappings scandal, which has been revealed in 2022 and attracted a lot of attention by press and citizens.",
        "trunc_text": "In this paper, we study the Greek wiretappings scandal, which has been revealed in 2022 and attracted a lot of attention",
        "x1": 6.852680683135986,
        "x2": 6.177694797515869,
        "y1": 5.515787601470947,
        "y2": 5.890470504760742
      },
      {
        "r": 0,
        "text": "Specifically, we propose a methodology for collecting data and analyzing patterns of online public discussions on Twitter.",
        "trunc_text": "Specifically, we propose a methodology for collecting data and analyzing patterns of online public discussions on Twitte",
        "x1": 5.201093673706055,
        "x2": 4.06983757019043,
        "y1": 7.701130390167236,
        "y2": 6.7314653396606445
      },
      {
        "r": 0,
        "text": "We apply our methodology to the Greek wiretappings use case, and present findings related to the evolution of the discussion over time, its polarization, and the role of the media.",
        "trunc_text": "We apply our methodology to the Greek wiretappings use case, and present findings related to the evolution of the discus",
        "x1": 6.685562610626221,
        "x2": 6.023103713989258,
        "y1": 5.522974014282227,
        "y2": 5.78513765335083
      },
      {
        "r": 0,
        "text": "The methodology can be of wider use and replicated to other topics.",
        "trunc_text": "The methodology can be of wider use and replicated to other topics.",
        "x1": 5.177931308746338,
        "x2": 6.282564163208008,
        "y1": 8.435941696166992,
        "y2": 4.430768966674805
      },
      {
        "r": 0,
        "text": "We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music.",
        "trunc_text": "We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two dat",
        "x1": 2.849273681640625,
        "x2": 0.7482661008834839,
        "y1": 2.0079033374786377,
        "y2": 6.476062297821045
      },
      {
        "r": 0,
        "text": "Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks.",
        "trunc_text": "Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety",
        "x1": 3.0498390197753906,
        "x2": 2.925259828567505,
        "y1": 5.07056999206543,
        "y2": 3.8864145278930664
      },
      {
        "r": 0,
        "text": "At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles.",
        "trunc_text": "At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles.",
        "x1": 2.822638511657715,
        "x2": 0.76996248960495,
        "y1": 2.1291770935058594,
        "y2": 6.303434371948242
      },
      {
        "r": 0,
        "text": "This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles.",
        "trunc_text": "This leads to research questions on whether these models can be used to learn representations for different music cultur",
        "x1": 2.8148269653320312,
        "x2": 0.7109054327011108,
        "y1": 2.1223039627075195,
        "y2": 6.355939865112305
      },
      {
        "r": 0,
        "text": "To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to.  ",
        "trunc_text": "To that end, we leverage transfer learning methods to derive insights about the similarities between the different music",
        "x1": 2.806580066680908,
        "x2": 0.7207145690917969,
        "y1": 2.08398175239563,
        "y2": 6.337890148162842
      },
      {
        "r": 0,
        "text": "Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset.",
        "trunc_text": "Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-",
        "x1": 2.8475770950317383,
        "x2": 0.8954960703849792,
        "y1": 2.3012733459472656,
        "y2": 6.1174092292785645
      },
      {
        "r": 0,
        "text": "Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture.",
        "trunc_text": "Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best ",
        "x1": 2.7872023582458496,
        "x2": 0.7253468036651611,
        "y1": 2.1329708099365234,
        "y2": 6.299222469329834
      },
      {
        "r": 0,
        "text": "The implementation and the trained models are both provided in a public repository.",
        "trunc_text": "The implementation and the trained models are both provided in a public repository.",
        "x1": 7.713784217834473,
        "x2": 8.151910781860352,
        "y1": 7.018916606903076,
        "y2": 2.842766523361206
      },
      {
        "r": 0,
        "text": "Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs).",
        "trunc_text": "Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs).",
        "x1": 5.429067134857178,
        "x2": 3.7302889823913574,
        "y1": 3.7054712772369385,
        "y2": 6.677892684936523
      },
      {
        "r": 0,
        "text": "However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency.",
        "trunc_text": "However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the ",
        "x1": 5.038514614105225,
        "x2": 3.658531665802002,
        "y1": 3.9350497722625732,
        "y2": 5.92626953125
      },
      {
        "r": 0,
        "text": "In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region.",
        "trunc_text": "In this study, we present precise referring instructions that utilize diverse reference representations such as points a",
        "x1": 4.964980125427246,
        "x2": 3.2184324264526367,
        "y1": 3.626098394393921,
        "y2": 5.342134475708008
      },
      {
        "r": 0,
        "text": "This enables MLLMs to focus on the region of interest and achieve finer-grained interaction.",
        "trunc_text": "This enables MLLMs to focus on the region of interest and achieve finer-grained interaction.",
        "x1": 3.4962661266326904,
        "x2": 3.302201747894287,
        "y1": 8.028621673583984,
        "y2": 4.741886138916016
      },
      {
        "r": 0,
        "text": "Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience.",
        "trunc_text": "Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that s",
        "x1": 5.260872840881348,
        "x2": 3.701476812362671,
        "y1": 3.540422201156616,
        "y2": 6.796702861785889
      },
      {
        "r": 0,
        "text": "We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating.",
        "trunc_text": "We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 gen",
        "x1": 5.23279333114624,
        "x2": 4.246975898742676,
        "y1": 3.9764344692230225,
        "y2": 6.103067398071289
      },
      {
        "r": 0,
        "text": "Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction.",
        "trunc_text": "Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction.",
        "x1": 3.8109495639801025,
        "x2": 4.629021644592285,
        "y1": 7.732794284820557,
        "y2": 2.7735812664031982
      },
      {
        "r": 0,
        "text": "Experimental results showcase ChatSpot's promising performance.",
        "trunc_text": "Experimental results showcase ChatSpot's promising performance.",
        "x1": 6.578462600708008,
        "x2": 4.715729713439941,
        "y1": 3.6068687438964844,
        "y2": 6.973701000213623
      },
      {
        "r": 0,
        "text": "To evaluate the proposed framework, we build a dataset consisting of 1414 2-minute video segments annotated with 13 actions and 112 video segments annotated with two engagement levels.",
        "trunc_text": "To evaluate the proposed framework, we build a dataset consisting of 1414 2-minute video segments annotated with 13 acti",
        "x1": 2.4973795413970947,
        "x2": 3.258488655090332,
        "y1": 8.481611251831055,
        "y2": 0.9846346974372864
      },
      {
        "r": 0,
        "text": "In this paper, we propose a novel technique for measuring behavioral engagement through students' actions recognition.",
        "trunc_text": "In this paper, we propose a novel technique for measuring behavioral engagement through students' actions recognition.",
        "x1": 2.8682162761688232,
        "x2": 3.8775227069854736,
        "y1": 8.806241035461426,
        "y2": -0.7932029962539673
      },
      {
        "r": 0,
        "text": "The proposed approach recognizes student actions then predicts the student behavioral engagement level.",
        "trunc_text": "The proposed approach recognizes student actions then predicts the student behavioral engagement level.",
        "x1": 2.927523612976074,
        "x2": 3.892244338989258,
        "y1": 8.811065673828125,
        "y2": -0.8818849325180054
      },
      {
        "r": 0,
        "text": "For student action recognition, we use human skeletons to model student postures and upper body movements.",
        "trunc_text": "For student action recognition, we use human skeletons to model student postures and upper body movements.",
        "x1": 2.60509991645813,
        "x2": 3.8732550144195557,
        "y1": 9.104683876037598,
        "y2": -0.6580865979194641
      },
      {
        "r": 0,
        "text": "To learn the dynamics of student upper body, a 3D-CNN model is used.",
        "trunc_text": "To learn the dynamics of student upper body, a 3D-CNN model is used.",
        "x1": 2.580315113067627,
        "x2": 3.864503860473633,
        "y1": 9.035221099853516,
        "y2": -0.5088149309158325
      },
      {
        "r": 0,
        "text": "The trained 3D-CNN model is used to recognize actions within every 2minute video segment then these actions are used to build a histogram of actions which encodes the student actions and their frequencies.",
        "trunc_text": "The trained 3D-CNN model is used to recognize actions within every 2minute video segment then these actions are used to ",
        "x1": 2.6734182834625244,
        "x2": 3.7766778469085693,
        "y1": 8.868809700012207,
        "y2": -0.6740590929985046
      },
      {
        "r": 0,
        "text": "This histogram is utilized as an input to SVM classifier to classify whether the student is engaged or disengaged.  ",
        "trunc_text": "This histogram is utilized as an input to SVM classifier to classify whether the student is engaged or disengaged.  ",
        "x1": 2.841703176498413,
        "x2": 3.840818405151367,
        "y1": 8.798054695129395,
        "y2": -0.7066394090652466
      },
      {
        "r": 0,
        "text": "Experimental results indicate that student actions can be recognized with top 1 accuracy 83.63% and the proposed framework can capture the average engagement of the class.",
        "trunc_text": "Experimental results indicate that student actions can be recognized with top 1 accuracy 83.63% and the proposed framewo",
        "x1": 2.896606922149658,
        "x2": 3.8858275413513184,
        "y1": 8.820012092590332,
        "y2": -0.7780877947807312
      },
      {
        "r": 0,
        "text": "Point cloud registration is to estimate a transformation to align point clouds collected in different perspectives.",
        "trunc_text": "Point cloud registration is to estimate a transformation to align point clouds collected in different perspectives.",
        "x1": 1.7287594079971313,
        "x2": 3.134178638458252,
        "y1": 9.305550575256348,
        "y2": -0.1969163715839386
      },
      {
        "r": 0,
        "text": "In learning-based point cloud registration, a robust descriptor is vital for high-accuracy registration.",
        "trunc_text": "In learning-based point cloud registration, a robust descriptor is vital for high-accuracy registration.",
        "x1": 1.6625837087631226,
        "x2": 3.1546127796173096,
        "y1": 9.245656967163086,
        "y2": -0.09725093096494675
      },
      {
        "r": 0,
        "text": "However, most methods are susceptible to noise and have poor generalization ability on unseen datasets.",
        "trunc_text": "However, most methods are susceptible to noise and have poor generalization ability on unseen datasets.",
        "x1": 3.522294759750366,
        "x2": 5.0084919929504395,
        "y1": 6.724600791931152,
        "y2": 3.3798413276672363
      },
      {
        "r": 0,
        "text": "Motivated by this, we introduce SphereNet to learn a noise-robust and unseen-general descriptor for point cloud registration.",
        "trunc_text": "Motivated by this, we introduce SphereNet to learn a noise-robust and unseen-general descriptor for point cloud registra",
        "x1": 1.6870787143707275,
        "x2": 3.060865640640259,
        "y1": 9.153977394104004,
        "y2": 0.06924831867218018
      },
      {
        "r": 0,
        "text": "In our method, first, the spheroid generator builds a geometric domain based on spherical voxelization to encode initial features.",
        "trunc_text": "In our method, first, the spheroid generator builds a geometric domain based on spherical voxelization to encode initial",
        "x1": 1.5822254419326782,
        "x2": 3.0670511722564697,
        "y1": 8.867067337036133,
        "y2": 0.41194018721580505
      },
      {
        "r": 0,
        "text": "Then, the spherical interpolation of the sphere is introduced to realize robustness against noise.",
        "trunc_text": "Then, the spherical interpolation of the sphere is introduced to realize robustness against noise.",
        "x1": 1.6757838726043701,
        "x2": 3.011610269546509,
        "y1": 8.919659614562988,
        "y2": 0.3668075203895569
      },
      {
        "r": 0,
        "text": "Finally, a new spherical convolutional neural network with spherical integrity padding completes the extraction of descriptors, which reduces the loss of features and fully captures the geometric features.",
        "trunc_text": "Finally, a new spherical convolutional neural network with spherical integrity padding completes the extraction of descr",
        "x1": 1.5087839365005493,
        "x2": 2.898561477661133,
        "y1": 8.83353328704834,
        "y2": 0.41208937764167786
      },
      {
        "r": 0,
        "text": "To evaluate our methods, a new benchmark 3DMatch-noise with strong noise is introduced.",
        "trunc_text": "To evaluate our methods, a new benchmark 3DMatch-noise with strong noise is introduced.",
        "x1": 1.6726588010787964,
        "x2": 3.0990679264068604,
        "y1": 8.864006042480469,
        "y2": 0.4866643249988556
      },
      {
        "r": 0,
        "text": "Extensive experiments are carried out on both indoor and outdoor datasets.",
        "trunc_text": "Extensive experiments are carried out on both indoor and outdoor datasets.",
        "x1": 4.208052635192871,
        "x2": 5.2165937423706055,
        "y1": 7.616657733917236,
        "y2": 2.558046340942383
      },
      {
        "r": 0,
        "text": "Under high-intensity noise, SphereNet increases the feature matching recall by more than 25 percentage points on 3DMatch-noise.",
        "trunc_text": "Under high-intensity noise, SphereNet increases the feature matching recall by more than 25 percentage points on 3DMatch",
        "x1": 1.6905722618103027,
        "x2": 3.0267574787139893,
        "y1": 8.98552131652832,
        "y2": 0.3933003544807434
      },
      {
        "r": 0,
        "text": "In addition, it sets a new state-of-the-art performance for the 3DMatch and 3DLoMatch benchmarks with 93.5\\% and 75.6\\% registration recall and also has the best generalization ability on unseen datasets.",
        "trunc_text": "In addition, it sets a new state-of-the-art performance for the 3DMatch and 3DLoMatch benchmarks with 93.5\\% and 75.6\\% ",
        "x1": 1.9140024185180664,
        "x2": 3.244074583053589,
        "y1": 8.830806732177734,
        "y2": 0.5434813499450684
      },
      {
        "r": 0,
        "text": "Sequential decision-making under uncertainty is often associated with long feedback delays.",
        "trunc_text": "Sequential decision-making under uncertainty is often associated with long feedback delays.",
        "x1": 3.804896593093872,
        "x2": 3.876189947128296,
        "y1": 5.048036098480225,
        "y2": 4.816310405731201
      },
      {
        "r": 0,
        "text": "Such delays degrade the performance of the learning agent in identifying a subset of arms with the optimal collective reward in the long run.",
        "trunc_text": "Such delays degrade the performance of the learning agent in identifying a subset of arms with the optimal collective re",
        "x1": 4.005458831787109,
        "x2": 3.8846075534820557,
        "y1": 5.011622428894043,
        "y2": 4.903377532958984
      },
      {
        "r": 0,
        "text": "This problem becomes significantly challenging in a non-stationary environment with structural dependencies amongst the reward distributions associated with the arms.",
        "trunc_text": "This problem becomes significantly challenging in a non-stationary environment with structural dependencies amongst the ",
        "x1": 3.970517635345459,
        "x2": 3.908860683441162,
        "y1": 5.075192928314209,
        "y2": 4.847713470458984
      },
      {
        "r": 0,
        "text": "Therefore, besides adapting to delays and environmental changes, learning the causal relations alleviates the adverse effects of feedback delay on the decision-making process.",
        "trunc_text": "Therefore, besides adapting to delays and environmental changes, learning the causal relations alleviates the adverse ef",
        "x1": 3.7205991744995117,
        "x2": 3.813011646270752,
        "y1": 5.083065509796143,
        "y2": 4.649128437042236
      },
      {
        "r": 0,
        "text": "We formalize the described setting as a non-stationary and delayed combinatorial semi-bandit problem with causally related rewards.",
        "trunc_text": "We formalize the described setting as a non-stationary and delayed combinatorial semi-bandit problem with causally relat",
        "x1": 3.9337406158447266,
        "x2": 3.9885354042053223,
        "y1": 5.004024505615234,
        "y2": 4.894299507141113
      },
      {
        "r": 0,
        "text": "We model the causal relations by a directed graph in a stationary structural equation model.",
        "trunc_text": "We model the causal relations by a directed graph in a stationary structural equation model.",
        "x1": 3.481376886367798,
        "x2": 3.3389294147491455,
        "y1": 5.16480016708374,
        "y2": 4.259895324707031
      },
      {
        "r": 0,
        "text": "The agent maximizes the long-term average payoff, defined as a linear function of the base arms' rewards.",
        "trunc_text": "The agent maximizes the long-term average payoff, defined as a linear function of the base arms' rewards.",
        "x1": 4.068691253662109,
        "x2": 3.898979663848877,
        "y1": 5.00710391998291,
        "y2": 4.875491142272949
      },
      {
        "r": 0,
        "text": "We develop a policy that learns the structural dependencies from delayed feedback and utilizes that to optimize the decision-making while adapting to drifts.",
        "trunc_text": "We develop a policy that learns the structural dependencies from delayed feedback and utilizes that to optimize the deci",
        "x1": 3.7797672748565674,
        "x2": 3.8964996337890625,
        "y1": 5.0715155601501465,
        "y2": 4.7812275886535645
      },
      {
        "r": 0,
        "text": "We prove a regret bound for the performance of the proposed algorithm.",
        "trunc_text": "We prove a regret bound for the performance of the proposed algorithm.",
        "x1": 3.804013729095459,
        "x2": 4.697231769561768,
        "y1": 6.795135974884033,
        "y2": 4.033336162567139
      },
      {
        "r": 0,
        "text": "Besides, we evaluate our method via numerical analysis using synthetic and real-world datasets to detect the regions that contribute the most to the spread of Covid-19 in Italy.",
        "trunc_text": "Besides, we evaluate our method via numerical analysis using synthetic and real-world datasets to detect the regions tha",
        "x1": 4.49308967590332,
        "x2": 5.864681720733643,
        "y1": 7.786054611206055,
        "y2": 1.8560234308242798
      },
      {
        "r": 0,
        "text": "In this work, we curate the first aerial thermal near-shore dataset",
        "trunc_text": "In this work, we curate the first aerial thermal near-shore dataset",
        "x1": 0.26780909299850464,
        "x2": 2.21362042427063,
        "y1": 8.996627807617188,
        "y2": -0.18119153380393982
      },
      {
        "r": 0,
        "text": "Code and datasets used in this work will be available at: https://github.com/connorlee77/uav-thermal-water-segmentation.",
        "trunc_text": "Code and datasets used in this work will be available at: https://github.com/connorlee77/uav-thermal-water-segmentation.",
        "x1": 0.3031013011932373,
        "x2": 2.310076951980591,
        "y1": 8.975193977355957,
        "y2": -0.1153559461236
      },
      {
        "r": 0,
        "text": "We present a new method to adapt an RGB-trained water segmentation network to target-domain aerial thermal imagery using online self-supervision by leveraging texture and motion cues as supervisory signals.",
        "trunc_text": "We present a new method to adapt an RGB-trained water segmentation network to target-domain aerial thermal imagery using",
        "x1": 0.3386456370353699,
        "x2": 2.335714340209961,
        "y1": 8.922775268554688,
        "y2": 0.030886078253388405
      },
      {
        "r": 0,
        "text": "This new thermal capability enables current autonomous aerial robots operating in near-shore environments to perform tasks such as visual navigation, bathymetry, and flow tracking at night.",
        "trunc_text": "This new thermal capability enables current autonomous aerial robots operating in near-shore environments to perform tas",
        "x1": 0.4407382607460022,
        "x2": 2.3545150756835938,
        "y1": 9.073634147644043,
        "y2": -0.15903934836387634
      },
      {
        "r": 0,
        "text": "Our method overcomes the problem of scarce and difficult-to-obtain near-shore thermal data that prevents the application of conventional supervised and unsupervised methods.",
        "trunc_text": "Our method overcomes the problem of scarce and difficult-to-obtain near-shore thermal data that prevents the application",
        "x1": 0.2494102418422699,
        "x2": 2.186756134033203,
        "y1": 8.986230850219727,
        "y2": -0.07810024172067642
      },
      {
        "r": 0,
        "text": ", show that our approach outperforms fully-supervised segmentation models trained on limited target-domain thermal data, and demonstrate real-time capabilities onboard an Nvidia Jetson embedded computing platform.",
        "trunc_text": ", show that our approach outperforms fully-supervised segmentation models trained on limited target-domain thermal data,",
        "x1": 0.3004002571105957,
        "x2": 2.3393781185150146,
        "y1": 8.730247497558594,
        "y2": 0.49786806106567383
      },
      {
        "r": 0,
        "text": "Code and datasets used in this work will be available at: https://g",
        "trunc_text": "Code and datasets used in this work will be available at: https://g",
        "x1": 7.352970123291016,
        "x2": 7.817337512969971,
        "y1": 7.764915943145752,
        "y2": 1.9716616868972778
      },
      {
        "r": 0,
        "text": "Facial expression recognition (FER) remains a challenging task due to the ambiguity of expressions.",
        "trunc_text": "Facial expression recognition (FER) remains a challenging task due to the ambiguity of expressions.",
        "x1": 1.5255873203277588,
        "x2": 1.646391749382019,
        "y1": 5.316534519195557,
        "y2": 3.404975414276123
      },
      {
        "r": 0,
        "text": "The derived noisy labels significantly harm the performance in real-world scenarios.",
        "trunc_text": "The derived noisy labels significantly harm the performance in real-world scenarios.",
        "x1": 1.055295705795288,
        "x2": 0.9299046993255615,
        "y1": 4.889992713928223,
        "y2": 3.4932637214660645
      },
      {
        "r": 0,
        "text": "To address this issue, we present a new FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks to mitigate the impact of label noise from two perspectives.",
        "trunc_text": "To address this issue, we present a new FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks to",
        "x1": 1.48393714427948,
        "x2": 1.5358312129974365,
        "y1": 5.325289726257324,
        "y2": 3.3435046672821045
      },
      {
        "r": 0,
        "text": "Firstly, LA-Net uses landmark information to suppress the uncertainty in expression space and constructs the label distribution of each sample by neighborhood aggregation, which in turn improves the quality of training supervision.",
        "trunc_text": "Firstly, LA-Net uses landmark information to suppress the uncertainty in expression space and constructs the label distr",
        "x1": 1.5927664041519165,
        "x2": 1.631884217262268,
        "y1": 5.1876935958862305,
        "y2": 3.529503345489502
      },
      {
        "r": 0,
        "text": "Secondly, the model incorporates landmark information into expression representations using the devised expression-landmark contrastive loss.",
        "trunc_text": "Secondly, the model incorporates landmark information into expression representations using the devised expression-landm",
        "x1": 1.663212776184082,
        "x2": 1.6292662620544434,
        "y1": 5.270634174346924,
        "y2": 3.487344264984131
      },
      {
        "r": 0,
        "text": "The enhanced expression feature extractor can be less susceptible to label noise.",
        "trunc_text": "The enhanced expression feature extractor can be less susceptible to label noise.",
        "x1": 1.3125091791152954,
        "x2": 1.2306900024414062,
        "y1": 5.110040187835693,
        "y2": 3.405165910720825
      },
      {
        "r": 0,
        "text": "Our method can be integrated with any deep neural network for better training supervision without introducing extra inference costs.",
        "trunc_text": "Our method can be integrated with any deep neural network for better training supervision without introducing extra infe",
        "x1": 1.9744082689285278,
        "x2": 2.0412940979003906,
        "y1": 5.1845221519470215,
        "y2": 3.6925048828125
      },
      {
        "r": 0,
        "text": "We conduct extensive experiments on both in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net achieves state-of-the-art performance.",
        "trunc_text": "We conduct extensive experiments on both in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net a",
        "x1": 4.226078033447266,
        "x2": 4.966983795166016,
        "y1": 7.227794170379639,
        "y2": 2.4588043689727783
      },
      {
        "r": 0,
        "text": "The JAZZVAR dataset is a collection of 502 pairs of Variation and Original MIDI segments.",
        "trunc_text": "The JAZZVAR dataset is a collection of 502 pairs of Variation and Original MIDI segments.",
        "x1": 2.769700765609741,
        "x2": 0.6760371923446655,
        "y1": 2.011850357055664,
        "y2": 6.424750804901123
      },
      {
        "r": 0,
        "text": "Jazz pianists often uniquely interpret jazz standards.",
        "trunc_text": "Jazz pianists often uniquely interpret jazz standards.",
        "x1": 2.7401130199432373,
        "x2": 0.6293870806694031,
        "y1": 2.0112857818603516,
        "y2": 6.406692028045654
      },
      {
        "r": 0,
        "text": "Passages from these interpretations can be viewed as sections of variation.",
        "trunc_text": "Passages from these interpretations can be viewed as sections of variation.",
        "x1": 3.364657163619995,
        "x2": 2.4037551879882812,
        "y1": 3.3713226318359375,
        "y2": 5.450159072875977
      },
      {
        "r": 0,
        "text": "We manually extracted such variations from solo jazz piano performances.  ",
        "trunc_text": "We manually extracted such variations from solo jazz piano performances.  ",
        "x1": 2.7383956909179688,
        "x2": 0.6496196985244751,
        "y1": 2.0024025440216064,
        "y2": 6.415919780731201
      },
      {
        "r": 0,
        "text": "Each Variation in the dataset is accompanied by a corresponding Original segment containing the melody and chords from the original jazz standard.",
        "trunc_text": "Each Variation in the dataset is accompanied by a corresponding Original segment containing the melody and chords from t",
        "x1": 2.751633644104004,
        "x2": 0.6359370350837708,
        "y1": 2.0055899620056152,
        "y2": 6.402970314025879
      },
      {
        "r": 0,
        "text": "Our approach differs from many existing jazz datasets in the music information retrieval (MIR) community, which often focus on improvisation sections within jazz performances.",
        "trunc_text": "Our approach differs from many existing jazz datasets in the music information retrieval (MIR) community, which often fo",
        "x1": 2.7656116485595703,
        "x2": 0.6595401167869568,
        "y1": 2.0031237602233887,
        "y2": 6.378526210784912
      },
      {
        "r": 0,
        "text": "In this paper, we outline the curation process for obtaining and sorting the repertoire, the pipeline for creating the Original and Variation pairs, and our analysis of the dataset.",
        "trunc_text": "In this paper, we outline the curation process for obtaining and sorting the repertoire, the pipeline for creating the O",
        "x1": 2.7660410404205322,
        "x2": 0.5745890736579895,
        "y1": 2.0030834674835205,
        "y2": 6.456392765045166
      },
      {
        "r": 0,
        "text": "We also introduce a new generative music task, Music Overpainting, and present a baseline Transformer model trained on the JAZZVAR dataset for this task.",
        "trunc_text": "We also introduce a new generative music task, Music Overpainting, and present a baseline Transformer model trained on t",
        "x1": 2.761766195297241,
        "x2": 0.6847557425498962,
        "y1": 2.092888116836548,
        "y2": 6.326992511749268
      },
      {
        "r": 0,
        "text": "Other potential applications of our dataset include expressive performance analysis and performer identification.",
        "trunc_text": "Other potential applications of our dataset include expressive performance analysis and performer identification.",
        "x1": 4.62082576751709,
        "x2": 5.572425365447998,
        "y1": 6.807800769805908,
        "y2": 2.9761571884155273
      },
      {
        "r": 0,
        "text": "The plant community composition is an essential indicator of environmental changes and is, for this reason, usually analyzed in ecological field studies in terms of the so-called plant cover.",
        "trunc_text": "The plant community composition is an essential indicator of environmental changes and is, for this reason, usually anal",
        "x1": 1.2547568082809448,
        "x2": 5.140260696411133,
        "y1": 7.63374137878418,
        "y2": 1.5245883464813232
      },
      {
        "r": 0,
        "text": "The manual acquisition of this kind of data is time-consuming, laborious, and prone to human error.",
        "trunc_text": "The manual acquisition of this kind of data is time-consuming, laborious, and prone to human error.",
        "x1": 5.703835964202881,
        "x2": 6.281440258026123,
        "y1": 6.895232677459717,
        "y2": 3.050266742706299
      },
      {
        "r": 0,
        "text": "Automated camera systems can collect high-resolution images of the surveyed vegetation plots at a high frequency.",
        "trunc_text": "Automated camera systems can collect high-resolution images of the surveyed vegetation plots at a high frequency.",
        "x1": 1.2999433279037476,
        "x2": 4.570896625518799,
        "y1": 7.563551425933838,
        "y2": 1.3626713752746582
      },
      {
        "r": 0,
        "text": "In combination with subsequent algorithmic analysis, it is possible to objectively extract information on plant community composition quickly and with little human effort.",
        "trunc_text": "In combination with subsequent algorithmic analysis, it is possible to objectively extract information on plant communit",
        "x1": 1.2878429889678955,
        "x2": 5.181869983673096,
        "y1": 7.6418609619140625,
        "y2": 1.5753165483474731
      },
      {
        "r": 0,
        "text": "An automated camera system can easily collect the large amounts of image data necessary to train a Deep Learning system for automatic analysis.",
        "trunc_text": "An automated camera system can easily collect the large amounts of image data necessary to train a Deep Learning system ",
        "x1": 1.8998066186904907,
        "x2": 3.3909049034118652,
        "y1": 7.053121089935303,
        "y2": 1.8503327369689941
      },
      {
        "r": 0,
        "text": "However, due to the amount of work required to annotate vegetation images with plant cover data, only few labeled samples are available.",
        "trunc_text": "However, due to the amount of work required to annotate vegetation images with plant cover data, only few labeled sample",
        "x1": 1.2929986715316772,
        "x2": 4.792065143585205,
        "y1": 7.611565113067627,
        "y2": 1.4203345775604248
      },
      {
        "r": 0,
        "text": "As automated camera systems can collect many pictures without labels, we introduce an approach to interpolate the sparse labels in the collected vegetation plot time series down to the intermediate dense and unlabeled images to artificially increase our training dataset to seven times its original size.",
        "trunc_text": "As automated camera systems can collect many pictures without labels, we introduce an approach to interpolate the sparse",
        "x1": 1.5526400804519653,
        "x2": 4.3843512535095215,
        "y1": 7.478700637817383,
        "y2": 1.437773585319519
      },
      {
        "r": 0,
        "text": "Moreover, we introduce a new method we call Monte-Carlo Cropping.",
        "trunc_text": "Moreover, we introduce a new method we call Monte-Carlo Cropping.",
        "x1": 1.8642522096633911,
        "x2": 3.307478666305542,
        "y1": 7.28705358505249,
        "y2": 1.7706246376037598
      },
      {
        "r": 0,
        "text": "This approach trains on a collection of cropped parts of the training images to deal with high-resolution images efficiently, implicitly augment the training images, and speed up training.",
        "trunc_text": "This approach trains on a collection of cropped parts of the training images to deal with high-resolution images efficie",
        "x1": 1.953166127204895,
        "x2": 3.3556768894195557,
        "y1": 7.255824089050293,
        "y2": 1.8838942050933838
      },
      {
        "r": 0,
        "text": "We evaluate both approaches on a plant cover dataset containing images of herbaceous plant communities and find that our methods lead to improvements in the species, community, and segmentation metrics investigated.",
        "trunc_text": "We evaluate both approaches on a plant cover dataset containing images of herbaceous plant communities and find that our",
        "x1": 1.2586880922317505,
        "x2": 4.931881427764893,
        "y1": 7.609460830688477,
        "y2": 1.4548051357269287
      },
      {
        "r": 0,
        "text": "Existing image editing tools, while powerful, typically disregard the underlying 3D geometry from which the image is projected.",
        "trunc_text": "Existing image editing tools, while powerful, typically disregard the underlying 3D geometry from which the image is pro",
        "x1": 1.6263906955718994,
        "x2": 3.516664981842041,
        "y1": 8.787945747375488,
        "y2": 0.5366443395614624
      },
      {
        "r": 0,
        "text": "As a result, edits made using these tools may become detached from the geometry and lighting conditions that are at the foundation of the image formation process.",
        "trunc_text": "As a result, edits made using these tools may become detached from the geometry and lighting conditions that are at the ",
        "x1": 1.74207603931427,
        "x2": 3.5779364109039307,
        "y1": 8.695252418518066,
        "y2": 0.6947761178016663
      },
      {
        "r": 0,
        "text": "In this work, we formulate the newt ask of language-guided 3D-aware editing, where objects in an image should be edited according to a language instruction in context of the underlying 3D scene.  ",
        "trunc_text": "In this work, we formulate the newt ask of language-guided 3D-aware editing, where objects in an image should be edited ",
        "x1": 2.06296443939209,
        "x2": 3.594954252243042,
        "y1": 8.699825286865234,
        "y2": 0.84123295545578
      },
      {
        "r": 0,
        "text": "Each example consists of an input image, editing instruction in language, and the edited image.",
        "trunc_text": "Each example consists of an input image, editing instruction in language, and the edited image.",
        "x1": 4.214249610900879,
        "x2": 2.824476480484009,
        "y1": 3.7397103309631348,
        "y2": 4.781581401824951
      },
      {
        "r": 0,
        "text": "We also introduce 3DIT : single and multi-task models for four editing tasks.",
        "trunc_text": "We also introduce 3DIT : single and multi-task models for four editing tasks.",
        "x1": 2.298631191253662,
        "x2": 3.6848435401916504,
        "y1": 8.775456428527832,
        "y2": 0.8643245697021484
      },
      {
        "r": 0,
        "text": "Our models show impressive abilities to understand the 3D composition of entire scenes, factoring in surrounding objects, surfaces, lighting conditions, shadows, and physically-plausible object configurations.",
        "trunc_text": "Our models show impressive abilities to understand the 3D composition of entire scenes, factoring in surrounding objects",
        "x1": 2.02075457572937,
        "x2": 3.679434299468994,
        "y1": 8.953666687011719,
        "y2": 0.496593177318573
      },
      {
        "r": 0,
        "text": "Surprisingly, training on only synthetic scenes from OBJECT, editing capabilities of 3DIT generalize to real-world images.",
        "trunc_text": "Surprisingly, training on only synthetic scenes from OBJECT, editing capabilities of 3DIT generalize to real-world image",
        "x1": 2.002535343170166,
        "x2": 3.61905574798584,
        "y1": 8.547934532165527,
        "y2": 0.9507827162742615
      },
      {
        "r": 0,
        "text": "A finely annotated segmentation dataset of approximately 10,000 consec-utive frames recorded during surgery is constructed for the first time for this field, addressing the problem of semantic segmentation.",
        "trunc_text": "A finely annotated segmentation dataset of approximately 10,000 consec-utive frames recorded during surgery is construct",
        "x1": 0.8356857299804688,
        "x2": 2.6360931396484375,
        "y1": 7.024962902069092,
        "y2": 1.3704533576965332
      },
      {
        "r": 0,
        "text": "Endoscopic surgery is currently an important treatment method in the field of spinal surgery and avoiding damage to the spinal nerves through video guidance is a key challenge.",
        "trunc_text": "Endoscopic surgery is currently an important treatment method in the field of spinal surgery and avoiding damage to the ",
        "x1": 1.0151793956756592,
        "x2": 2.954510450363159,
        "y1": 6.821726322174072,
        "y2": 1.399590253829956
      },
      {
        "r": 0,
        "text": "This paper presents the first real-time segmentation method for spinal nerves in endoscopic surgery, which provides crucial navigational information for surgeons.  ",
        "trunc_text": "This paper presents the first real-time segmentation method for spinal nerves in endoscopic surgery, which provides cruc",
        "x1": 0.932152509689331,
        "x2": 2.7942771911621094,
        "y1": 6.744690418243408,
        "y2": 1.5376042127609253
      },
      {
        "r": 0,
        "text": "Based on this dataset, we propose FUnet (Frame-Unet), which achieves state-of-the-art performance by utilizing inter-frame information and self-attention mechanisms.",
        "trunc_text": "Based on this dataset, we propose FUnet (Frame-Unet), which achieves state-of-the-art performance by utilizing inter-fra",
        "x1": 2.607231378555298,
        "x2": 4.071783065795898,
        "y1": 7.791486740112305,
        "y2": 2.2566964626312256
      },
      {
        "r": 0,
        "text": "We also conduct extended exper-iments on a similar polyp endoscopy video dataset and show that the model has good generalization ability with advantageous performance.",
        "trunc_text": "We also conduct extended exper-iments on a similar polyp endoscopy video dataset and show that the model has good genera",
        "x1": 1.7499260902404785,
        "x2": 3.0743656158447266,
        "y1": 7.73898983001709,
        "y2": 1.2388287782669067
      },
      {
        "r": 0,
        "text": "The dataset and code of this work are presented at: https://github.com/zzzzzzpc/FUnet .",
        "trunc_text": "The dataset and code of this work are presented at: https://github.com/zzzzzzpc/FUnet .",
        "x1": 7.167586803436279,
        "x2": 7.61766242980957,
        "y1": 7.97251033782959,
        "y2": 1.7927072048187256
      },
      {
        "r": 0,
        "text": "Recent advances in deep learning have significantly improved the performance of various computer vision applications.",
        "trunc_text": "Recent advances in deep learning have significantly improved the performance of various computer vision applications.",
        "x1": 2.104736089706421,
        "x2": 2.732355833053589,
        "y1": 6.316733360290527,
        "y2": 2.68656849861145
      },
      {
        "r": 0,
        "text": "However, discovering novel categories in an incremental learning scenario remains a challenging problem due to the lack of prior knowledge about the number and nature of new categories.",
        "trunc_text": "However, discovering novel categories in an incremental learning scenario remains a challenging problem due to the lack ",
        "x1": 2.3607406616210938,
        "x2": 2.4939444065093994,
        "y1": 5.278645038604736,
        "y2": 3.6245269775390625
      },
      {
        "r": 0,
        "text": "Existing methods for novel category discovery are limited by their reliance on labeled datasets and prior knowledge about the number of novel categories and the proportion of novel samples in the batch.",
        "trunc_text": "Existing methods for novel category discovery are limited by their reliance on labeled datasets and prior knowledge abou",
        "x1": 2.1910014152526855,
        "x2": 2.365283966064453,
        "y1": 5.231783390045166,
        "y2": 3.5691089630126953
      },
      {
        "r": 0,
        "text": "To address the limitations and more accurately reflect real-world scenarios, in this paper, we propose a novel unsupervised class incremental learning approach for discovering novel categories on unlabeled sets without prior knowledge.",
        "trunc_text": "To address the limitations and more accurately reflect real-world scenarios, in this paper, we propose a novel unsupervi",
        "x1": 2.2140724658966064,
        "x2": 2.3597447872161865,
        "y1": 5.195850849151611,
        "y2": 3.6179251670837402
      },
      {
        "r": 0,
        "text": "The proposed method fine-tunes the feature extractor and proxy anchors on labeled sets, then splits samples into old and novel categories and clusters on the unlabeled dataset.",
        "trunc_text": "The proposed method fine-tunes the feature extractor and proxy anchors on labeled sets, then splits samples into old and",
        "x1": 1.7846311330795288,
        "x2": 1.917237639427185,
        "y1": 5.0264434814453125,
        "y2": 3.5035877227783203
      },
      {
        "r": 0,
        "text": "Furthermore, the proxy anchors-based exemplar generates representative category vectors to mitigate catastrophic forgetting.",
        "trunc_text": "Furthermore, the proxy anchors-based exemplar generates representative category vectors to mitigate catastrophic forgett",
        "x1": 2.8245017528533936,
        "x2": 2.8853423595428467,
        "y1": 5.480306625366211,
        "y2": 3.8680272102355957
      },
      {
        "r": 0,
        "text": "Self-supervised learning (SSL) for clinical time series data has received significant attention in recent literature, since these data are highly rich and provide important information about a patient's physiological state.",
        "trunc_text": "Self-supervised learning (SSL) for clinical time series data has received significant attention in recent literature, si",
        "x1": 3.366694211959839,
        "x2": 4.181117534637451,
        "y1": 7.108234882354736,
        "y2": 2.4621598720550537
      },
      {
        "r": 0,
        "text": "However, most existing SSL methods for clinical time series are limited in that they are designed for unimodal time series, such as a sequence of structured features (e.g., lab values and vitals signs) or an individual high-dimensional physiological signal (e.g., an electrocardiogram).",
        "trunc_text": "However, most existing SSL methods for clinical time series are limited in that they are designed for unimodal time seri",
        "x1": 3.4621219635009766,
        "x2": 4.143274784088135,
        "y1": 7.196358680725098,
        "y2": 2.461991310119629
      },
      {
        "r": 0,
        "text": "These existing methods cannot be readily extended to model time series that exhibit multimodality, with structured features and high-dimensional data being recorded at each timestep in the sequence.",
        "trunc_text": "These existing methods cannot be readily extended to model time series that exhibit multimodality, with structured featu",
        "x1": 3.214801788330078,
        "x2": 3.871281862258911,
        "y1": 6.886262893676758,
        "y2": 2.6786656379699707
      },
      {
        "r": 0,
        "text": "In this work, we address this gap and propose a new SSL method -- Sequential Multi-Dimensional SSL -- where a SSL loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence in order to better capture information at both scales.",
        "trunc_text": "In this work, we address this gap and propose a new SSL method -- Sequential Multi-Dimensional SSL -- where a SSL loss i",
        "x1": 5.790951728820801,
        "x2": 1.621352195739746,
        "y1": 5.527505874633789,
        "y2": 2.921999216079712
      },
      {
        "r": 0,
        "text": "Our strategy is agnostic to the specific form of loss function used at each level -- it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg.",
        "trunc_text": "Our strategy is agnostic to the specific form of loss function used at each level -- it can be contrastive, as in SimCLR",
        "x1": 2.850344657897949,
        "x2": 3.0084574222564697,
        "y1": 5.795543670654297,
        "y2": 3.720754623413086
      },
      {
        "r": 0,
        "text": "Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.",
        "trunc_text": "Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves pe",
        "x1": 4.279532432556152,
        "x2": 4.246452808380127,
        "y1": 5.40595006942749,
        "y2": 4.522185325622559
      },
      {
        "r": 0,
        "text": "Human motion generation aims to generate natural human pose sequences and shows immense potential for real-world applications.",
        "trunc_text": "Human motion generation aims to generate natural human pose sequences and shows immense potential for real-world applica",
        "x1": 2.666842460632324,
        "x2": 4.202852725982666,
        "y1": 9.593814849853516,
        "y2": -0.624938428401947
      },
      {
        "r": 0,
        "text": "Most research within this field focuses on generating human motions based on conditional signals, such as text, audio, and scene contexts.",
        "trunc_text": "Most research within this field focuses on generating human motions based on conditional signals, such as text, audio, a",
        "x1": 2.717651844024658,
        "x2": 4.263990879058838,
        "y1": 9.637211799621582,
        "y2": -0.6734068989753723
      },
      {
        "r": 0,
        "text": "While significant advancements have been made in recent years, the task continues to pose challenges due to the intricate nature of human motion and its implicit relationship with conditional signals.",
        "trunc_text": "While significant advancements have been made in recent years, the task continues to pose challenges due to the intricat",
        "x1": 2.8613007068634033,
        "x2": 4.449643611907959,
        "y1": 9.521510124206543,
        "y2": -0.6451390981674194
      },
      {
        "r": 0,
        "text": "In this survey, we present a comprehensive literature review of human motion generation, which, to the best of our knowledge, is the first of its kind in this field.",
        "trunc_text": "In this survey, we present a comprehensive literature review of human motion generation, which, to the best of our knowl",
        "x1": 2.7010977268218994,
        "x2": 4.218376159667969,
        "y1": 9.616804122924805,
        "y2": -0.6507754921913147
      },
      {
        "r": 0,
        "text": "We begin by introducing the background of human motion and generative models, followed by an examination of representative methods for three mainstream sub-tasks: text-conditioned, audio-conditioned, and scene-conditioned human motion generation.",
        "trunc_text": "We begin by introducing the background of human motion and generative models, followed by an examination of representati",
        "x1": 2.669661521911621,
        "x2": 4.216127395629883,
        "y1": 9.622925758361816,
        "y2": -0.638776957988739
      },
      {
        "r": 0,
        "text": "Lastly, we discuss open problems and outline potential future research directions.",
        "trunc_text": "Lastly, we discuss open problems and outline potential future research directions.",
        "x1": 5.045174598693848,
        "x2": 6.342433452606201,
        "y1": 8.448265075683594,
        "y2": 4.172346591949463
      },
      {
        "r": 0,
        "text": "We hope that this survey could provide the community with a comprehensive glimpse of this rapidly evolving field and inspire novel ideas that address the outstanding challenges.",
        "trunc_text": "We hope that this survey could provide the community with a comprehensive glimpse of this rapidly evolving field and ins",
        "x1": 5.0091681480407715,
        "x2": 6.0439934730529785,
        "y1": 7.944122791290283,
        "y2": 2.785902261734009
      },
      {
        "r": 0,
        "text": "Having efficient testing strategies is a core challenge that needs to be overcome for the release of automated driving.",
        "trunc_text": "Having efficient testing strategies is a core challenge that needs to be overcome for the release of automated driving.",
        "x1": 3.8002054691314697,
        "x2": 5.327116966247559,
        "y1": 9.939008712768555,
        "y2": 0.04213874042034149
      },
      {
        "r": 0,
        "text": "This necessitates clear requirements as well as suitable methods for testing.",
        "trunc_text": "This necessitates clear requirements as well as suitable methods for testing.",
        "x1": 5.240859031677246,
        "x2": 6.582093238830566,
        "y1": 9.035848617553711,
        "y2": 4.900697708129883
      },
      {
        "r": 0,
        "text": "In this work, the requirements for perception modules are considered with respect to relevance.",
        "trunc_text": "In this work, the requirements for perception modules are considered with respect to relevance.",
        "x1": 3.9766056537628174,
        "x2": 5.3311686515808105,
        "y1": 8.995159149169922,
        "y2": 0.6421670317649841
      },
      {
        "r": 0,
        "text": "The concept of relevance currently remains insufficiently defined and specified.",
        "trunc_text": "The concept of relevance currently remains insufficiently defined and specified.",
        "x1": 4.143317222595215,
        "x2": 5.441493034362793,
        "y1": 8.953912734985352,
        "y2": 0.6650429964065552
      },
      {
        "r": 0,
        "text": "In this paper, we propose a novel methodology to overcome this challenge by exemplary application to collision safety in the highway domain.",
        "trunc_text": "In this paper, we propose a novel methodology to overcome this challenge by exemplary application to collision safety in",
        "x1": 3.574402093887329,
        "x2": 5.271669387817383,
        "y1": 9.939140319824219,
        "y2": 0.08014880865812302
      },
      {
        "r": 0,
        "text": "Using this general system and use case specification, a corresponding concept for relevance is derived.",
        "trunc_text": "Using this general system and use case specification, a corresponding concept for relevance is derived.",
        "x1": 4.224771499633789,
        "x2": 5.503273010253906,
        "y1": 8.933782577514648,
        "y2": 0.7551108598709106
      },
      {
        "r": 0,
        "text": "Irrelevant objects are thus defined as objects which do not limit the set of safe actions available to the ego vehicle under consideration of all uncertainties.",
        "trunc_text": "Irrelevant objects are thus defined as objects which do not limit the set of safe actions available to the ego vehicle u",
        "x1": 3.968332290649414,
        "x2": 5.419186592102051,
        "y1": 9.115918159484863,
        "y2": 0.5454463362693787
      },
      {
        "r": 0,
        "text": "As an initial step, the use case is decomposed into functional scenarios with respect to collision relevance.",
        "trunc_text": "As an initial step, the use case is decomposed into functional scenarios with respect to collision relevance.",
        "x1": 4.209129333496094,
        "x2": 5.480504989624023,
        "y1": 8.949647903442383,
        "y2": 0.7121300101280212
      },
      {
        "r": 0,
        "text": "For each functional scenario, possible actions of both the ego vehicle and any other dynamic object are formalized as equations.",
        "trunc_text": "For each functional scenario, possible actions of both the ego vehicle and any other dynamic object are formalized as eq",
        "x1": 3.9778542518615723,
        "x2": 5.436192512512207,
        "y1": 9.171446800231934,
        "y2": 0.5054675936698914
      },
      {
        "r": 0,
        "text": "This set of possible actions is constrained by traffic rules, yielding relevance criteria.",
        "trunc_text": "This set of possible actions is constrained by traffic rules, yielding relevance criteria.",
        "x1": 4.153671741485596,
        "x2": 5.504000663757324,
        "y1": 8.997879028320312,
        "y2": 0.6673425436019897
      },
      {
        "r": 0,
        "text": "As a result, we present a conservative estimation which dynamic objects are relevant for perception and need to be considered for a complete evaluation.",
        "trunc_text": "As a result, we present a conservative estimation which dynamic objects are relevant for perception and need to be consi",
        "x1": 3.472557306289673,
        "x2": 4.7593159675598145,
        "y1": 8.89557933807373,
        "y2": 0.6264352798461914
      },
      {
        "r": 0,
        "text": "The estimation provides requirements which are applicable for offline testing and validation of perception components.",
        "trunc_text": "The estimation provides requirements which are applicable for offline testing and validation of perception components.",
        "x1": 3.747079849243164,
        "x2": 4.985555648803711,
        "y1": 8.963587760925293,
        "y2": 0.6298995018005371
      },
      {
        "r": 0,
        "text": "A visualization is presented for examples from the highD dataset, showing the plausibility of the results.",
        "trunc_text": "A visualization is presented for examples from the highD dataset, showing the plausibility of the results.",
        "x1": 4.331534385681152,
        "x2": 5.815240859985352,
        "y1": 8.057012557983398,
        "y2": 2.212592124938965
      },
      {
        "r": 0,
        "text": "Finally, a possibility for a future validation of the presented relevance concept is outlined.",
        "trunc_text": "Finally, a possibility for a future validation of the presented relevance concept is outlined.",
        "x1": 4.275547981262207,
        "x2": 5.472744464874268,
        "y1": 8.914665222167969,
        "y2": 0.7115567922592163
      },
      {
        "r": 0,
        "text": "In a conventional Speech emotion recognition (SER) task, a classifier for a given language is trained on a pre-existing dataset for that same language.",
        "trunc_text": "In a conventional Speech emotion recognition (SER) task, a classifier for a given language is trained on a pre-existing ",
        "x1": 4.905487060546875,
        "x2": 2.977177381515503,
        "y1": 2.606362819671631,
        "y2": 6.9860453605651855
      },
      {
        "r": 0,
        "text": "However, where training data for a language does not exist, data from other languages can be used instead.",
        "trunc_text": "However, where training data for a language does not exist, data from other languages can be used instead.",
        "x1": 4.424870014190674,
        "x2": 2.717484712600708,
        "y1": 2.918844223022461,
        "y2": 6.570581436157227
      },
      {
        "r": 0,
        "text": "We experiment with cross-lingual and multilingual SER, working with Amharic, English, German and URDU.",
        "trunc_text": "We experiment with cross-lingual and multilingual SER, working with Amharic, English, German and URDU.",
        "x1": 4.2901530265808105,
        "x2": 2.438265085220337,
        "y1": 2.6445956230163574,
        "y2": 6.745308876037598
      },
      {
        "r": 0,
        "text": "We followed previous research in mapping labels for all datasets to just two classes, positive and negative.",
        "trunc_text": "We followed previous research in mapping labels for all datasets to just two classes, positive and negative.",
        "x1": 1.2667936086654663,
        "x2": 0.9016814827919006,
        "y1": 4.368219375610352,
        "y2": 3.9529552459716797
      },
      {
        "r": 0,
        "text": "Thus we can compare performance on different languages directly, and combine languages for training and testing.",
        "trunc_text": "Thus we can compare performance on different languages directly, and combine languages for training and testing.",
        "x1": 4.407656669616699,
        "x2": 2.5586180686950684,
        "y1": 2.7408463954925537,
        "y2": 6.765918731689453
      },
      {
        "r": 0,
        "text": "In Experiment 1, monolingual SER trials were carried out using three classifiers, AlexNet, VGGE (a proposed variant of VGG), and ResNet50.",
        "trunc_text": "In Experiment 1, monolingual SER trials were carried out using three classifiers, AlexNet, VGGE (a proposed variant of V",
        "x1": 4.329977035522461,
        "x2": 2.554617404937744,
        "y1": 2.667567729949951,
        "y2": 6.789125919342041
      },
      {
        "r": 0,
        "text": "Results averaged for the three models were very similar for ASED and RAVDESS, suggesting that Amharic and English SER are equally difficult.",
        "trunc_text": "Results averaged for the three models were very similar for ASED and RAVDESS, suggesting that Amharic and English SER ar",
        "x1": 4.420401573181152,
        "x2": 2.46859073638916,
        "y1": 2.5413382053375244,
        "y2": 6.882235527038574
      },
      {
        "r": 0,
        "text": "Similarly, German SER is more difficult, and Urdu SER is easier.",
        "trunc_text": "Similarly, German SER is more difficult, and Urdu SER is easier.",
        "x1": 4.312516689300537,
        "x2": 2.4576098918914795,
        "y1": 2.5876309871673584,
        "y2": 6.7867584228515625
      },
      {
        "r": 0,
        "text": "In Experiment 2, we trained on one language and tested on another, in both directions for each pair:",
        "trunc_text": "In Experiment 2, we trained on one language and tested on another, in both directions for each pair:",
        "x1": 4.402694225311279,
        "x2": 2.5266008377075195,
        "y1": 2.699291229248047,
        "y2": 6.788720607757568
      },
      {
        "r": 0,
        "text": "Amharic<->German, Amharic<->English, and Amharic<->Urdu.",
        "trunc_text": "Amharic<->German, Amharic<->English, and Amharic<->Urdu.",
        "x1": 4.315546035766602,
        "x2": 2.4423723220825195,
        "y1": 2.5307745933532715,
        "y2": 6.820879936218262
      },
      {
        "r": 0,
        "text": "Results with Amharic as target suggested that using English or German as source will give the best result.",
        "trunc_text": "Results with Amharic as target suggested that using English or German as source will give the best result.",
        "x1": 4.351926803588867,
        "x2": 2.4249391555786133,
        "y1": 2.5295493602752686,
        "y2": 6.866894245147705
      },
      {
        "r": 0,
        "text": "In Experiment 3, we trained on several non-Amharic languages and then tested on Amharic.",
        "trunc_text": "In Experiment 3, we trained on several non-Amharic languages and then tested on Amharic.",
        "x1": 4.3958892822265625,
        "x2": 2.468531370162964,
        "y1": 2.5849356651306152,
        "y2": 6.819216251373291
      },
      {
        "r": 0,
        "text": "The best accuracy obtained was several percent greater than the best accuracy in Experiment 2, suggesting that a better result can be obtained when using two or three non-Amharic languages for training than when using just one non-Amharic language.",
        "trunc_text": "The best accuracy obtained was several percent greater than the best accuracy in Experiment 2, suggesting that a better ",
        "x1": 4.340633392333984,
        "x2": 2.4908218383789062,
        "y1": 2.60319185256958,
        "y2": 6.856110095977783
      },
      {
        "r": 0,
        "text": "Overall, the results suggest that cross-lingual and multilingual training can be an effective strategy for training a SER classifier when resources for a language are scarce.",
        "trunc_text": "Overall, the results suggest that cross-lingual and multilingual training can be an effective strategy for training a SE",
        "x1": 4.34004545211792,
        "x2": 2.515136957168579,
        "y1": 2.72590708732605,
        "y2": 6.726593494415283
      },
      {
        "r": 0,
        "text": "In this paper, we first establish a large-scale audio-visual quality assessment dataset for omnidirectional videos, which includes 375 distorted omnidirectional audio-visual (A/V) sequences generated from 15 high-quality pristine omnidirectional A/V contents",
        "trunc_text": "In this paper, we first establish a large-scale audio-visual quality assessment dataset for omnidirectional videos, whic",
        "x1": 0.20324283838272095,
        "x2": 1.9352264404296875,
        "y1": 8.281805992126465,
        "y2": 0.6347479820251465
      },
      {
        "r": 0,
        "text": "Omnidirectional videos (ODVs) play an increasingly important role in the application fields of medical, education, advertising, tourism, etc.",
        "trunc_text": "Omnidirectional videos (ODVs) play an increasingly important role in the application fields of medical, education, adver",
        "x1": 0.2084692269563675,
        "x2": 1.8924951553344727,
        "y1": 8.261967658996582,
        "y2": 0.6322329044342041
      },
      {
        "r": 0,
        "text": "Assessing the quality of ODVs is significant for service-providers to improve the user's Quality of Experience (QoE).",
        "trunc_text": "Assessing the quality of ODVs is significant for service-providers to improve the user's Quality of Experience (QoE).",
        "x1": 0.19768370687961578,
        "x2": 1.8331388235092163,
        "y1": 8.221905708312988,
        "y2": 0.6343514323234558
      },
      {
        "r": 0,
        "text": "However, most existing quality assessment studies for ODVs only focus on the visual distortions of videos, while ignoring that the overall QoE also depends on the accompanying audio signals.",
        "trunc_text": "However, most existing quality assessment studies for ODVs only focus on the visual distortions of videos, while ignorin",
        "x1": 0.17891226708889008,
        "x2": 1.8480221033096313,
        "y1": 8.271754264831543,
        "y2": 0.624634325504303
      },
      {
        "r": 0,
        "text": ", and the corresponding perceptual audio-visual quality scores.",
        "trunc_text": ", and the corresponding perceptual audio-visual quality scores.",
        "x1": 0.1770005226135254,
        "x2": 1.864586353302002,
        "y1": 8.261510848999023,
        "y2": 0.6490064263343811
      },
      {
        "r": 0,
        "text": "Then, we design three baseline methods for full-reference omnidirectional audio-visual quality assessment (OAVQA), which combine existing state-of-the-art single-mode audio and video QA models via multimodal fusion strategies.",
        "trunc_text": "Then, we design three baseline methods for full-reference omnidirectional audio-visual quality assessment (OAVQA), which",
        "x1": 0.11649463325738907,
        "x2": 1.8566983938217163,
        "y1": 8.31056022644043,
        "y2": 0.634585976600647
      },
      {
        "r": 0,
        "text": "We validate the effectiveness of the A/V multimodal fusion method for OAVQA on our dataset, which provides a new benchmark for omnidirectional QoE evaluation.",
        "trunc_text": "We validate the effectiveness of the A/V multimodal fusion method for OAVQA on our dataset, which provides a new benchma",
        "x1": 0.180777907371521,
        "x2": 1.8709574937820435,
        "y1": 8.255463600158691,
        "y2": 0.6591119766235352
      },
      {
        "r": 0,
        "text": "With the increasing amount of spatial-temporal~(ST) ocean data, numerous spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, e.g., climate forecasting and disaster warning.",
        "trunc_text": "With the increasing amount of spatial-temporal~(ST) ocean data, numerous spatial-temporal data mining (STDM) studies hav",
        "x1": -0.30651748180389404,
        "x2": 1.7278683185577393,
        "y1": 9.26257610321045,
        "y2": -0.39250677824020386
      },
      {
        "r": 0,
        "text": "Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated with some unique characteristics, e.g., diverse regionality and high sparsity.",
        "trunc_text": "Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated with some unique characteristics, ",
        "x1": -0.3160945177078247,
        "x2": 1.7126930952072144,
        "y1": 9.267380714416504,
        "y2": -0.40852561593055725
      },
      {
        "r": 0,
        "text": "These characteristics make it difficult to design and train STDM models.",
        "trunc_text": "These characteristics make it difficult to design and train STDM models.",
        "x1": -0.32511454820632935,
        "x2": 1.7718274593353271,
        "y1": 9.291497230529785,
        "y2": -0.37020817399024963
      },
      {
        "r": 0,
        "text": "Unfortunately, an overview of these studies is still missing, hindering computer scientists to identify the research issues in ocean while discouraging researchers in ocean science from applying advanced STDM techniques.",
        "trunc_text": "Unfortunately, an overview of these studies is still missing, hindering computer scientists to identify the research iss",
        "x1": -0.3170706331729889,
        "x2": 1.7210804224014282,
        "y1": 9.278544425964355,
        "y2": -0.40495142340660095
      },
      {
        "r": 0,
        "text": "To remedy this situation, we provide a comprehensive survey to summarize existing STDM studies in ocean.",
        "trunc_text": "To remedy this situation, we provide a comprehensive survey to summarize existing STDM studies in ocean.",
        "x1": -0.31946054100990295,
        "x2": 1.7268726825714111,
        "y1": 9.260191917419434,
        "y2": -0.39879274368286133
      },
      {
        "r": 0,
        "text": "Then, typical ST ocean data quality enhancement techniques are discussed.",
        "trunc_text": "Then, typical ST ocean data quality enhancement techniques are discussed.",
        "x1": -0.3257221579551697,
        "x2": 1.7001681327819824,
        "y1": 9.283775329589844,
        "y2": -0.4199766218662262
      },
      {
        "r": 0,
        "text": "Next, we classify existing STDM studies for ocean into four types of tasks, i.e., prediction, event detection, pattern mining, and anomaly detection, and elaborate the techniques for these tasks.",
        "trunc_text": "Next, we classify existing STDM studies for ocean into four types of tasks, i.e., prediction, event detection, pattern m",
        "x1": -0.3236026465892792,
        "x2": 1.749382495880127,
        "y1": 9.277865409851074,
        "y2": -0.3786054849624634
      },
      {
        "r": 0,
        "text": "Finally, promising research opportunities are highlighted.",
        "trunc_text": "Finally, promising research opportunities are highlighted.",
        "x1": 5.0847978591918945,
        "x2": 6.380002021789551,
        "y1": 8.482120513916016,
        "y2": 4.17656946182251
      },
      {
        "r": 0,
        "text": "This survey will help scientists from the fields of both computer science and ocean science have a better understanding of the fundamental concepts, key techniques, and open challenges of STDM in ocean.",
        "trunc_text": "This survey will help scientists from the fields of both computer science and ocean science have a better understanding ",
        "x1": -0.34735557436943054,
        "x2": 1.7196369171142578,
        "y1": 9.299901962280273,
        "y2": -0.4132312536239624
      },
      {
        "r": 0,
        "text": "Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic.",
        "trunc_text": "Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet u",
        "x1": 1.2148555517196655,
        "x2": 0.8486151099205017,
        "y1": 4.794662952423096,
        "y2": 3.568356513977051
      },
      {
        "r": 0,
        "text": "Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples.",
        "trunc_text": "Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with ha",
        "x1": 1.1511093378067017,
        "x2": 0.8433364629745483,
        "y1": 4.877102375030518,
        "y2": 3.512265920639038
      },
      {
        "r": 0,
        "text": "However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model.",
        "trunc_text": "However, when both types of data are present, most existing methods treat them equally, which results in a decline in th",
        "x1": 3.9760806560516357,
        "x2": 5.213326454162598,
        "y1": 7.123549461364746,
        "y2": 3.3762688636779785
      },
      {
        "r": 0,
        "text": "Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples.",
        "trunc_text": "Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differ",
        "x1": 1.408353328704834,
        "x2": 0.712662935256958,
        "y1": 4.944248199462891,
        "y2": 3.456670045852661
      },
      {
        "r": 0,
        "text": "These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples.",
        "trunc_text": "These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples",
        "x1": 5.396218776702881,
        "x2": 6.549380779266357,
        "y1": 8.950565338134766,
        "y2": 4.857952117919922
      },
      {
        "r": 0,
        "text": "Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples.",
        "trunc_text": "Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the h",
        "x1": 1.1968820095062256,
        "x2": 0.9906975626945496,
        "y1": 4.942254066467285,
        "y2": 3.4779345989227295
      },
      {
        "r": 0,
        "text": "We study various data partitioning methods in the presence of label noise and observe that filtering out noisy samples from hard samples with this proposed metric results in the best datasets as evidenced by the high test accuracy achieved after models are trained on the filtered datasets.",
        "trunc_text": "We study various data partitioning methods in the presence of label noise and observe that filtering out noisy samples f",
        "x1": 1.3305163383483887,
        "x2": 1.1293179988861084,
        "y1": 4.980914115905762,
        "y2": 3.403322696685791
      },
      {
        "r": 0,
        "text": "We demonstrate this for both our created synthetic datasets and for datasets with real-world label noise.",
        "trunc_text": "We demonstrate this for both our created synthetic datasets and for datasets with real-world label noise.",
        "x1": 1.133834719657898,
        "x2": 1.0865435600280762,
        "y1": 5.0028204917907715,
        "y2": 3.305863618850708
      },
      {
        "r": 0,
        "text": "Furthermore, our proposed data partitioning method significantly outperforms other methods when employed within a semi-supervised learning framework.",
        "trunc_text": "Furthermore, our proposed data partitioning method significantly outperforms other methods when employed within a semi-s",
        "x1": 1.5080735683441162,
        "x2": 1.6989054679870605,
        "y1": 5.104507923126221,
        "y2": 3.2572147846221924
      },
      {
        "r": 0,
        "text": "SlowTV contains 1.7M images from a rich diversity of environments, such as worldwide seasonal hiking, scenic driving and scuba diving.",
        "trunc_text": "SlowTV contains 1.7M images from a rich diversity of environments, such as worldwide seasonal hiking, scenic driving and",
        "x1": 2.5497188568115234,
        "x2": 4.25711727142334,
        "y1": 8.539565086364746,
        "y2": 0.7763827443122864
      },
      {
        "r": 0,
        "text": "Self-supervised monocular depth estimation (SS-MDE) has the potential to scale to vast quantities of data.",
        "trunc_text": "Self-supervised monocular depth estimation (SS-MDE) has the potential to scale to vast quantities of data.",
        "x1": 0.9057349562644958,
        "x2": 2.6652302742004395,
        "y1": 8.78227424621582,
        "y2": 0.31500208377838135
      },
      {
        "r": 0,
        "text": "Unfortunately, existing approaches limit themselves to the automotive domain, resulting in models incapable of generalizing to complex environments such as natural or indoor settings.    ",
        "trunc_text": "Unfortunately, existing approaches limit themselves to the automotive domain, resulting in models incapable of generaliz",
        "x1": 3.9245564937591553,
        "x2": 5.345417022705078,
        "y1": 9.157514572143555,
        "y2": 0.6066541075706482
      },
      {
        "r": 0,
        "text": "Using this dataset, wutperforms all existing SSL approaches and closes the gap on supervised SoTA, despite using a more efficient architecture.   ",
        "trunc_text": "Using this dataset, wutperforms all existing SSL approaches and closes the gap on supervised SoTA, despite using a more ",
        "x1": 5.804256916046143,
        "x2": 1.6044658422470093,
        "y1": 5.5603437423706055,
        "y2": 2.918576955795288
      },
      {
        "r": 0,
        "text": "We additionally introduce a collection of best-practices to further maximize performance and zero-shot generalization.",
        "trunc_text": "We additionally introduce a collection of best-practices to further maximize performance and zero-shot generalization.",
        "x1": 2.9237847328186035,
        "x2": 3.579378366470337,
        "y1": 6.647524833679199,
        "y2": 3.303340435028076
      },
      {
        "r": 0,
        "text": "This includes 1) aspect ratio augmentation, 2) camera intrinsic estimation, 3) support frame randomization and 4) flexible motion estimation.",
        "trunc_text": "This includes 1) aspect ratio augmentation, 2) camera intrinsic estimation, 3) support frame randomization and 4) flexib",
        "x1": 1.5197265148162842,
        "x2": 3.710937023162842,
        "y1": 9.143086433410645,
        "y2": 0.08979537338018417
      },
      {
        "r": 0,
        "text": "Code is available at https://github.com/jspenmar/slowtv_monodepth.",
        "trunc_text": "Code is available at https://github.com/jspenmar/slowtv_monodepth.",
        "x1": 2.7015633583068848,
        "x2": 8.618963241577148,
        "y1": 8.482518196105957,
        "y2": 2.134558916091919
      },
      {
        "r": 0,
        "text": "As an alternative, we present CZEch~NEws~Classification~dataset (CZE-NEC), one of the largest Czech classification datasets, composed of news articles from various sources spanning over twenty years, which allows a more rigorous evaluation of such models.",
        "trunc_text": "As an alternative, we present CZEch~NEws~Classification~dataset (CZE-NEC), one of the largest Czech classification datas",
        "x1": 3.4892685413360596,
        "x2": 2.971330165863037,
        "y1": 3.5847132205963135,
        "y2": 5.451573371887207
      },
      {
        "r": 0,
        "text": "Pre-trained models for Czech Natural Language Processing are often evaluated on purely linguistic tasks (POS tagging, parsing, NER) and relatively simple classification tasks such as sentiment classification or article classification from a single news source.  ",
        "trunc_text": "Pre-trained models for Czech Natural Language Processing are often evaluated on purely linguistic tasks (POS tagging, pa",
        "x1": 3.4987661838531494,
        "x2": 2.9883909225463867,
        "y1": 3.6670992374420166,
        "y2": 5.537342071533203
      },
      {
        "r": 0,
        "text": "We define four classification tasks: news source, news category, inferred author's gender, and day of the week.",
        "trunc_text": "We define four classification tasks: news source, news category, inferred author's gender, and day of the week.",
        "x1": 3.3724217414855957,
        "x2": 3.137887477874756,
        "y1": 3.8504934310913086,
        "y2": 5.428618431091309
      },
      {
        "r": 0,
        "text": "To verify the task difficulty, we conducted a human evaluation, which revealed that human performance lags behind strong machine-learning baselines built upon pre-trained transformer models.",
        "trunc_text": "To verify the task difficulty, we conducted a human evaluation, which revealed that human performance lags behind strong",
        "x1": 4.12851095199585,
        "x2": 3.5033531188964844,
        "y1": 5.119800567626953,
        "y2": 4.6610307693481445
      },
      {
        "r": 0,
        "text": "Furthermore, we show that language-specific pre-trained encoder analysis outperforms selected commercially available large-scale generative language models.",
        "trunc_text": "Furthermore, we show that language-specific pre-trained encoder analysis outperforms selected commercially available lar",
        "x1": 4.484679698944092,
        "x2": 3.4459035396575928,
        "y1": 4.299028396606445,
        "y2": 5.314325332641602
      },
      {
        "r": 0,
        "text": "To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving",
        "trunc_text": "To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine",
        "x1": 5.101953983306885,
        "x2": 5.093278884887695,
        "y1": 6.3442063331604,
        "y2": 4.334066867828369
      },
      {
        "r": 0,
        "text": "SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics.",
        "trunc_text": "SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems ",
        "x1": 5.22761344909668,
        "x2": 5.730612277984619,
        "y1": 6.567922115325928,
        "y2": 3.1843764781951904
      },
      {
        "r": 0,
        "text": "Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks.",
        "trunc_text": "Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks.",
        "x1": 5.105381488800049,
        "x2": 3.70320987701416,
        "y1": 3.9117674827575684,
        "y2": 6.17744779586792
      },
      {
        "r": 0,
        "text": "However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. .",
        "trunc_text": "However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only",
        "x1": 4.853541851043701,
        "x2": 4.929274559020996,
        "y1": 6.05787992477417,
        "y2": 4.3336639404296875
      },
      {
        "r": 0,
        "text": "SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set sfactory performance, with an overall score of merely 35.80%.",
        "trunc_text": "SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems ",
        "x1": 5.147823810577393,
        "x2": 5.564624786376953,
        "y1": 6.548840522766113,
        "y2": 3.189424991607666
      },
      {
        "r": 0,
        "text": "Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities.",
        "trunc_text": "Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities.",
        "x1": 5.384047985076904,
        "x2": 4.354433059692383,
        "y1": 4.353490352630615,
        "y2": 6.265400409698486
      },
      {
        "r": 0,
        "text": "Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills.",
        "trunc_text": "Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demon",
        "x1": 5.127754211425781,
        "x2": 4.110619068145752,
        "y1": 4.298185348510742,
        "y2": 5.897154808044434
      },
      {
        "r": 0,
        "text": "We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.",
        "trunc_text": "We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately cont",
        "x1": 5.961049556732178,
        "x2": 5.01384162902832,
        "y1": 4.967451572418213,
        "y2": 5.956304550170898
      },
      {
        "r": 0,
        "text": "Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications.",
        "trunc_text": "Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications",
        "x1": 0.4029601514339447,
        "x2": 7.327147006988525,
        "y1": 4.9180684089660645,
        "y2": 4.859574794769287
      },
      {
        "r": 0,
        "text": "Existing federated learning works mainly focus on model homogeneous settings.",
        "trunc_text": "Existing federated learning works mainly focus on model homogeneous settings.",
        "x1": 0.40721532702445984,
        "x2": 7.299927234649658,
        "y1": 4.914963245391846,
        "y2": 4.8647356033325195
      },
      {
        "r": 0,
        "text": "However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients.",
        "trunc_text": "However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, netw",
        "x1": 0.3935117721557617,
        "x2": 7.319459915161133,
        "y1": 4.913294792175293,
        "y2": 4.861259460449219
      },
      {
        "r": 0,
        "text": "Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex.",
        "trunc_text": "Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex.",
        "x1": 0.38969188928604126,
        "x2": 7.305155277252197,
        "y1": 4.925146579742432,
        "y2": 4.845953941345215
      },
      {
        "r": 0,
        "text": "Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential.",
        "trunc_text": "Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential.",
        "x1": 4.372535705566406,
        "x2": 6.365551471710205,
        "y1": 7.701674461364746,
        "y2": 3.8793444633483887
      },
      {
        "r": 0,
        "text": "In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges.",
        "trunc_text": "In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity",
        "x1": 6.022125720977783,
        "x2": 7.086733818054199,
        "y1": 8.537540435791016,
        "y2": 4.722023963928223
      },
      {
        "r": 0,
        "text": "In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons.",
        "trunc_text": "In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth",
        "x1": 5.981969356536865,
        "x2": 7.003044605255127,
        "y1": 8.56277847290039,
        "y2": 4.743120193481445
      },
      {
        "r": 0,
        "text": "We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and server-level.",
        "trunc_text": "We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and se",
        "x1": 5.99235200881958,
        "x2": 7.009185791015625,
        "y1": 8.523969650268555,
        "y2": 4.718352317810059
      },
      {
        "r": 0,
        "text": "Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further developments in this field.",
        "trunc_text": "Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further de",
        "x1": 6.049599647521973,
        "x2": 7.049874305725098,
        "y1": 8.529020309448242,
        "y2": 4.705418586730957
      },
      {
        "r": 0,
        "text": "A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey.",
        "trunc_text": "A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey.",
        "x1": 6.2005295753479,
        "x2": 7.123237609863281,
        "y1": 8.439342498779297,
        "y2": 4.605522632598877
      },
      {
        "r": 0,
        "text": "In this paper, we analyze the data that we have collected on the pendency of 24 high courts in the Republic of India as they were made available on High Court NJDG (HC-NJDG).",
        "trunc_text": "In this paper, we analyze the data that we have collected on the pendency of 24 high courts in the Republic of India as ",
        "x1": 0.505068838596344,
        "x2": 7.887324333190918,
        "y1": 1.9506001472473145,
        "y2": 5.042522430419922
      },
      {
        "r": 0,
        "text": "We collected data on 73 days beginning August 31, 2017 to December 26, 2018, including these days.",
        "trunc_text": "We collected data on 73 days beginning August 31, 2017 to December 26, 2018, including these days.",
        "x1": 5.779409885406494,
        "x2": 6.442714691162109,
        "y1": 7.732832908630371,
        "y2": 2.1043155193328857
      },
      {
        "r": 0,
        "text": "Thus, the data collected by us spans a period of almost sixteen months.",
        "trunc_text": "Thus, the data collected by us spans a period of almost sixteen months.",
        "x1": 5.627292633056641,
        "x2": 6.3611016273498535,
        "y1": 7.763860702514648,
        "y2": 2.1601247787475586
      },
      {
        "r": 0,
        "text": "Indian Judiciary is suffering from burden of millions of cases that are lying pending in its courts at all the levels.  ",
        "trunc_text": "Indian Judiciary is suffering from burden of millions of cases that are lying pending in its courts at all the levels.  ",
        "x1": 0.49645042419433594,
        "x2": 7.915730953216553,
        "y1": 1.9412263631820679,
        "y2": 5.061129093170166
      },
      {
        "r": 0,
        "text": "We imited to the number of judges in each high court, the number of cases pending in each high court, d and disposed, cases filed by women and senior citizens, etc.",
        "trunc_text": "We imited to the number of judges in each high court, the number of cases pending in each high court, d and disposed, ca",
        "x1": 0.5061671733856201,
        "x2": 7.879624366760254,
        "y1": 1.9518276453018188,
        "y2": 5.029893398284912
      },
      {
        "r": 0,
        "text": "Our results show that: 1) statistics as important as the number of judges in high courts have serious errors on NJDG (Fig. 1, 2, 10, 11, Table V).",
        "trunc_text": "Our results show that: 1) statistics as important as the number of judges in high courts have serious errors on NJDG (Fi",
        "x1": 0.5407401919364929,
        "x2": 7.785584449768066,
        "y1": 1.9890079498291016,
        "y2": 4.971103668212891
      },
      {
        "r": 0,
        "text": "2) pending cases in most of the high courts are increasing rather than decreasing (Fig. 3, 13).",
        "trunc_text": "2) pending cases in most of the high courts are increasing rather than decreasing (Fig. 3, 13).",
        "x1": 0.5089559555053711,
        "x2": 7.91082763671875,
        "y1": 1.9548134803771973,
        "y2": 5.057729244232178
      },
      {
        "r": 0,
        "text": "3) regular update of HC-NJDG is required for it to be useful.",
        "trunc_text": "3) regular update of HC-NJDG is required for it to be useful.",
        "x1": 5.266136169433594,
        "x2": 6.5433526039123535,
        "y1": 8.687594413757324,
        "y2": 4.59324312210083
      },
      {
        "r": 0,
        "text": "Data related to some high courts is not being updated regularly or is updated erroneously on the portal (Fig. 14).",
        "trunc_text": "Data related to some high courts is not being updated regularly or is updated erroneously on the portal (Fig. 14).",
        "x1": 0.5246401429176331,
        "x2": 7.908778667449951,
        "y1": 1.9735915660858154,
        "y2": 5.043401718139648
      },
      {
        "r": 0,
        "text": "4) there is a huge difference in terms of average load of cases on judges of different high courts (Fig. 6).",
        "trunc_text": "4) there is a huge difference in terms of average load of cases on judges of different high courts (Fig. 6).",
        "x1": 0.5017511248588562,
        "x2": 7.895174503326416,
        "y1": 1.947342038154602,
        "y2": 5.048717021942139
      },
      {
        "r": 0,
        "text": "5) if all the high courts operate at their approved strength of judges, then for most of the high courts pendency can be nullified within 20 years from now (Fig. 21, 22).",
        "trunc_text": "5) if all the high courts operate at their approved strength of judges, then for most of the high courts pendency can be",
        "x1": 0.5069203972816467,
        "x2": 7.908424377441406,
        "y1": 1.953696608543396,
        "y2": 5.052576065063477
      },
      {
        "r": 0,
        "text": "6) the pending cases filed by women and senior citizens are disproportionately low, they together constitute less than 10% of the total pending cases (Fig. 23 - 27) 7) a better scheduling process for preparing causelists in courts can help reducing the number of pending cases in the High Courts (Fig. 29).",
        "trunc_text": "6) the pending cases filed by women and senior citizens are disproportionately low, they together constitute less than 1",
        "x1": 0.5023419260978699,
        "x2": 7.900892734527588,
        "y1": 1.9480453729629517,
        "y2": 5.046849250793457
      },
      {
        "r": 0,
        "text": "8) some statistics are not well defined (Fig. 31).",
        "trunc_text": "8) some statistics are not well defined (Fig. 31).",
        "x1": 5.380089282989502,
        "x2": 6.234803199768066,
        "y1": 6.650971412658691,
        "y2": 3.76275634765625
      },
      {
        "r": 0,
        "text": "In this work, we describe the curation of a massive speech dataset of 8740 hours consisting of $\\sim9.8$K technical lectures in the English language along with their transcripts delivered by instructors representing various parts of Indian demography.",
        "trunc_text": "In this work, we describe the curation of a massive speech dataset of 8740 hours consisting of $\\sim9.8$K technical lect",
        "x1": 4.663727283477783,
        "x2": 3.06716251373291,
        "y1": 3.041635513305664,
        "y2": 6.484460830688477
      },
      {
        "r": 0,
        "text": "The dataset is sourced from the very popular NPTEL MOOC platform.",
        "trunc_text": "The dataset is sourced from the very popular NPTEL MOOC platform.",
        "x1": 7.069141864776611,
        "x2": 7.540042400360107,
        "y1": 7.824009895324707,
        "y2": 2.0829873085021973
      },
      {
        "r": 0,
        "text": "Automatic speech recognition (ASR) systems are designed to transcribe spoken language into written text and find utility in a variety of applications including voice assistants and transcription services.",
        "trunc_text": "Automatic speech recognition (ASR) systems are designed to transcribe spoken language into written text and find utility",
        "x1": 4.945700168609619,
        "x2": 2.9613394737243652,
        "y1": 2.9131765365600586,
        "y2": 6.661527156829834
      },
      {
        "r": 0,
        "text": "However, it has been observed that state-of-the-art ASR systems which deliver impressive benchmark results, struggle with speakers of certain regions or demographics due to variation in their speech properties.  ",
        "trunc_text": "However, it has been observed that state-of-the-art ASR systems which deliver impressive benchmark results, struggle wit",
        "x1": 4.9615478515625,
        "x2": 3.1421377658843994,
        "y1": 2.904742956161499,
        "y2": 6.543051242828369
      },
      {
        "r": 0,
        "text": "We use the curated dataset to measure the existing disparity in YouTube Automatic Captions and OpenAI Whisper model performance across the diverse demographic traits of speakers in Indi and speech rate of speakers, disparity based on caste is non-existent.",
        "trunc_text": "We use the curated dataset to measure the existing disparity in YouTube Automatic Captions and OpenAI Whisper model perf",
        "x1": 4.754390239715576,
        "x2": 2.9366581439971924,
        "y1": 2.858107089996338,
        "y2": 6.366524696350098
      },
      {
        "r": 0,
        "text": "We also observe statistically significant disparity across the disciplines of the lectures.",
        "trunc_text": "We also observe statistically significant disparity across the disciplines of the lectures.",
        "x1": 3.124955415725708,
        "x2": 6.004965305328369,
        "y1": 4.623254776000977,
        "y2": 3.786606550216675
      },
      {
        "r": 0,
        "text": "These results indicate the need of more inclusive and robust ASR systems and more representational datasets for disparity evaluation in them.",
        "trunc_text": "These results indicate the need of more inclusive and robust ASR systems and more representational datasets for disparit",
        "x1": 4.981767177581787,
        "x2": 3.0649220943450928,
        "y1": 2.9115731716156006,
        "y2": 6.400781631469727
      },
      {
        "r": 0,
        "text": "In this work, we present DNA-Rendering, a large-scale, high-fidelity repository of human performance data for neural actor rendering",
        "trunc_text": "In this work, we present DNA-Rendering, a large-scale, high-fidelity repository of human performance data for neural act",
        "x1": 2.239668369293213,
        "x2": 3.8339641094207764,
        "y1": 8.580987930297852,
        "y2": 0.9923580288887024
      },
      {
        "r": 0,
        "text": "First, our dataset contains over 1500 human subjects, 5000 motion sequences, and 67.5M frames' data volume.",
        "trunc_text": "First, our dataset contains over 1500 human subjects, 5000 motion sequences, and 67.5M frames' data volume.",
        "x1": 2.611278533935547,
        "x2": 4.013363838195801,
        "y1": 9.420513153076172,
        "y2": -0.32383495569229126
      },
      {
        "r": 0,
        "text": "Second, we provide rich assets for each subject -- 2D/3D human body keypoints, foreground masks, SMPLX models, cloth/accessory materials, multi-view images, and videos",
        "trunc_text": "Second, we provide rich assets for each subject -- 2D/3D human body keypoints, foreground masks, SMPLX models, cloth/acc",
        "x1": 2.356799602508545,
        "x2": 3.795377254486084,
        "y1": 9.361971855163574,
        "y2": -0.19198204576969147
      },
      {
        "r": 0,
        "text": "Third, we construct a professional multi-view system to capture data, which contains 60 synchronous cameras with max 4096 x 3000 resolution, 15 fps speed, and stern camera calibration steps, ensuring high-quality resources for task training and evaluation",
        "trunc_text": "Third, we construct a professional multi-view system to capture data, which contains 60 synchronous cameras with max 409",
        "x1": 1.3934141397476196,
        "x2": 3.624717950820923,
        "y1": 9.189517974853516,
        "y2": 0.13307636976242065
      },
      {
        "r": 0,
        "text": "The dataset, code, and benchmarks will be publicly available at https://dna-rendering.github.io/",
        "trunc_text": "The dataset, code, and benchmarks will be publicly available at https://dna-rendering.github.io/",
        "x1": 6.826903820037842,
        "x2": 7.490466117858887,
        "y1": 8.079850196838379,
        "y2": 1.9542418718338013
      },
      {
        "r": 0,
        "text": "Realistic human-centric rendering plays a key role in both computer vision and computer graphics.",
        "trunc_text": "Realistic human-centric rendering plays a key role in both computer vision and computer graphics.",
        "x1": 2.1418356895446777,
        "x2": 3.842724561691284,
        "y1": 8.855441093444824,
        "y2": 0.6534899473190308
      },
      {
        "r": 0,
        "text": "Rapid progress has been made in the algorithm aspect over the years, yet existing human-centric rendering datasets and benchmarks are rather impoverished in terms of diversity, which are crucial for rendering effect.",
        "trunc_text": "Rapid progress has been made in the algorithm aspect over the years, yet existing human-centric rendering datasets and b",
        "x1": 2.0325136184692383,
        "x2": 3.8516411781311035,
        "y1": 8.705238342285156,
        "y2": 0.8344326615333557
      },
      {
        "r": 0,
        "text": "Researchers are usually constrained to explore and evaluate a small set of rendering problems on current datasets, while real-world applications require methods to be robust across different scenarios. .",
        "trunc_text": "Researchers are usually constrained to explore and evaluate a small set of rendering problems on current datasets, while",
        "x1": 2.0527405738830566,
        "x2": 3.8579905033111572,
        "y1": 8.783369064331055,
        "y2": 0.8843030333518982
      },
      {
        "r": 0,
        "text": "DNA-Rendering presents several alluring attributes.",
        "trunc_text": "DNA-Rendering presents several alluring attributes.",
        "x1": 3.402583360671997,
        "x2": 4.3670268058776855,
        "y1": 8.344474792480469,
        "y2": 1.096642255783081
      },
      {
        "r": 0,
        "text": "Second, we provide rich rials, multi-view images, and videos.",
        "trunc_text": "Second, we provide rich rials, multi-view images, and videos.",
        "x1": 1.8590446710586548,
        "x2": 4.202246189117432,
        "y1": 8.404990196228027,
        "y2": 0.8728452324867249
      },
      {
        "r": 0,
        "text": "These assets boost the current method's accuracy on downstream renderid stern camera calibration steps, ensuring high-quality resources for task training and evaluation.",
        "trunc_text": "These assets boost the current method's accuracy on downstream renderid stern camera calibration steps, ensuring high-qu",
        "x1": 1.5196245908737183,
        "x2": 3.584620714187622,
        "y1": 9.134832382202148,
        "y2": 0.15962474048137665
      },
      {
        "r": 0,
        "text": "Along with the dataset, we provide a large-scale and quantitative benchmark in full-scale, with multiple tasks to evaluate the existing progress of net, code, and benchmarks will be publicly available at https://dna-rendering.github.io/",
        "trunc_text": "Along with the dataset, we provide a large-scale and quantitative benchmark in full-scale, with multiple tasks to evalua",
        "x1": 3.4055850505828857,
        "x2": 4.292346000671387,
        "y1": 8.124837875366211,
        "y2": 2.4840357303619385
      },
      {
        "r": 0,
        "text": "We investigate the use of 2D black-and-white textures for the visualization of categorical data and contribute a summary of texture attributes, and the results of three experiments that elicited design strategies as well as aesthetic and effectiveness measures.",
        "trunc_text": "We investigate the use of 2D black-and-white textures for the visualization of categorical data and contribute a summary",
        "x1": 3.7474782466888428,
        "x2": 5.581990718841553,
        "y1": 8.425545692443848,
        "y2": 1.1696964502334595
      },
      {
        "r": 0,
        "text": "Black-and-white textures are useful, for instance, as a visual channel for categorical data on low-color displays, in 2D/3D print, to achieve the aesthetic of historic visualizations, or to retain the color hue channel for other visual mappings.",
        "trunc_text": "Black-and-white textures are useful, for instance, as a visual channel for categorical data on low-color displays, in 2D",
        "x1": 3.7107582092285156,
        "x2": 5.543303489685059,
        "y1": 8.471657752990723,
        "y2": 1.2082877159118652
      },
      {
        "r": 0,
        "text": "We specifically study how to use what we call geometric and iconic textures.",
        "trunc_text": "We specifically study how to use what we call geometric and iconic textures.",
        "x1": 3.723188877105713,
        "x2": 5.495968341827393,
        "y1": 8.518178939819336,
        "y2": 1.165986180305481
      },
      {
        "r": 0,
        "text": "Geometric textures use patterns of repeated abstract geometric shapes, while iconic textures use repeated icons that may stand for data categories.",
        "trunc_text": "Geometric textures use patterns of repeated abstract geometric shapes, while iconic textures use repeated icons that may",
        "x1": 3.7347214221954346,
        "x2": 5.541639804840088,
        "y1": 8.466107368469238,
        "y2": 1.2280282974243164
      },
      {
        "r": 0,
        "text": "We parameterized both types of textures and developed a tool for designers to create textures on simple charts by adjusting texture parameters.",
        "trunc_text": "We parameterized both types of textures and developed a tool for designers to create textures on simple charts by adjust",
        "x1": 3.805143356323242,
        "x2": 5.5983357429504395,
        "y1": 8.491146087646484,
        "y2": 1.2060184478759766
      },
      {
        "r": 0,
        "text": "30 visualization experts used our tool and designed 66 textured bar charts, pie charts, and maps.",
        "trunc_text": "30 visualization experts used our tool and designed 66 textured bar charts, pie charts, and maps.",
        "x1": 3.927051067352295,
        "x2": 5.733231544494629,
        "y1": 8.49007511138916,
        "y2": 1.2510517835617065
      },
      {
        "r": 0,
        "text": "We then had 150 participants rate these designs for aesthetics.",
        "trunc_text": "We then had 150 participants rate these designs for aesthetics.",
        "x1": 4.209939479827881,
        "x2": 5.705558776855469,
        "y1": 8.56069564819336,
        "y2": 1.291528344154358
      },
      {
        "r": 0,
        "text": "Finally, with the top-rated geometric and iconic textures, our perceptual assessment experiment with 150 participants revealed that textured charts perform about equally well as non-textured charts, and that there are some differences depending on the type of chart.",
        "trunc_text": "Finally, with the top-rated geometric and iconic textures, our perceptual assessment experiment with 150 participants re",
        "x1": 3.758676052093506,
        "x2": 5.587839603424072,
        "y1": 8.48698616027832,
        "y2": 1.1809309720993042
      },
      {
        "r": 0,
        "text": "As part of the data-driven paradigm and open science movement, the data paper is becoming a popular way for researchers to publish their research data, based on academic norms that cross knowledge domains.",
        "trunc_text": "As part of the data-driven paradigm and open science movement, the data paper is becoming a popular way for researchers ",
        "x1": 5.95906400680542,
        "x2": 6.615833282470703,
        "y1": 7.592213153839111,
        "y2": 2.746663808822632
      },
      {
        "r": 0,
        "text": "Data journals have also been created to host this new academic genre.",
        "trunc_text": "Data journals have also been created to host this new academic genre.",
        "x1": 5.979799747467041,
        "x2": 6.626950263977051,
        "y1": 7.636319637298584,
        "y2": 2.8204782009124756
      },
      {
        "r": 0,
        "text": "The growing number of data papers and journals has made them an important large-scale data source for understanding how research data is published and reused in our research system.",
        "trunc_text": "The growing number of data papers and journals has made them an important large-scale data source for understanding how ",
        "x1": 5.91996955871582,
        "x2": 6.650550365447998,
        "y1": 7.554281711578369,
        "y2": 2.785430908203125
      },
      {
        "r": 0,
        "text": "One barrier to this research agenda is a lack of knowledge as to how data journals and their publications are indexed in the scholarly databases used for quantitative analysis.",
        "trunc_text": "One barrier to this research agenda is a lack of knowledge as to how data journals and their publications are indexed in",
        "x1": 5.993988037109375,
        "x2": 6.604754447937012,
        "y1": 7.603559494018555,
        "y2": 2.8925974369049072
      },
      {
        "r": 0,
        "text": "To address this gap, this study examines how a list of 18 exclusively data journals (i.e., journals that primarily accept data papers) are indexed in four popular scholarly databases: the Web of Science, Scopus, Dimensions, and OpenAlex.",
        "trunc_text": "To address this gap, this study examines how a list of 18 exclusively data journals (i.e., journals that primarily accep",
        "x1": 5.947299480438232,
        "x2": 6.6123127937316895,
        "y1": 7.588642120361328,
        "y2": 2.8224995136260986
      },
      {
        "r": 0,
        "text": "We investigate how comprehensively these databases cover the selected data journals and, in particular, how they present the document type information of data papers.",
        "trunc_text": "We investigate how comprehensively these databases cover the selected data journals and, in particular, how they present",
        "x1": 5.994882106781006,
        "x2": 6.694980144500732,
        "y1": 7.520323753356934,
        "y2": 2.7996790409088135
      },
      {
        "r": 0,
        "text": "We find that the coverage of data papers, as well as their document type information, is highly inconsistent across databases, which creates major challenges for future efforts to study them quantitatively.",
        "trunc_text": "We find that the coverage of data papers, as well as their document type information, is highly inconsistent across data",
        "x1": 5.934259414672852,
        "x2": 6.635190486907959,
        "y1": 7.499487400054932,
        "y2": 2.852414608001709
      },
      {
        "r": 0,
        "text": "As a result, we argue that efforts should be made by data journals and databases to improve the quality of metadata for this emerging genre.",
        "trunc_text": "As a result, we argue that efforts should be made by data journals and databases to improve the quality of metadata for ",
        "x1": 5.972278594970703,
        "x2": 6.651761531829834,
        "y1": 7.533310890197754,
        "y2": 2.788778305053711
      },
      {
        "r": 0,
        "text": "Situated visualization has become an increasingly popular research area in the visualization community, fueled by advancements in augmented reality (AR) technology and immersive analytics.",
        "trunc_text": "Situated visualization has become an increasingly popular research area in the visualization community, fueled by advanc",
        "x1": 3.8578927516937256,
        "x2": 5.771083831787109,
        "y1": 8.563986778259277,
        "y2": 1.1982263326644897
      },
      {
        "r": 0,
        "text": "Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations not present in traditional visualization, which researchers are now beginning to explore.",
        "trunc_text": "Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations no",
        "x1": 3.9637484550476074,
        "x2": 5.77553653717041,
        "y1": 8.4434175491333,
        "y2": 1.2982792854309082
      },
      {
        "r": 0,
        "text": "However, the AR research community has an extensive history of designing graphics that are displayed in highly physical contexts.",
        "trunc_text": "However, the AR research community has an extensive history of designing graphics that are displayed in highly physical ",
        "x1": 3.935155153274536,
        "x2": 5.734534740447998,
        "y1": 8.569355010986328,
        "y2": 1.208306908607483
      },
      {
        "r": 0,
        "text": "In this work, we leverage the richness of AR research and apply it to situated visualization.",
        "trunc_text": "In this work, we leverage the richness of AR research and apply it to situated visualization.",
        "x1": 3.9238393306732178,
        "x2": 5.78073787689209,
        "y1": 8.51732349395752,
        "y2": 1.190704584121704
      },
      {
        "r": 0,
        "text": "We derive design patterns which summarize common approaches of visualizing data in situ.",
        "trunc_text": "We derive design patterns which summarize common approaches of visualizing data in situ.",
        "x1": 4.051280975341797,
        "x2": 5.787625789642334,
        "y1": 8.436732292175293,
        "y2": 1.2754037380218506
      },
      {
        "r": 0,
        "text": "The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as our own expertise.",
        "trunc_text": "The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as ou",
        "x1": 4.069435119628906,
        "x2": 5.640648365020752,
        "y1": 8.579911231994629,
        "y2": 1.164232850074768
      },
      {
        "r": 0,
        "text": "We discuss design dimensions that help to describe both our patterns and previous work in the literature.",
        "trunc_text": "We discuss design dimensions that help to describe both our patterns and previous work in the literature.",
        "x1": 4.299562454223633,
        "x2": 5.694761753082275,
        "y1": 8.68363094329834,
        "y2": 1.3025002479553223
      },
      {
        "r": 0,
        "text": "This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints imposed by the real world.",
        "trunc_text": "This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints impos",
        "x1": 4.207836151123047,
        "x2": 5.575802326202393,
        "y1": 8.735917091369629,
        "y2": 0.9271230101585388
      },
      {
        "r": 0,
        "text": "We conclude by discussing future research directions that will help establish a complete understanding of the design of situated visualization, including the role of interactivity, tasks, and workflows.",
        "trunc_text": "We conclude by discussing future research directions that will help establish a complete understanding of the design of ",
        "x1": 3.9534175395965576,
        "x2": 5.768718719482422,
        "y1": 8.47197151184082,
        "y2": 1.2359915971755981
      },
      {
        "r": 0,
        "text": "Automatic diagnosis (AD), a critical application of AI in healthcare, employs machine learning techniques to assist doctors in gathering patient symptom information for precise disease diagnosis.",
        "trunc_text": "Automatic diagnosis (AD), a critical application of AI in healthcare, employs machine learning techniques to assist doct",
        "x1": 3.491551399230957,
        "x2": 4.58835506439209,
        "y1": 7.855942249298096,
        "y2": 2.0127944946289062
      },
      {
        "r": 0,
        "text": "The Transformer-based method utilizes an input symptom sequence, predicts itself through auto-regression, and employs the hidden state of the final symptom to determine the disease.",
        "trunc_text": "The Transformer-based method utilizes an input symptom sequence, predicts itself through auto-regression, and employs th",
        "x1": 3.4741287231445312,
        "x2": 4.582046985626221,
        "y1": 7.902402400970459,
        "y2": 1.948469638824463
      },
      {
        "r": 0,
        "text": "Despite its simplicity and superior performance demonstrated, a decline in disease diagnosis accuracy is observed caused by 1) a mismatch between symptoms observed during training and generation, and 2) the effect of different symptom orders on disease prediction.",
        "trunc_text": "Despite its simplicity and superior performance demonstrated, a decline in disease diagnosis accuracy is observed caused",
        "x1": 3.4491448402404785,
        "x2": 4.55079460144043,
        "y1": 7.8767781257629395,
        "y2": 1.9679011106491089
      },
      {
        "r": 0,
        "text": "To address the above obstacles, we introduce the CoAD, a novel disease and symptom collaborative generation framework, which incorporates several key innovations to improve AD: 1) aligning sentence-level disease labels with multiple possible symptom inquiry steps to bridge the gap between training and generation; 2) expanding symptom labels for each sub-sequence of symptoms to enhance annotation and eliminate the effect of symptom order; 3) developing a repeated symptom input schema to effectively and efficiently learn the expanded disease and symptom labels.",
        "trunc_text": "To address the above obstacles, we introduce the CoAD, a novel disease and symptom collaborative generation framework, w",
        "x1": 3.483572006225586,
        "x2": 4.580073356628418,
        "y1": 7.821650981903076,
        "y2": 1.9865314960479736
      },
      {
        "r": 0,
        "text": "We evaluate the CoAD framework using four datasets, including three public and one private, and demonstrate that it achieves an average 2.3% improvement over previous state-of-the-art results in automatic disease diagnosis.",
        "trunc_text": "We evaluate the CoAD framework using four datasets, including three public and one private, and demonstrate that it achi",
        "x1": 3.542245388031006,
        "x2": 4.651258945465088,
        "y1": 7.797755241394043,
        "y2": 2.056854009628296
      },
      {
        "r": 0,
        "text": "For reproducibility, we release the code and data at https://github.com/KwanWaiChung/coad.",
        "trunc_text": "For reproducibility, we release the code and data at https://github.com/KwanWaiChung/coad.",
        "x1": 7.431596755981445,
        "x2": 7.9848761558532715,
        "y1": 7.671525955200195,
        "y2": 1.9895479679107666
      },
      {
        "r": 0,
        "text": "To avoid potential risks posed by vulnerabilities in third-party libraries, security researchers maintain databases containing vulnerability reports, e.g., the National Vulnerability Database (NVD).",
        "trunc_text": "To avoid potential risks posed by vulnerabilities in third-party libraries, security researchers maintain databases cont",
        "x1": 7.921443939208984,
        "x2": 6.918313503265381,
        "y1": 5.262145519256592,
        "y2": 6.556741714477539
      },
      {
        "r": 0,
        "text": "Application developers can identify vulnerable libraries by directly querying the databases with the name of each used library.",
        "trunc_text": "Application developers can identify vulnerable libraries by directly querying the databases with the name of each used l",
        "x1": 7.973855495452881,
        "x2": 6.9090256690979,
        "y1": 5.28684663772583,
        "y2": 6.5379252433776855
      },
      {
        "r": 0,
        "text": "However, the querying results of vulnerable libraries are not reliable due to the incompleteness of vulnerability reports.",
        "trunc_text": "However, the querying results of vulnerable libraries are not reliable due to the incompleteness of vulnerability report",
        "x1": 7.881862163543701,
        "x2": 6.868445873260498,
        "y1": 5.320412635803223,
        "y2": 6.516029357910156
      },
      {
        "r": 0,
        "text": "Thus, current approaches model the task of identifying vulnerable libraries as an extreme multi-label learning (XML) task.",
        "trunc_text": "Thus, current approaches model the task of identifying vulnerable libraries as an extreme multi-label learning (XML) tas",
        "x1": 7.883612155914307,
        "x2": 6.93532133102417,
        "y1": 5.201116561889648,
        "y2": 6.569701194763184
      },
      {
        "r": 0,
        "text": "These approaches suffer from highly inaccurate results and cannot identify zero-shot libraries (i.e., those not appearing during model training).",
        "trunc_text": "These approaches suffer from highly inaccurate results and cannot identify zero-shot libraries (i.e., those not appearin",
        "x1": 2.8947830200195312,
        "x2": 3.524599552154541,
        "y1": 6.646605968475342,
        "y2": 3.383892774581909
      },
      {
        "r": 0,
        "text": "To address these limitations, in this paper, we propose the first entity-linking approach named VulLibMiner to identify vulnerable third-party libraries from textual descriptions of vulnerabilities and libraries, together with VulLib, a Java vulnerability dataset with vulnerability-affected libraries.",
        "trunc_text": "To address these limitations, in this paper, we propose the first entity-linking approach named VulLibMiner to identify ",
        "x1": 7.938597679138184,
        "x2": 6.911036491394043,
        "y1": 5.2781901359558105,
        "y2": 6.541804790496826
      },
      {
        "r": 0,
        "text": "VulLibMiner consists of a coarse-grained TF-IDF matcher to efficiently screen out a small set of candidate libraries and a fine-grained BERT-FNN model to identify vulnerable libraries from these candidates effectively.",
        "trunc_text": "VulLibMiner consists of a coarse-grained TF-IDF matcher to efficiently screen out a small set of candidate libraries and",
        "x1": 7.9560017585754395,
        "x2": 6.918768405914307,
        "y1": 5.279459476470947,
        "y2": 6.5408172607421875
      },
      {
        "r": 0,
        "text": "We evaluate VulLibMiner using two state-of-the-art/practice approaches of library identification (FastXML, LightXML) on both their dataset named VeraJava and our VulLib dataset.",
        "trunc_text": "We evaluate VulLibMiner using two state-of-the-art/practice approaches of library identification (FastXML, LightXML) on ",
        "x1": 7.95280122756958,
        "x2": 6.9635820388793945,
        "y1": 5.264064788818359,
        "y2": 6.557315826416016
      },
      {
        "r": 0,
        "text": "Our evaluation results show that VulLibMiner can effectively identify vulnerable libraries with an average F1 score of 0.542 while the state-of-the-art/practice approaches achieve only 0.377.",
        "trunc_text": "Our evaluation results show that VulLibMiner can effectively identify vulnerable libraries with an average F1 score of 0",
        "x1": 7.943577289581299,
        "x2": 6.926799297332764,
        "y1": 5.275354862213135,
        "y2": 6.532130241394043
      },
      {
        "r": 0,
        "text": "We demonstrate VulLibMiner's high value of security practice by using VulLibMiner to identify 12,716 <vulnerability, library> pairs, and 7,936 of them do not appear in NVD.",
        "trunc_text": "We demonstrate VulLibMiner's high value of security practice by using VulLibMiner to identify 12,716 <vulnerability, lib",
        "x1": 7.878318786621094,
        "x2": 6.859717845916748,
        "y1": 5.206169128417969,
        "y2": 6.503829002380371
      },
      {
        "r": 0,
        "text": "We specify a file-oriented data format suitable for parallel, partition-independent disk I/O. Here, a partition refers to a disjoint and ordered distribution of the data elements between one or more processes.",
        "trunc_text": "We specify a file-oriented data format suitable for parallel, partition-independent disk I/O. Here, a partition refers t",
        "x1": 6.406952857971191,
        "x2": 7.022014141082764,
        "y1": 6.744699001312256,
        "y2": 3.3403327465057373
      },
      {
        "r": 0,
        "text": "The format is designed such that the file contents are invariant under linear (i. e., unpermuted), parallel repartition of the data prior to writing.",
        "trunc_text": "The format is designed such that the file contents are invariant under linear (i. e., unpermuted), parallel repartition ",
        "x1": 6.429530620574951,
        "x2": 7.061404228210449,
        "y1": 6.679556846618652,
        "y2": 3.3816475868225098
      },
      {
        "r": 0,
        "text": "The file contents are indistinguishable from writing in serial.",
        "trunc_text": "The file contents are indistinguishable from writing in serial.",
        "x1": 6.471691131591797,
        "x2": 7.098672866821289,
        "y1": 6.647228240966797,
        "y2": 3.430694341659546
      },
      {
        "r": 0,
        "text": "In the same vein, the file can be read on any number of processes that agree on any partition of the number of elements stored.   ",
        "trunc_text": "In the same vein, the file can be read on any number of processes that agree on any partition of the number of elements ",
        "x1": 6.414999961853027,
        "x2": 7.064462661743164,
        "y1": 6.73185396194458,
        "y2": 3.366718053817749
      },
      {
        "r": 0,
        "text": "In addition to the format specification we propose an optional convention to implement transparent per-element data compression.",
        "trunc_text": "In addition to the format specification we propose an optional convention to implement transparent per-element data comp",
        "x1": 6.333337783813477,
        "x2": 7.052720546722412,
        "y1": 6.7730207443237305,
        "y2": 3.366196393966675
      },
      {
        "r": 0,
        "text": "The compressed data and metadata is layered inside ordinary format elements.",
        "trunc_text": "The compressed data and metadata is layered inside ordinary format elements.",
        "x1": 6.4032206535339355,
        "x2": 7.027979373931885,
        "y1": 6.7023749351501465,
        "y2": 3.3478240966796875
      },
      {
        "r": 0,
        "text": "Overall, we pay special attention to both human and machine readability.",
        "trunc_text": "Overall, we pay special attention to both human and machine readability.",
        "x1": 4.738095283508301,
        "x2": 4.20383882522583,
        "y1": 4.755175590515137,
        "y2": 5.196206569671631
      },
      {
        "r": 0,
        "text": "If pure ASCII data is written, or compressed data is reencoded to ASCII, the entire file including its header and sectioning metadata remains entirely in ASCII.",
        "trunc_text": "If pure ASCII data is written, or compressed data is reencoded to ASCII, the entire file including its header and sectio",
        "x1": 6.453871726989746,
        "x2": 7.086159706115723,
        "y1": 6.604069232940674,
        "y2": 3.3961424827575684
      },
      {
        "r": 0,
        "text": "If binary data is written, the metadata stays easy on the human eye.   ",
        "trunc_text": "If binary data is written, the metadata stays easy on the human eye.   ",
        "x1": 6.323326587677002,
        "x2": 6.951194763183594,
        "y1": 6.759194374084473,
        "y2": 3.2818939685821533
      },
      {
        "r": 0,
        "text": "We refer to this format as scda.",
        "trunc_text": "We refer to this format as scda.",
        "x1": 6.587334632873535,
        "x2": 7.232342720031738,
        "y1": 6.807425022125244,
        "y2": 3.193311929702759
      },
      {
        "r": 0,
        "text": "Conceptually, it lies one layer below and is oblivious to the definition of variables, the binary representation of numbers, considerations of endianness, and self-describing headers, which may all be specified on top of scda.",
        "trunc_text": "Conceptually, it lies one layer below and is oblivious to the definition of variables, the binary representation of numb",
        "x1": 6.471840858459473,
        "x2": 7.132233619689941,
        "y1": 6.6727375984191895,
        "y2": 3.310241937637329
      },
      {
        "r": 0,
        "text": "The main purpose of the format is to abstract any parallelism and provide sufficient structure as a foundation for a generic and flexible archival and checkpoint/restart.",
        "trunc_text": "The main purpose of the format is to abstract any parallelism and provide sufficient structure as a foundation for a gen",
        "x1": 6.40519380569458,
        "x2": 7.029829978942871,
        "y1": 6.693946361541748,
        "y2": 3.3604962825775146
      },
      {
        "r": 0,
        "text": "A documented reference implementation is available as part of the general-purpose libsc free software library.",
        "trunc_text": "A documented reference implementation is available as part of the general-purpose libsc free software library.",
        "x1": 7.741451740264893,
        "x2": 8.066771507263184,
        "y1": 6.48930549621582,
        "y2": 2.806636333465576
      },
      {
        "r": 0,
        "text": "This paper explores the integration of Large Language Models (LLMs) into Automatic Speech Recognition (ASR) systems to improve transcription accuracy.",
        "trunc_text": "This paper explores the integration of Large Language Models (LLMs) into Automatic Speech Recognition (ASR) systems to i",
        "x1": 5.0182881355285645,
        "x2": 3.3237593173980713,
        "y1": 3.2044801712036133,
        "y2": 6.517227649688721
      },
      {
        "r": 0,
        "text": "The increasing sophistication of LLMs, with their in-context learning capabilities and instruction-following behavior, has drawn significant attention in the field of Natural Language Processing (NLP).",
        "trunc_text": "The increasing sophistication of LLMs, with their in-context learning capabilities and instruction-following behavior, h",
        "x1": 5.420999050140381,
        "x2": 4.009821891784668,
        "y1": 3.7292721271514893,
        "y2": 6.523056507110596
      },
      {
        "r": 0,
        "text": "Our primary focus is to investigate the potential of using an LLM's in-context learning capabilities to enhance the performance of ASR systems, which currently face challenges such as ambient noise, speaker accents, and complex linguistic contexts.",
        "trunc_text": "Our primary focus is to investigate the potential of using an LLM's in-context learning capabilities to enhance the perf",
        "x1": 5.154312610626221,
        "x2": 3.473813056945801,
        "y1": 3.2402844429016113,
        "y2": 6.538867473602295
      },
      {
        "r": 0,
        "text": "We designed a study using the Aishell-1 and LibriSpeech datasets, with ChatGPT and GPT-4 serving as benchmarks for LLM capabilities.",
        "trunc_text": "We designed a study using the Aishell-1 and LibriSpeech datasets, with ChatGPT and GPT-4 serving as benchmarks for LLM c",
        "x1": 6.4717817306518555,
        "x2": 4.760863304138184,
        "y1": 3.732724905014038,
        "y2": 6.758472919464111
      },
      {
        "r": 0,
        "text": "Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in-context learning for ASR applications.",
        "trunc_text": "Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in",
        "x1": 5.138372421264648,
        "x2": 3.574059247970581,
        "y1": 3.360194206237793,
        "y2": 6.392197132110596
      },
      {
        "r": 0,
        "text": "Despite further exploration with varied settings and models, the corrected sentences from the LLMs frequently resulted in higher Word Error Rates (WER), demonstrating the limitations of LLMs in speech applications.",
        "trunc_text": "Despite further exploration with varied settings and models, the corrected sentences from the LLMs frequently resulted i",
        "x1": 5.107995986938477,
        "x2": 3.433295488357544,
        "y1": 3.213407516479492,
        "y2": 6.517754554748535
      },
      {
        "r": 0,
        "text": "This paper provides a detailed overview of these experiments, their results, and implications, establishing that using LLMs' in-context learning capabilities to correct potential errors in speech recognition transcriptions is still a challenging task at the current stage.",
        "trunc_text": "This paper provides a detailed overview of these experiments, their results, and implications, establishing that using L",
        "x1": 5.122913360595703,
        "x2": 3.465599298477173,
        "y1": 3.2334492206573486,
        "y2": 6.533501148223877
      },
      {
        "r": 0,
        "text": "The application offers functionalities for data loading, management, summarization, basic graphs, advanced analysis, and contact.",
        "trunc_text": "The application offers functionalities for data loading, management, summarization, basic graphs, advanced analysis, and",
        "x1": 5.757659912109375,
        "x2": 6.490729808807373,
        "y1": 7.050039291381836,
        "y2": 2.696439743041992
      },
      {
        "r": 0,
        "text": "Additionally, the application offers statistical tools such as time series analysis using ARIMA and SARIMA models, forecasting, and Ljung-Box statistic.",
        "trunc_text": "Additionally, the application offers statistical tools such as time series analysis using ARIMA and SARIMA models, forec",
        "x1": 5.554285049438477,
        "x2": 6.306642532348633,
        "y1": 7.059990882873535,
        "y2": 2.6999123096466064
      },
      {
        "r": 0,
        "text": "Its user-friendly interface empowers individuals from various domains, including beginners in statistics, to make informed decisions.",
        "trunc_text": "Its user-friendly interface empowers individuals from various domains, including beginners in statistics, to make inform",
        "x1": 5.875085830688477,
        "x2": 6.386085510253906,
        "y1": 6.7576212882995605,
        "y2": 3.1416149139404297
      },
      {
        "r": 0,
        "text": "Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.",
        "trunc_text": "Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evalua",
        "x1": 3.8149402141571045,
        "x2": 2.406956195831299,
        "y1": 3.3132190704345703,
        "y2": 4.973773956298828
      },
      {
        "r": 0,
        "text": "Captions are crucial for understanding scientific visualizations and documents.",
        "trunc_text": "Captions are crucial for understanding scientific visualizations and documents.",
        "x1": 3.820626974105835,
        "x2": 2.329404354095459,
        "y1": 3.316770553588867,
        "y2": 4.993471622467041
      },
      {
        "r": 0,
        "text": "Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences.",
        "trunc_text": "Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, m",
        "x1": 3.8176677227020264,
        "x2": 2.3120675086975098,
        "y1": 3.2806923389434814,
        "y2": 4.976884841918945
      },
      {
        "r": 0,
        "text": "To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences.",
        "trunc_text": "To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption gen",
        "x1": 3.8991928100585938,
        "x2": 2.3342082500457764,
        "y1": 3.2995457649230957,
        "y2": 4.999024868011475
      },
      {
        "r": 0,
        "text": "Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences.",
        "trunc_text": "Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforceme",
        "x1": 3.84675669670105,
        "x2": 2.3644027709960938,
        "y1": 3.265014171600342,
        "y2": 5.02269983291626
      },
      {
        "r": 0,
        "text": "We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models.",
        "trunc_text": "We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning acr",
        "x1": 3.2116024494171143,
        "x2": 3.7538819313049316,
        "y1": 5.813749313354492,
        "y2": 4.714362621307373
      },
      {
        "r": 0,
        "text": "In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively.",
        "trunc_text": "In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROU",
        "x1": 4.59236478805542,
        "x2": 5.584161281585693,
        "y1": 6.365384578704834,
        "y2": 4.073418140411377
      },
      {
        "r": 0,
        "text": "In this paper, we introduce RetouchingFFHQ, a large-scale and fine-grained face retouching dataset that contains over half a million conditionally-retouched images.",
        "trunc_text": "In this paper, we introduce RetouchingFFHQ, a large-scale and fine-grained face retouching dataset that contains over ha",
        "x1": 2.5015811920166016,
        "x2": 3.8957674503326416,
        "y1": 7.6797194480896,
        "y2": 1.7088048458099365
      },
      {
        "r": 0,
        "text": "RetouchingFFHQ stands out from previous datasets due to its large scale, high quality, fine-grainedness, and customization",
        "trunc_text": "RetouchingFFHQ stands out from previous datasets due to its large scale, high quality, fine-grainedness, and customizati",
        "x1": 2.506588935852051,
        "x2": 3.8804049491882324,
        "y1": 7.6739277839660645,
        "y2": 1.707607626914978
      },
      {
        "r": 0,
        "text": "With the proposed new dataset, we believe there is great potential for future work to tackle the challenging problem of real-world fine-grained face retouching detection.",
        "trunc_text": "With the proposed new dataset, we believe there is great potential for future work to tackle the challenging problem of ",
        "x1": 2.5445263385772705,
        "x2": 3.887500047683716,
        "y1": 7.642125129699707,
        "y2": 1.7427653074264526
      },
      {
        "r": 0,
        "text": "The widespread use of face retouching filters on short-video platforms has raised concerns about the authenticity of digital appearances and the impact of deceptive advertising.",
        "trunc_text": "The widespread use of face retouching filters on short-video platforms has raised concerns about the authenticity of dig",
        "x1": 2.553922414779663,
        "x2": 3.8501622676849365,
        "y1": 7.680905818939209,
        "y2": 1.7027071714401245
      },
      {
        "r": 0,
        "text": "To address these issues, there is a pressing need to develop advanced face retouching techniques.",
        "trunc_text": "To address these issues, there is a pressing need to develop advanced face retouching techniques.",
        "x1": 2.571132183074951,
        "x2": 3.9033727645874023,
        "y1": 7.696365833282471,
        "y2": 1.762710452079773
      },
      {
        "r": 0,
        "text": "However, the lack of large-scale and fine-grained face retouching datasets has been a major obstacle to progress in this field.  ",
        "trunc_text": "However, the lack of large-scale and fine-grained face retouching datasets has been a major obstacle to progress in this",
        "x1": 2.500277042388916,
        "x2": 3.8898510932922363,
        "y1": 7.6910719871521,
        "y2": 1.6902984380722046
      },
      {
        "r": 0,
        "text": "RetouchingFFHQ stands out from previous datasets due to its large scale, high quality, fine-grainedness, and customization.",
        "trunc_text": "RetouchingFFHQ stands out from previous datasets due to its large scale, high quality, fine-grainedness, and customizati",
        "x1": 2.513324737548828,
        "x2": 3.8794989585876465,
        "y1": 7.711585998535156,
        "y2": 1.7306064367294312
      },
      {
        "r": 0,
        "text": "By including four typical types of face  multi-retouching type, and multi-retouching level estimation problem.",
        "trunc_text": "By including four typical types of face  multi-retouching type, and multi-retouching level estimation problem.",
        "x1": 2.5758609771728516,
        "x2": 3.935149669647217,
        "y1": 7.721739768981934,
        "y2": 1.6548614501953125
      },
      {
        "r": 0,
        "text": "Additionally, we propose a Multi-granularity Attention Module (MAM) as a plugin for CNN backbones for enhanced cross-scale representation learning.",
        "trunc_text": "Additionally, we propose a Multi-granularity Attention Module (MAM) as a plugin for CNN backbones for enhanced cross-sca",
        "x1": 3.9376559257507324,
        "x2": 2.4256975650787354,
        "y1": 3.757981061935425,
        "y2": 4.705504417419434
      },
      {
        "r": 0,
        "text": "Extensive experiments using different baselines as well as our proposed method on RetouchingFFHQ show decent performance on face retouching detection.",
        "trunc_text": "Extensive experiments using different baselines as well as our proposed method on RetouchingFFHQ show decent performance",
        "x1": 2.5494072437286377,
        "x2": 3.8960201740264893,
        "y1": 7.638518333435059,
        "y2": 1.704453706741333
      },
      {
        "r": 0,
        "text": "In this study, we introduce an event-based dataset on fine-grained manipulation actions and perform an experimental study on the use of transformers for action prediction with events",
        "trunc_text": "In this study, we introduce an event-based dataset on fine-grained manipulation actions and perform an experimental stud",
        "x1": 2.7822558879852295,
        "x2": 4.00795316696167,
        "y1": 8.851359367370605,
        "y2": -0.7277156710624695
      },
      {
        "r": 0,
        "text": "Finally, we release the new event dataset, which is the first in the literature for manipulation action recognition.",
        "trunc_text": "Finally, we release the new event dataset, which is the first in the literature for manipulation action recognition.",
        "x1": 2.6776859760284424,
        "x2": 3.9025840759277344,
        "y1": 8.914202690124512,
        "y2": -0.704689085483551
      },
      {
        "r": 0,
        "text": "Neuromorphic visual sensors are artificial retinas that output sequences of asynchronous events when brightness changes occur in the scene.",
        "trunc_text": "Neuromorphic visual sensors are artificial retinas that output sequences of asynchronous events when brightness changes ",
        "x1": 1.97693932056427,
        "x2": 4.092166900634766,
        "y1": 9.96465015411377,
        "y2": 0.6362077593803406
      },
      {
        "r": 0,
        "text": "These sensors offer many advantages including very high temporal resolution, no motion blur and smart data compression ideal for real-time processing. .",
        "trunc_text": "These sensors offer many advantages including very high temporal resolution, no motion blur and smart data compression i",
        "x1": 3.0601539611816406,
        "x2": 4.577234268188477,
        "y1": 9.66333293914795,
        "y2": 0.11731395870447159
      },
      {
        "r": 0,
        "text": "There is enormous interest in the fields of cognitive robotics and human-robot interaction on understanding and predicting human actions as early as possible.",
        "trunc_text": "There is enormous interest in the fields of cognitive robotics and human-robot interaction on understanding and predicti",
        "x1": 3.038236379623413,
        "x2": 4.372328281402588,
        "y1": 9.214238166809082,
        "y2": -0.5719005465507507
      },
      {
        "r": 0,
        "text": "Early prediction allows anticipating complex stages for planning, enabling effective and real-time interaction.",
        "trunc_text": "Early prediction allows anticipating complex stages for planning, enabling effective and real-time interaction.",
        "x1": 3.0436694622039795,
        "x2": 4.415681838989258,
        "y1": 9.11897087097168,
        "y2": -0.6688441634178162
      },
      {
        "r": 0,
        "text": "Our Transformer network uses events to predict manipulation actions as they occur, using online inference.",
        "trunc_text": "Our Transformer network uses events to predict manipulation actions as they occur, using online inference.",
        "x1": 2.7989416122436523,
        "x2": 4.025538444519043,
        "y1": 8.715327262878418,
        "y2": -0.7207086086273193
      },
      {
        "r": 0,
        "text": "The model succeeds at predicting actions early on, building up confidence over time and achieving state-of-the-art classification.",
        "trunc_text": "The model succeeds at predicting actions early on, building up confidence over time and achieving state-of-the-art class",
        "x1": 3.0658905506134033,
        "x2": 4.1170735359191895,
        "y1": 8.918659210205078,
        "y2": -0.7449308037757874
      },
      {
        "r": 0,
        "text": "Moreover, the attention-based transformer architecture allows us to study the role of the spatio-temporal patterns selected by the model.",
        "trunc_text": "Moreover, the attention-based transformer architecture allows us to study the role of the spatio-temporal patterns selec",
        "x1": 2.781041145324707,
        "x2": 2.381789445877075,
        "y1": 8.450185775756836,
        "y2": 2.3218088150024414
      },
      {
        "r": 0,
        "text": "Our experiments show that the Transformer network captures action dynamic features outperforming video-based approaches and succeeding with scenarios where the differences between actions lie in very subtle cues.",
        "trunc_text": "Our experiments show that the Transformer network captures action dynamic features outperforming video-based approaches ",
        "x1": 2.6108040809631348,
        "x2": 2.846113443374634,
        "y1": 8.632658958435059,
        "y2": 1.1318718194961548
      },
      {
        "r": 0,
        "text": "Code will be available at https://github.com/DaniDeniz/EventVisio",
        "trunc_text": "Code will be available at https://github.com/DaniDeniz/EventVisio",
        "x1": 8.287457466125488,
        "x2": 8.838475227355957,
        "y1": 7.577404975891113,
        "y2": 2.336435556411743
      },
      {
        "r": 0,
        "text": "This dataset encompasses three image collections in which rodent neuronal cells' nuclei and cytoplasm are stained with diverse markers to highlight their anatomical or functional characteristics",
        "trunc_text": "This dataset encompasses three image collections in which rodent neuronal cells' nuclei and cytoplasm are stained with d",
        "x1": 1.7527328729629517,
        "x2": 3.5208349227905273,
        "y1": 7.002979278564453,
        "y2": 1.8845576047897339
      },
      {
        "r": 0,
        "text": "Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting",
        "trunc_text": "Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, o",
        "x1": 0.9041340947151184,
        "x2": 2.1880054473876953,
        "y1": 7.340303897857666,
        "y2": 1.603177547454834
      },
      {
        "r": 0,
        "text": "First, given the variety of annotations and their accessible formats, we envision our work facilitating methodological advancements in computer vision approaches for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas",
        "trunc_text": "First, given the variety of annotations and their accessible formats, we envision our work facilitating methodological a",
        "x1": 1.0149805545806885,
        "x2": 2.45702862739563,
        "y1": 7.206521034240723,
        "y2": 1.4014548063278198
      },
      {
        "r": 0,
        "text": "The data are available at: https://amsacta.unibo.it/id/eprint/7347",
        "trunc_text": "The data are available at: https://amsacta.unibo.it/id/eprint/7347",
        "x1": 7.172682285308838,
        "x2": 7.538913726806641,
        "y1": 8.002861022949219,
        "y2": 1.833769679069519
      },
      {
        "r": 0,
        "text": "Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annotations, designed to foster innovative research in the domains of Life Sciences and Deep Learning. .",
        "trunc_text": "Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annot",
        "x1": 1.658272385597229,
        "x2": 3.317843198776245,
        "y1": 6.988468170166016,
        "y2": 1.9167132377624512
      },
      {
        "r": 0,
        "text": "Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting.",
        "trunc_text": "Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, o",
        "x1": 0.8999128937721252,
        "x2": 2.162463426589966,
        "y1": 7.362030982971191,
        "y2": 1.6322271823883057
      },
      {
        "r": 0,
        "text": "The contribution is two-fold.",
        "trunc_text": "The contribution is two-fold.",
        "x1": 4.82585334777832,
        "x2": 6.211182117462158,
        "y1": 8.268561363220215,
        "y2": 4.111898422241211
      },
      {
        "r": 0,
        "text": "First, given for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas.",
        "trunc_text": "First, given for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning",
        "x1": 1.0292266607284546,
        "x2": 2.445899486541748,
        "y1": 7.059238433837891,
        "y2": 1.5622193813323975
      },
      {
        "r": 0,
        "text": "Second, by enabling extensive exploration and benchma",
        "trunc_text": "Second, by enabling extensive exploration and benchma",
        "x1": 4.671114921569824,
        "x2": 4.459412097930908,
        "y1": 6.660494804382324,
        "y2": 3.992178201675415
      },
      {
        "r": 0,
        "text": "Traditional Chinese Painting (TCP) is an invaluable cultural heritage resource and a unique visual art style.",
        "trunc_text": "Traditional Chinese Painting (TCP) is an invaluable cultural heritage resource and a unique visual art style.",
        "x1": 6.355337142944336,
        "x2": 6.010117053985596,
        "y1": 5.834901809692383,
        "y2": 5.22206449508667
      },
      {
        "r": 0,
        "text": "In recent years, increasing interest has been placed on digitalizing TCPs to preserve and revive the culture.",
        "trunc_text": "In recent years, increasing interest has been placed on digitalizing TCPs to preserve and revive the culture.",
        "x1": 6.380686283111572,
        "x2": 6.020004749298096,
        "y1": 5.816457271575928,
        "y2": 5.234255313873291
      },
      {
        "r": 0,
        "text": "The resulting digital copies have enabled the advancement of computational methods for structured and systematic understanding of TCPs.",
        "trunc_text": "The resulting digital copies have enabled the advancement of computational methods for structured and systematic underst",
        "x1": 6.352293491363525,
        "x2": 5.998893737792969,
        "y1": 5.78102970123291,
        "y2": 5.2647480964660645
      },
      {
        "r": 0,
        "text": "To explore this topic, we conducted an in-depth analysis of 92 pieces of literature.",
        "trunc_text": "To explore this topic, we conducted an in-depth analysis of 92 pieces of literature.",
        "x1": 4.8771891593933105,
        "x2": 6.113553524017334,
        "y1": 8.3767728805542,
        "y2": 3.80376935005188
      },
      {
        "r": 0,
        "text": "We examined the current use of computer technologies on TCPs from three perspectives, based on numerous conversations with specialists.",
        "trunc_text": "We examined the current use of computer technologies on TCPs from three perspectives, based on numerous conversations wi",
        "x1": 6.372971057891846,
        "x2": 5.994333267211914,
        "y1": 5.723133087158203,
        "y2": 5.252900123596191
      },
      {
        "r": 0,
        "text": "First, in light of the \"Six Principles of Painting\" theory, we categorized the articles according to their research focus on artistic elements.",
        "trunc_text": "First, in light of the \"Six Principles of Painting\" theory, we categorized the articles according to their research focu",
        "x1": 3.855620861053467,
        "x2": 5.418476581573486,
        "y1": 8.514755249023438,
        "y2": 1.3090089559555054
      },
      {
        "r": 0,
        "text": "Second, we created a four-stage framework to illustrate the purposes of TCP applications.",
        "trunc_text": "Second, we created a four-stage framework to illustrate the purposes of TCP applications.",
        "x1": 6.330599784851074,
        "x2": 5.991469383239746,
        "y1": 5.764347553253174,
        "y2": 5.267467021942139
      },
      {
        "r": 0,
        "text": "Third, we summarized the popular computational techniques applied to TCPs.",
        "trunc_text": "Third, we summarized the popular computational techniques applied to TCPs.",
        "x1": 6.339875221252441,
        "x2": 5.96691370010376,
        "y1": 5.769615650177002,
        "y2": 5.239360809326172
      },
      {
        "r": 0,
        "text": "The framework also provides insights into potential applications and future prospects, with professional opinion.",
        "trunc_text": "The framework also provides insights into potential applications and future prospects, with professional opinion.",
        "x1": 4.8623046875,
        "x2": 5.996049404144287,
        "y1": 8.567617416381836,
        "y2": 4.291512966156006
      },
      {
        "r": 0,
        "text": "The list of surveyed publications and related information is available online at https://ca4tcp.com.",
        "trunc_text": "The list of surveyed publications and related information is available online at https://ca4tcp.com.",
        "x1": 6.180426120758057,
        "x2": 6.8003830909729,
        "y1": 7.565116882324219,
        "y2": 2.339735269546509
      },
      {
        "r": 0,
        "text": "Using the datasets built from in-the-wild images, our method gradually presents a controllable appearance editing function",
        "trunc_text": "Using the datasets built from in-the-wild images, our method gradually presents a controllable appearance editing functi",
        "x1": 2.1355936527252197,
        "x2": 3.722646713256836,
        "y1": 7.7005109786987305,
        "y2": 1.5988563299179077
      },
      {
        "r": 0,
        "text": "We will release the dataset and code on https://lty2226262.github.io/car-studio/ to facilitate further research in the field.",
        "trunc_text": "We will release the dataset and code on https://lty2226262.github.io/car-studio/ to facilitate further research in the f",
        "x1": 7.145564079284668,
        "x2": 7.764983177185059,
        "y1": 7.704531669616699,
        "y2": 2.085916519165039
      },
      {
        "r": 0,
        "text": "Compositional neural scene graph studies have shown that radiance fields can be an efficient tool in an editable autonomous driving simulator.",
        "trunc_text": "Compositional neural scene graph studies have shown that radiance fields can be an efficient tool in an editable autonom",
        "x1": 2.4701907634735107,
        "x2": 4.479189872741699,
        "y1": 10.074626922607422,
        "y2": 0.40091803669929504
      },
      {
        "r": 0,
        "text": "However, previous studies learned within a sequence of autonomous driving datasets, resulting in unsatisfactory blurring when rotating the car in the simulator.",
        "trunc_text": "However, previous studies learned within a sequence of autonomous driving datasets, resulting in unsatisfactory blurring",
        "x1": 2.624100685119629,
        "x2": 4.600581645965576,
        "y1": 10.062175750732422,
        "y2": 0.3753696382045746
      },
      {
        "r": 0,
        "text": "In this letter, we propose a pipeline for learning unconstrained images and building a dataset from processed images.",
        "trunc_text": "In this letter, we propose a pipeline for learning unconstrained images and building a dataset from processed images.",
        "x1": 1.9879077672958374,
        "x2": 3.637382745742798,
        "y1": 7.315383434295654,
        "y2": 1.6928176879882812
      },
      {
        "r": 0,
        "text": "To meet the requirements of the simulator, which demands that the vehicle maintain clarity when the perspective changes and that the contour remains sharp from the background to avoid artifacts when editing, we design a radiation field of the vehicle, a crucial part of the urban scene foreground.",
        "trunc_text": "To meet the requirements of the simulator, which demands that the vehicle maintain clarity when the perspective changes ",
        "x1": 2.6886279582977295,
        "x2": 4.580243110656738,
        "y1": 10.064275741577148,
        "y2": 0.35701069235801697
      },
      {
        "r": 0,
        "text": "Through experiments, we demonstrate that our model achieves competitive performance compared to baselines. .",
        "trunc_text": "Through experiments, we demonstrate that our model achieves competitive performance compared to baselines. .",
        "x1": 4.0037078857421875,
        "x2": 4.491018772125244,
        "y1": 6.224398136138916,
        "y2": 3.791468620300293
      },
      {
        "r": 0,
        "text": "We will release the dataset and code on https://lty2226262.github.io/car-studio/ to facilitate further research in the fie",
        "trunc_text": "We will release the dataset and code on https://lty2226262.github.io/car-studio/ to facilitate further research in the f",
        "x1": 7.117008209228516,
        "x2": 7.783912181854248,
        "y1": 7.648959159851074,
        "y2": 2.138698101043701
      },
      {
        "r": 0,
        "text": "To solve this task, we build a heterogeneous multi-agent tidying-up benchmark dataset in a large number of houses with multiple rooms based on ProcTHOR-10K.",
        "trunc_text": "To solve this task, we build a heterogeneous multi-agent tidying-up benchmark dataset in a large number of houses with m",
        "x1": 6.015069484710693,
        "x2": 4.800170421600342,
        "y1": 4.330073356628418,
        "y2": 2.8776726722717285
      },
      {
        "r": 0,
        "text": "Multi-agent embodied tasks have recently been studied in complex indoor visual environments.",
        "trunc_text": "Multi-agent embodied tasks have recently been studied in complex indoor visual environments.",
        "x1": 6.020981311798096,
        "x2": 4.242302894592285,
        "y1": 4.156507968902588,
        "y2": 7.593174934387207
      },
      {
        "r": 0,
        "text": "Collaboration among multiple agents can improve work efficiency and has significant practical value.",
        "trunc_text": "Collaboration among multiple agents can improve work efficiency and has significant practical value.",
        "x1": 6.058979034423828,
        "x2": 4.282104015350342,
        "y1": 4.111149311065674,
        "y2": 7.607621669769287
      },
      {
        "r": 0,
        "text": "However, most of the existing research focuses on homogeneous multi-agent tasks.",
        "trunc_text": "However, most of the existing research focuses on homogeneous multi-agent tasks.",
        "x1": 6.0290374755859375,
        "x2": 4.300132751464844,
        "y1": 4.21917200088501,
        "y2": 7.6253838539123535
      },
      {
        "r": 0,
        "text": "Compared with homogeneous agents, heterogeneous agents can leverage their different capabilities to allocate corresponding sub-tasks and cooperate to complete complex tasks.",
        "trunc_text": "Compared with homogeneous agents, heterogeneous agents can leverage their different capabilities to allocate correspondi",
        "x1": 5.965475082397461,
        "x2": 4.304276943206787,
        "y1": 4.256414413452148,
        "y2": 7.581475734710693
      },
      {
        "r": 0,
        "text": "Heterogeneous multi-agent tasks are common in real-world scenarios, and the collaboration strategy among heterogeneous agents is a challenging and important problem to be solved.",
        "trunc_text": "Heterogeneous multi-agent tasks are common in real-world scenarios, and the collaboration strategy among heterogeneous a",
        "x1": 6.044524192810059,
        "x2": 4.322589874267578,
        "y1": 4.163333892822266,
        "y2": 7.656591892242432
      },
      {
        "r": 0,
        "text": "To study collaboration among heterogeneous agents, we propose the heterogeneous multi-agent tidying-up task, in which multiple heterogeneous agents with different capabilities collaborate with each other to detect misplaced objects and place them in reasonable locations.",
        "trunc_text": "To study collaboration among heterogeneous agents, we propose the heterogeneous multi-agent tidying-up task, in which mu",
        "x1": 6.100832939147949,
        "x2": 4.287842273712158,
        "y1": 4.26023006439209,
        "y2": 7.646531105041504
      },
      {
        "r": 0,
        "text": "This is a demanding task since it requires agents to make the best use of their different capabilities to conduct reasonable task planning and complete the whole task.  ",
        "trunc_text": "This is a demanding task since it requires agents to make the best use of their different capabilities to conduct reason",
        "x1": 6.033448219299316,
        "x2": 4.363101005554199,
        "y1": 4.206526756286621,
        "y2": 7.504972457885742
      },
      {
        "r": 0,
        "text": "We propose the hierarchical decision model based on misplaced object detection, reasonable receptacle prediction, as well as the handshake-based group communication mechanism.",
        "trunc_text": "We propose the hierarchical decision model based on misplaced object detection, reasonable receptacle prediction, as wel",
        "x1": 0.5388190746307373,
        "x2": 2.056114673614502,
        "y1": 7.593660354614258,
        "y2": 1.2172595262527466
      },
      {
        "r": 0,
        "text": "Extensive experiments are conducted to demonstrate the effectiveness of the proposed model.",
        "trunc_text": "Extensive experiments are conducted to demonstrate the effectiveness of the proposed model.",
        "x1": 5.495563983917236,
        "x2": 6.585172176361084,
        "y1": 8.903402328491211,
        "y2": 4.945082664489746
      },
      {
        "r": 0,
        "text": "The project's website and videos of experiments can be found at https://hetercol.github.io/.",
        "trunc_text": "The project's website and videos of experiments can be found at https://hetercol.github.io/.",
        "x1": 5.628384590148926,
        "x2": 6.726992130279541,
        "y1": 8.80181884765625,
        "y2": 4.723001480102539
      },
      {
        "r": 0,
        "text": "The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking.",
        "trunc_text": "The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous ",
        "x1": 3.3371150493621826,
        "x2": 4.955095291137695,
        "y1": 9.825932502746582,
        "y2": -0.4790072739124298
      },
      {
        "r": 0,
        "text": "While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets.",
        "trunc_text": "While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it",
        "x1": 5.671968460083008,
        "x2": 6.321621894836426,
        "y1": 7.009474277496338,
        "y2": 2.873344898223877
      },
      {
        "r": 0,
        "text": "To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets.",
        "trunc_text": "To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets.",
        "x1": 3.5593013763427734,
        "x2": 5.025426864624023,
        "y1": 9.773430824279785,
        "y2": -0.4340353310108185
      },
      {
        "r": 0,
        "text": "At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data.",
        "trunc_text": "At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data.",
        "x1": 4.001838684082031,
        "x2": 5.618433475494385,
        "y1": 9.775463104248047,
        "y2": -0.23717278242111206
      },
      {
        "r": 0,
        "text": "As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights.",
        "trunc_text": "As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing traject",
        "x1": 3.361469268798828,
        "x2": 4.978826522827148,
        "y1": 9.821532249450684,
        "y2": -0.4274636507034302
      },
      {
        "r": 0,
        "text": "trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata",
        "trunc_text": "trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata",
        "x1": 4.202900409698486,
        "x2": 6.092565536499023,
        "y1": 9.780813217163086,
        "y2": 0.12201882153749466
      },
      {
        "r": 0,
        "text": "The core recipe of GrammarGPT is to leverage the hybrid dataset of ChatGPT-generated and human-annotated.",
        "trunc_text": "The core recipe of GrammarGPT is to leverage the hybrid dataset of ChatGPT-generated and human-annotated.",
        "x1": 4.9471025466918945,
        "x2": 3.3393313884735107,
        "y1": 3.1474430561065674,
        "y2": 6.4980268478393555
      },
      {
        "r": 0,
        "text": "Grammatical error correction aims to correct ungrammatical sentences automatically.",
        "trunc_text": "Grammatical error correction aims to correct ungrammatical sentences automatically.",
        "x1": 4.76859712600708,
        "x2": 2.9760947227478027,
        "y1": 3.144329309463501,
        "y2": 6.505489826202393
      },
      {
        "r": 0,
        "text": "Recently, some work has demonstrated the excellent capabilities of closed-source Large Language Models (LLMs, e.g., ChatGPT) in grammatical error correction.",
        "trunc_text": "Recently, some work has demonstrated the excellent capabilities of closed-source Large Language Models (LLMs, e.g., Chat",
        "x1": 4.992603302001953,
        "x2": 3.3107826709747314,
        "y1": 3.3843774795532227,
        "y2": 6.451784610748291
      },
      {
        "r": 0,
        "text": "However, the potential of open-source LLMs remains unexplored.",
        "trunc_text": "However, the potential of open-source LLMs remains unexplored.",
        "x1": 6.318532943725586,
        "x2": 5.13886022567749,
        "y1": 4.790218830108643,
        "y2": 6.131834506988525
      },
      {
        "r": 0,
        "text": "In this paper, we introduced GrammarGPT, an open-source LLM, to preliminary explore its potential for native Chinese grammatical error correction.  ",
        "trunc_text": "In this paper, we introduced GrammarGPT, an open-source LLM, to preliminary explore its potential for native Chinese gra",
        "x1": 4.788001537322998,
        "x2": 3.054264783859253,
        "y1": 3.1352105140686035,
        "y2": 6.485937118530273
      },
      {
        "r": 0,
        "text": "For grammatical errors with clues, we proposed a heuristic method to guide ChatGPT to generate ungrammatical sentences by providing those clues.",
        "trunc_text": "For grammatical errors with clues, we proposed a heuristic method to guide ChatGPT to generate ungrammatical sentences b",
        "x1": 4.903066635131836,
        "x2": 3.1113779544830322,
        "y1": 3.1537630558013916,
        "y2": 6.513689994812012
      },
      {
        "r": 0,
        "text": "For grammatical errors without clues, we collected ungrammatical sentences from publicly available websites and manually corrected them.",
        "trunc_text": "For grammatical errors without clues, we collected ungrammatical sentences from publicly available websites and manually",
        "x1": 4.763025283813477,
        "x2": 3.0058810710906982,
        "y1": 3.151794910430908,
        "y2": 6.5013837814331055
      },
      {
        "r": 0,
        "text": "In addition, we employed an error-invariant augmentation method to enhance the ability of the model to correct native Chinese grammatical errors.",
        "trunc_text": "In addition, we employed an error-invariant augmentation method to enhance the ability of the model to correct native Ch",
        "x1": 4.663517475128174,
        "x2": 3.0612854957580566,
        "y1": 3.179379940032959,
        "y2": 6.468390941619873
      },
      {
        "r": 0,
        "text": "We ultimately constructed about 1k parallel data and utilized these data to fine-tune open-source LLMs (e.g., Phoenix, released by The Chinese University of Hong Kong, Shenzhen) with instruction tuning.",
        "trunc_text": "We ultimately constructed about 1k parallel data and utilized these data to fine-tune open-source LLMs (e.g., Phoenix, r",
        "x1": 6.209160327911377,
        "x2": 4.975001335144043,
        "y1": 4.614804267883301,
        "y2": 6.2762980461120605
      },
      {
        "r": 0,
        "text": "The experimental results show that GrammarGPT outperforms the existing SOTA system significantly.",
        "trunc_text": "The experimental results show that GrammarGPT outperforms the existing SOTA system significantly.",
        "x1": 4.853464603424072,
        "x2": 3.225600481033325,
        "y1": 3.1745924949645996,
        "y2": 6.511866092681885
      },
      {
        "r": 0,
        "text": "Although model parameters are 20x larger than the SOTA baseline, the required amount of data for instruction tuning is 1200x smaller, illustrating the potential of open-source LLMs on native CGEC.",
        "trunc_text": "Although model parameters are 20x larger than the SOTA baseline, the required amount of data for instruction tuning is 1",
        "x1": 5.75048303604126,
        "x2": 4.660152912139893,
        "y1": 4.404854774475098,
        "y2": 6.2992939949035645
      },
      {
        "r": 0,
        "text": "Our GrammarGPT ranks $3^{rd}$ on NLPCC2023 SharedTask1, demonstrating our approach's effectiveness.",
        "trunc_text": "Our GrammarGPT ranks $3^{rd}$ on NLPCC2023 SharedTask1, demonstrating our approach's effectiveness.",
        "x1": 4.853261470794678,
        "x2": 3.192885398864746,
        "y1": 3.152524709701538,
        "y2": 6.466552257537842
      },
      {
        "r": 0,
        "text": "The code and data are available at \\url{https://github.com/FreedomIntelligence/GrammarGPT}.",
        "trunc_text": "The code and data are available at \\url{https://github.com/FreedomIntelligence/GrammarGPT}.",
        "x1": 7.641327857971191,
        "x2": 8.122660636901855,
        "y1": 7.730670928955078,
        "y2": 2.0693438053131104
      },
      {
        "r": 0,
        "text": "Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks.",
        "trunc_text": "Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying ep",
        "x1": 3.4926209449768066,
        "x2": 3.366037130355835,
        "y1": 5.105859756469727,
        "y2": 4.297192096710205
      },
      {
        "r": 0,
        "text": "However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions.",
        "trunc_text": "However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs",
        "x1": 3.2430806159973145,
        "x2": 3.216512441635132,
        "y1": 5.175005912780762,
        "y2": 4.077943325042725
      },
      {
        "r": 0,
        "text": "Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs.",
        "trunc_text": "Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variatio",
        "x1": 3.2571794986724854,
        "x2": 3.267148494720459,
        "y1": 5.207043647766113,
        "y2": 4.101719856262207
      },
      {
        "r": 0,
        "text": "In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations.",
        "trunc_text": "In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Mont",
        "x1": 3.492377519607544,
        "x2": 3.4112062454223633,
        "y1": 5.1807684898376465,
        "y2": 4.219034194946289
      },
      {
        "r": 0,
        "text": "Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models.",
        "trunc_text": "Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws fun",
        "x1": 3.330292224884033,
        "x2": 3.326563596725464,
        "y1": 5.175970077514648,
        "y2": 4.147641658782959
      },
      {
        "r": 0,
        "text": "To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations.",
        "trunc_text": "To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilitie",
        "x1": 2.852071523666382,
        "x2": 3.040634870529175,
        "y1": 5.544195652008057,
        "y2": 3.7923643589019775
      },
      {
        "r": 0,
        "text": "To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery.",
        "trunc_text": "To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery.",
        "x1": 3.426313638687134,
        "x2": 3.402128219604492,
        "y1": 5.207386493682861,
        "y2": 4.213170528411865
      },
      {
        "r": 0,
        "text": "Empirical evaluations on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.",
        "trunc_text": "Empirical evaluations on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of",
        "x1": 4.346774578094482,
        "x2": 4.993696212768555,
        "y1": 7.352647304534912,
        "y2": 2.4762957096099854
      },
      {
        "r": 0,
        "text": "We present YOLOBench, a benchmark comprised of 550+ YOLO-based object detection models on 4 different datasets and 4 different embedded hardware platforms (x86 CPU, ARM CPU, Nvidia GPU, NPU).",
        "trunc_text": "We present YOLOBench, a benchmark comprised of 550+ YOLO-based object detection models on 4 different datasets and 4 dif",
        "x1": 0.44952866435050964,
        "x2": 1.9618746042251587,
        "y1": 7.376844882965088,
        "y2": 1.4380663633346558
      },
      {
        "r": 0,
        "text": "We collect accuracy and latency numbers for a variety of YOLO-based one-stage detectors at different model scales by performing a fair, controlled comparison of these detectors with a fixed training environment (code and training hyperparameters).",
        "trunc_text": "We collect accuracy and latency numbers for a variety of YOLO-based one-stage detectors at different model scales by per",
        "x1": 0.41365155577659607,
        "x2": 1.9537168741226196,
        "y1": 7.278835296630859,
        "y2": 1.5730371475219727
      },
      {
        "r": 0,
        "text": "Pareto-optimality analysis of the collected data reveals that, if modern detection heads and training techniques are incorporated into the learning process, multiple architectures of the YOLO series achieve a good accuracy-latency trade-off, including older models like YOLOv3 and YOLOv4.",
        "trunc_text": "Pareto-optimality analysis of the collected data reveals that, if modern detection heads and training techniques are inc",
        "x1": 0.36800241470336914,
        "x2": 1.8735066652297974,
        "y1": 7.347084999084473,
        "y2": 1.4902260303497314
      },
      {
        "r": 0,
        "text": "We also evaluate training-free accuracy estimators used in neural architecture search on YOLOBench and demonstrate that, while most state-of-the-art zero-cost accuracy estimators are outperformed by a simple baseline like MAC count, some of them can be effectively used to predict Pareto-optimal detection models.",
        "trunc_text": "We also evaluate training-free accuracy estimators used in neural architecture search on YOLOBench and demonstrate that,",
        "x1": 0.38739365339279175,
        "x2": 1.874879002571106,
        "y1": 7.273761749267578,
        "y2": 1.55820894241333
      },
      {
        "r": 0,
        "text": "We showcase that by using a zero-cost proxy to identify a YOLO architecture competitive against a state-of-the-art YOLOv8 model on a Raspberry Pi 4 CPU.",
        "trunc_text": "We showcase that by using a zero-cost proxy to identify a YOLO architecture competitive against a state-of-the-art YOLOv",
        "x1": 0.3729957938194275,
        "x2": 1.7988686561584473,
        "y1": 7.334629058837891,
        "y2": 1.488445520401001
      },
      {
        "r": 0,
        "text": "The code and data are available at https://github.com/Deeplite/deeplite-torch-zoo",
        "trunc_text": "The code and data are available at https://github.com/Deeplite/deeplite-torch-zoo",
        "x1": 7.40887451171875,
        "x2": 7.899814605712891,
        "y1": 7.781668186187744,
        "y2": 1.9360212087631226
      },
      {
        "r": 0,
        "text": "Advancements in large pre-trained generative models have expanded their potential as effective data generators in visual recognition.",
        "trunc_text": "Advancements in large pre-trained generative models have expanded their potential as effective data generators in visual",
        "x1": 2.370781660079956,
        "x2": 3.0093414783477783,
        "y1": 6.5109992027282715,
        "y2": 2.8569602966308594
      },
      {
        "r": 0,
        "text": "This work delves into the impact of generative images, primarily comparing paradigms that harness external data (\\ie generative \\vs retrieval \\vs original).   ",
        "trunc_text": "This work delves into the impact of generative images, primarily comparing paradigms that harness external data (\\ie gen",
        "x1": 2.6072988510131836,
        "x2": 3.106328248977661,
        "y1": 6.308080196380615,
        "y2": 3.016517162322998
      },
      {
        "r": 0,
        "text": "Our key contributions are: \\textbf{1) GenBench Construction:}",
        "trunc_text": "Our key contributions are: \\textbf{1) GenBench Construction:}",
        "x1": 4.649815082550049,
        "x2": 3.5214426517486572,
        "y1": 4.2455525398254395,
        "y2": 5.781637668609619
      },
      {
        "r": 0,
        "text": "We devise \\textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548 categories, to appraise generative data across various visual recognition tasks.",
        "trunc_text": "We devise \\textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548 categories, to appraise generative data ",
        "x1": 2.7730438709259033,
        "x2": 3.806196451187134,
        "y1": 6.499858856201172,
        "y2": 2.8317716121673584
      },
      {
        "r": 0,
        "text": "\\textbf{2) CLER Score:} To address the insufficient correlation of existing metrics (\\eg, FID, CLIP score) with downstream recognition performance, we propose \\textbf{CLER}, a training-free metric indicating generative data's efficiency for recognition tasks prior to training.",
        "trunc_text": "\\textbf{2) CLER Score:} To address the insufficient correlation of existing metrics (\\eg, FID, CLIP score) with downstre",
        "x1": 2.846620559692383,
        "x2": 3.663292407989502,
        "y1": 6.101071357727051,
        "y2": 3.05959415435791
      },
      {
        "r": 0,
        "text": "\\textbf{3) New Baselines:} Comparisons of generative data with retrieved data from the same external pool help to elucidate the unique traits of generative data.",
        "trunc_text": "\\textbf{3) New Baselines:} Comparisons of generative data with retrieved data from the same external pool help to elucid",
        "x1": 3.071422815322876,
        "x2": 3.83406925201416,
        "y1": 6.267148494720459,
        "y2": 3.0198898315429688
      },
      {
        "r": 0,
        "text": "\\textbf{4)",
        "trunc_text": "\\textbf{4)",
        "x1": 4.010644435882568,
        "x2": 3.492280960083008,
        "y1": 4.3859543800354,
        "y2": 5.698287487030029
      },
      {
        "r": 0,
        "text": "Our exhaustive benchmark and analysis spotlight generative data's promise in visual recognition, while identifying key challenges for future investigation.",
        "trunc_text": "Our exhaustive benchmark and analysis spotlight generative data's promise in visual recognition, while identifying key c",
        "x1": 2.6977710723876953,
        "x2": 3.769714593887329,
        "y1": 6.503466606140137,
        "y2": 2.809248447418213
      },
      {
        "r": 0,
        "text": "We introduce this large-scale synthesised dataset of 250K photorealistic images and corresponding 3DMM parameters.",
        "trunc_text": "We introduce this large-scale synthesised dataset of 250K photorealistic images and corresponding 3DMM parameters.",
        "x1": 2.149867057800293,
        "x2": 3.639037609100342,
        "y1": 8.397265434265137,
        "y2": 1.1686738729476929
      },
      {
        "r": 0,
        "text": "Accurate 3D face shape estimation is an enabling technology with applications in healthcare, security, and creative industries, yet current state-of-the-art methods either rely on self-supervised training with 2D image data or supervised training with very limited 3D data.",
        "trunc_text": "Accurate 3D face shape estimation is an enabling technology with applications in healthcare, security, and creative indu",
        "x1": 2.2518036365509033,
        "x2": 3.172929286956787,
        "y1": 7.280348300933838,
        "y2": 1.4917843341827393
      },
      {
        "r": 0,
        "text": "To bridge this gap, we present a novel approach which uses a conditioned stable diffusion model for face image generation, leveraging the abundance of 2D facial information to inform 3D space.",
        "trunc_text": "To bridge this gap, we present a novel approach which uses a conditioned stable diffusion model for face image generatio",
        "x1": 2.1016457080841064,
        "x2": 3.1840829849243164,
        "y1": 7.149386405944824,
        "y2": 1.8282265663146973
      },
      {
        "r": 0,
        "text": "By conditioning stable diffusion on depth maps sampled from a 3D Morphable Model (3DMM) of the human face, we generate diverse and shape-consistent images, forming the basis of SynthFace.  ",
        "trunc_text": "By conditioning stable diffusion on depth maps sampled from a 3D Morphable Model (3DMM) of the human face, we generate d",
        "x1": 2.0649054050445557,
        "x2": 3.2015786170959473,
        "y1": 7.2122979164123535,
        "y2": 1.5932650566101074
      },
      {
        "r": 0,
        "text": "We further propose ControlFace, a deep neural network, trained on SynthFace, which achieves competitive performance on the NoW benchmark, without requiring 3D supervision or manual 3D asset creation.",
        "trunc_text": "We further propose ControlFace, a deep neural network, trained on SynthFace, which achieves competitive performance on t",
        "x1": 2.1718485355377197,
        "x2": 3.6789982318878174,
        "y1": 8.445944786071777,
        "y2": 1.0712906122207642
      },
      {
        "r": 0,
        "text": "To this aim we focused only on images generated by the famous DALL-E 2 and collected images (both original and generated) of five renowned artists.",
        "trunc_text": "To this aim we focused only on images generated by the famous DALL-E 2 and collected images (both original and generated",
        "x1": 2.383751392364502,
        "x2": 4.046688556671143,
        "y1": 7.93234395980835,
        "y2": 1.4614840745925903
      },
      {
        "r": 0,
        "text": "Dataset and code are available at: https://github.com/ictlab-unict/not-with-my-name .",
        "trunc_text": "Dataset and code are available at: https://github.com/ictlab-unict/not-with-my-name .",
        "x1": 7.1366071701049805,
        "x2": 7.600360870361328,
        "y1": 7.658900737762451,
        "y2": 2.1476736068725586
      },
      {
        "r": 0,
        "text": "Diffusion Models (DM) are highly effective at generating realistic, high-quality images.",
        "trunc_text": "Diffusion Models (DM) are highly effective at generating realistic, high-quality images.",
        "x1": 2.0004029273986816,
        "x2": 3.1843812465667725,
        "y1": 6.969145774841309,
        "y2": 1.912350058555603
      },
      {
        "r": 0,
        "text": "However, these models lack creativity and merely compose outputs based on their training data, guided by a textual input provided at creation time.",
        "trunc_text": "However, these models lack creativity and merely compose outputs based on their training data, guided by a textual input",
        "x1": 4.564558506011963,
        "x2": 3.3017404079437256,
        "y1": 3.8365862369537354,
        "y2": 5.975499153137207
      },
      {
        "r": 0,
        "text": "Is it acceptable to generate images reminiscent of an artist, employing his name as input?",
        "trunc_text": "Is it acceptable to generate images reminiscent of an artist, employing his name as input?",
        "x1": 2.394479274749756,
        "x2": 3.7913529872894287,
        "y1": 7.805811882019043,
        "y2": 1.5093637704849243
      },
      {
        "r": 0,
        "text": "This imply that if the DM is able to replicate an artist's work then it was trained on some or all of his artworks thus violating copyright.",
        "trunc_text": "This imply that if the DM is able to replicate an artist's work then it was trained on some or all of his artworks thus ",
        "x1": 2.413776159286499,
        "x2": 3.775848388671875,
        "y1": 7.829711437225342,
        "y2": 1.458857774734497
      },
      {
        "r": 0,
        "text": "In this paper, a preliminary study to infer the probability of use of an artist's name in the input string of a generated image is presented.  ",
        "trunc_text": "In this paper, a preliminary study to infer the probability of use of an artist's name in the input string of a generate",
        "x1": 2.3978331089019775,
        "x2": 3.826676368713379,
        "y1": 7.859180450439453,
        "y2": 1.4811402559280396
      },
      {
        "r": 0,
        "text": "Finally, a dedicated Siamese Neural Network was employed to have a first kind of probability.",
        "trunc_text": "Finally, a dedicated Siamese Neural Network was employed to have a first kind of probability.",
        "x1": 2.3317549228668213,
        "x2": 2.8479225635528564,
        "y1": 6.334539413452148,
        "y2": 2.7408621311187744
      },
      {
        "r": 0,
        "text": "Experimental results demonstrate that our approach is an optimal starting point and can be employed as a prior for predicting a complete input string of an investigated image.",
        "trunc_text": "Experimental results demonstrate that our approach is an optimal starting point and can be employed as a prior for predi",
        "x1": 2.1285603046417236,
        "x2": 3.393134593963623,
        "y1": 7.9534993171691895,
        "y2": 1.2402127981185913
      },
      {
        "r": 0,
        "text": "The consumption of podcast media has been increasing rapidly.",
        "trunc_text": "The consumption of podcast media has been increasing rapidly.",
        "x1": 3.5129051208496094,
        "x2": 2.259305238723755,
        "y1": 2.8021862506866455,
        "y2": 5.765183925628662
      },
      {
        "r": 0,
        "text": "Due to the lengthy nature of podcast episodes, users often carefully select which ones to listen to.",
        "trunc_text": "Due to the lengthy nature of podcast episodes, users often carefully select which ones to listen to.",
        "x1": 3.5146656036376953,
        "x2": 2.1833455562591553,
        "y1": 2.8249058723449707,
        "y2": 5.821738243103027
      },
      {
        "r": 0,
        "text": "Although episode descriptions aid users by providing a summary of the entire podcast, they do not provide a topic-by-topic breakdown.",
        "trunc_text": "Although episode descriptions aid users by providing a summary of the entire podcast, they do not provide a topic-by-top",
        "x1": 3.5120859146118164,
        "x2": 2.2205471992492676,
        "y1": 2.896472692489624,
        "y2": 5.8259758949279785
      },
      {
        "r": 0,
        "text": "This study explores the combined application of topic segmentation and text summarisation methods to investigate how podcast episode comprehension can be improved.",
        "trunc_text": "This study explores the combined application of topic segmentation and text summarisation methods to investigate how pod",
        "x1": 3.5283851623535156,
        "x2": 2.2709388732910156,
        "y1": 2.850186824798584,
        "y2": 5.831407070159912
      },
      {
        "r": 0,
        "text": "We have sampled 10 episodes from Spotify's English-Language Podcast Dataset and employed TextTiling and TextSplit to segment them.",
        "trunc_text": "We have sampled 10 episodes from Spotify's English-Language Podcast Dataset and employed TextTiling and TextSplit to seg",
        "x1": 3.6008472442626953,
        "x2": 2.3572678565979004,
        "y1": 2.8622000217437744,
        "y2": 5.692455291748047
      },
      {
        "r": 0,
        "text": "Moreover, three text summarisation models, namely T5, BART, and Pegasus, were applied to provide a very short title for each segment.",
        "trunc_text": "Moreover, three text summarisation models, namely T5, BART, and Pegasus, were applied to provide a very short title for ",
        "x1": 3.474553108215332,
        "x2": 2.4276340007781982,
        "y1": 3.2889702320098877,
        "y2": 5.841603755950928
      },
      {
        "r": 0,
        "text": "The segmentation part was evaluated using our annotated sample with the $P_k$ and WindowDiff ($WD$) metrics.",
        "trunc_text": "The segmentation part was evaluated using our annotated sample with the $P_k$ and WindowDiff ($WD$) metrics.",
        "x1": 0.783449649810791,
        "x2": 2.5470175743103027,
        "y1": 7.096861839294434,
        "y2": 1.3966466188430786
      },
      {
        "r": 0,
        "text": "A survey was also rolled out ($N=25$) to assess the quality of the generated summaries.",
        "trunc_text": "A survey was also rolled out ($N=25$) to assess the quality of the generated summaries.",
        "x1": 2.910146713256836,
        "x2": 1.9245857000350952,
        "y1": 3.808349132537842,
        "y2": 5.503452301025391
      },
      {
        "r": 0,
        "text": "The TextSplit algorithm achieved the lowest mean for both evaluation metrics ($\\bar{P_k}=0.41$ and $\\bar{WD}=0.41$), while the T5 model produced the best summaries, achieving a relevancy score only $8\\%$ less to the one achieved by the human-written titles.",
        "trunc_text": "The TextSplit algorithm achieved the lowest mean for both evaluation metrics ($\\bar{P_k}=0.41$ and $\\bar{WD}=0.41$), whi",
        "x1": 3.1865768432617188,
        "x2": 2.402684211730957,
        "y1": 3.611689805984497,
        "y2": 5.723160743713379
      },
      {
        "r": 0,
        "text": "Our team gathered a dataset of 19,779 accounts that meet standardized criteria to enable future research on bots in open-source projects.",
        "trunc_text": "Our team gathered a dataset of 19,779 accounts that meet standardized criteria to enable future research on bots in open",
        "x1": 7.586928844451904,
        "x2": 6.267671585083008,
        "y1": 3.481889009475708,
        "y2": 7.672161102294922
      },
      {
        "r": 0,
        "text": "Social coding platforms have revolutionized collaboration in software development, leading to using software bots for streamlining operations.",
        "trunc_text": "Social coding platforms have revolutionized collaboration in software development, leading to using software bots for st",
        "x1": 7.578125953674316,
        "x2": 6.233310699462891,
        "y1": 3.4683542251586914,
        "y2": 7.658453941345215
      },
      {
        "r": 0,
        "text": "However, The presence of open-source software (OSS) bots gives rise to problems including impersonation, spamming, bias, and security risks.",
        "trunc_text": "However, The presence of open-source software (OSS) bots gives rise to problems including impersonation, spamming, bias,",
        "x1": 7.603571891784668,
        "x2": 6.276472091674805,
        "y1": 3.4691174030303955,
        "y2": 7.665693759918213
      },
      {
        "r": 0,
        "text": "Identifying bot accounts and behavior is a challenging task in the OSS project.",
        "trunc_text": "Identifying bot accounts and behavior is a challenging task in the OSS project.",
        "x1": 7.59915018081665,
        "x2": 6.258169651031494,
        "y1": 3.4633421897888184,
        "y2": 7.691966533660889
      },
      {
        "r": 0,
        "text": "This research aims to investigate bots' behavior in open-source software projects and identify bot accounts with maximum possible accuracy.  ",
        "trunc_text": "This research aims to investigate bots' behavior in open-source software projects and identify bot accounts with maximum",
        "x1": 7.600886821746826,
        "x2": 6.271824359893799,
        "y1": 3.4659597873687744,
        "y2": 7.66868782043457
      },
      {
        "r": 0,
        "text": "We follow a rigorous workflow to ensure that the data we collect is accurate, generalizable, scalable, and up-to-date.",
        "trunc_text": "We follow a rigorous workflow to ensure that the data we collect is accurate, generalizable, scalable, and up-to-date.",
        "x1": 5.525330543518066,
        "x2": 6.116517543792725,
        "y1": 6.596436977386475,
        "y2": 3.283201217651367
      },
      {
        "r": 0,
        "text": "We've identified four types of bot accounts in open-source software projects by analyzing their behavior across 17 features in 5 dimensions.",
        "trunc_text": "We've identified four types of bot accounts in open-source software projects by analyzing their behavior across 17 featu",
        "x1": 7.602247714996338,
        "x2": 6.264139175415039,
        "y1": 3.4729866981506348,
        "y2": 7.675652503967285
      },
      {
        "r": 0,
        "text": "Our team created BotHawk, a highly effective model for detecting bots in open-source software projects.",
        "trunc_text": "Our team created BotHawk, a highly effective model for detecting bots in open-source software projects.",
        "x1": 7.619665622711182,
        "x2": 6.275322437286377,
        "y1": 3.4976646900177,
        "y2": 7.667390823364258
      },
      {
        "r": 0,
        "text": "It outperforms other models, achieving an AUC of 0.947 and an F1-score of 0.89.",
        "trunc_text": "It outperforms other models, achieving an AUC of 0.947 and an F1-score of 0.89.",
        "x1": 4.182827472686768,
        "x2": 5.115634441375732,
        "y1": 6.140926837921143,
        "y2": 3.980581045150757
      },
      {
        "r": 0,
        "text": "BotHawk can detect a wider variety of bots, including CI/CD and scanning bots.",
        "trunc_text": "BotHawk can detect a wider variety of bots, including CI/CD and scanning bots.",
        "x1": 7.571020126342773,
        "x2": 6.25481653213501,
        "y1": 3.472553014755249,
        "y2": 7.668100833892822
      },
      {
        "r": 0,
        "text": "Furthermore, we find that the number of followers, number of repositories, and tags contain the most relevant features to identify the account type.",
        "trunc_text": "Furthermore, we find that the number of followers, number of repositories, and tags contain the most relevant features t",
        "x1": 7.591462135314941,
        "x2": 6.2610273361206055,
        "y1": 3.5038487911224365,
        "y2": 7.655490875244141
      },
      {
        "r": 0,
        "text": "We curate a large-scale dataset to enable pre-training of 3D medical radiology images (MRI and CT).",
        "trunc_text": "We curate a large-scale dataset to enable pre-training of 3D medical radiology images (MRI and CT).",
        "x1": 1.5060696601867676,
        "x2": 2.9376323223114014,
        "y1": 6.808959007263184,
        "y2": 1.963875412940979
      },
      {
        "r": 0,
        "text": "Harnessing the power of pre-training on large-scale datasets like ImageNet forms a fundamental building block for the progress of representation learning-driven solutions in computer vision.",
        "trunc_text": "Harnessing the power of pre-training on large-scale datasets like ImageNet forms a fundamental building block for the pr",
        "x1": 2.264604091644287,
        "x2": 2.7406327724456787,
        "y1": 6.2460174560546875,
        "y2": 3.2208762168884277
      },
      {
        "r": 0,
        "text": "Medical images are inherently different from natural images as they are acquired in the form of many modalities (CT, MR, PET, Ultrasound etc.) and contain granulated information like tissue, lesion, organs etc.",
        "trunc_text": "Medical images are inherently different from natural images as they are acquired in the form of many modalities (CT, MR,",
        "x1": 1.7678948640823364,
        "x2": 2.6981968879699707,
        "y1": 6.47104549407959,
        "y2": 2.460190534591675
      },
      {
        "r": 0,
        "text": "These characteristics of medical images require special attention towards learning features representative of local context.",
        "trunc_text": "These characteristics of medical images require special attention towards learning features representative of local cont",
        "x1": 1.683583378791809,
        "x2": 2.7340540885925293,
        "y1": 6.626495838165283,
        "y2": 2.3192739486694336
      },
      {
        "r": 0,
        "text": "In this work, we focus on designing an effective pre-training framework for 3D radiology images.",
        "trunc_text": "In this work, we focus on designing an effective pre-training framework for 3D radiology images.",
        "x1": 1.5964279174804688,
        "x2": 2.8960537910461426,
        "y1": 6.877275466918945,
        "y2": 1.9913971424102783
      },
      {
        "r": 0,
        "text": "First, we propose a new masking strategy called local masking where the masking is performed across channel embeddings instead of tokens to improve the learning of local feature representations.",
        "trunc_text": "First, we propose a new masking strategy called local masking where the masking is performed across channel embeddings i",
        "x1": 2.890312433242798,
        "x2": 2.6227869987487793,
        "y1": 5.089662075042725,
        "y2": 3.8671822547912598
      },
      {
        "r": 0,
        "text": "We combine this with classical low-level perturbations like adding noise and downsampling to further enable low-level representation learning.",
        "trunc_text": "We combine this with classical low-level perturbations like adding noise and downsampling to further enable low-level re",
        "x1": 2.6183207035064697,
        "x2": 2.617662191390991,
        "y1": 5.6858367919921875,
        "y2": 3.5659751892089844
      },
      {
        "r": 0,
        "text": "To this end, we introduce Disruptive Autoencoders, a pre-training framework that attempts to reconstruct the original image from disruptions created by a combination of local masking and low-level perturbations.",
        "trunc_text": "To this end, we introduce Disruptive Autoencoders, a pre-training framework that attempts to reconstruct the original im",
        "x1": 2.045100688934326,
        "x2": 3.1762595176696777,
        "y1": 7.004472255706787,
        "y2": 2.1857306957244873
      },
      {
        "r": 0,
        "text": "Additionally, we also devise a cross-modal contrastive loss (CMCL) to accommodate the pre-training of multiple modalities in a single framework.  ",
        "trunc_text": "Additionally, we also devise a cross-modal contrastive loss (CMCL) to accommodate the pre-training of multiple modalitie",
        "x1": 2.842381715774536,
        "x2": 2.9739394187927246,
        "y1": 5.849470615386963,
        "y2": 3.779224157333374
      },
      {
        "r": 0,
        "text": "The proposed pre-training framework is tested across multiple downstream tasks and achieves state-of-the-art performance.",
        "trunc_text": "The proposed pre-training framework is tested across multiple downstream tasks and achieves state-of-the-art performance",
        "x1": 4.289330959320068,
        "x2": 4.178390026092529,
        "y1": 5.42429256439209,
        "y2": 4.528893947601318
      },
      {
        "r": 0,
        "text": "Notably, our proposed method tops the public test leaderboard of BTCV multi-organ segmentation challenge.",
        "trunc_text": "Notably, our proposed method tops the public test leaderboard of BTCV multi-organ segmentation challenge.",
        "x1": 0.7565867900848389,
        "x2": 2.546506404876709,
        "y1": 7.095240116119385,
        "y2": 1.4413864612579346
      },
      {
        "r": 0,
        "text": "In this paper, we introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset)",
        "trunc_text": "In this paper, we introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-s",
        "x1": 4.810444355010986,
        "x2": 3.3959672451019287,
        "y1": 3.679954767227173,
        "y2": 6.071994304656982
      },
      {
        "r": 0,
        "text": "Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subset of MIRACL, a publicly available information retrieval dataset.",
        "trunc_text": "Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop ",
        "x1": 4.753816604614258,
        "x2": 3.2732596397399902,
        "y1": 3.66131854057312,
        "y2": 6.109007358551025
      },
      {
        "r": 0,
        "text": "The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines that are capable of generating search results in natural language text, imbued with citations for supporting sources.",
        "trunc_text": "The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines ",
        "x1": 5.00120735168457,
        "x2": 3.624523878097534,
        "y1": 3.775029420852661,
        "y2": 6.250682353973389
      },
      {
        "r": 0,
        "text": "Building generative information-seeking models demands openly accessible datasets, which currently remain lacking.  for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations.",
        "trunc_text": "Building generative information-seeking models demands openly accessible datasets, which currently remain lacking.  for ",
        "x1": 4.911474227905273,
        "x2": 3.519362211227417,
        "y1": 3.6010794639587402,
        "y2": 6.195493698120117
      },
      {
        "r": 0,
        "text": "Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subontext citation style using an LLM, i.e. GPT-3.5.",
        "trunc_text": "Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop ",
        "x1": 4.762074947357178,
        "x2": 3.2520759105682373,
        "y1": 3.6818480491638184,
        "y2": 6.126900672912598
      },
      {
        "r": 0,
        "text": "Next, we ask human annotators to evaluate the LLM explanations based on two criteria: informativeness and attributability.",
        "trunc_text": "Next, we ask human annotators to evaluate the LLM explanations based on two criteria: informativeness and attributabilit",
        "x1": 2.835601806640625,
        "x2": 1.5688117742538452,
        "y1": 3.3333499431610107,
        "y2": 5.583792686462402
      },
      {
        "r": 0,
        "text": "HAGRID serves as a catalyst for the development of information-seeking models with better attribution capabilities.",
        "trunc_text": "HAGRID serves as a catalyst for the development of information-seeking models with better attribution capabilities.",
        "x1": 4.872262477874756,
        "x2": 3.4846248626708984,
        "y1": 3.765056610107422,
        "y2": 6.097890853881836
      },
      {
        "r": 0,
        "text": "Retriever-augmented instruction-following models are attractive alternatives to fine-tuned approaches for information-seeking tasks such as question answering (QA).",
        "trunc_text": "Retriever-augmented instruction-following models are attractive alternatives to fine-tuned approaches for information-se",
        "x1": 5.143650531768799,
        "x2": 4.008076190948486,
        "y1": 4.165677070617676,
        "y2": 5.990797996520996
      },
      {
        "r": 0,
        "text": "By simply prepending retrieved documents in its input along with an instruction, these models can be adapted to various information domains and tasks without additional fine-tuning.",
        "trunc_text": "By simply prepending retrieved documents in its input along with an instruction, these models can be adapted to various ",
        "x1": 5.126718997955322,
        "x2": 3.9938137531280518,
        "y1": 4.176410675048828,
        "y2": 6.014768600463867
      },
      {
        "r": 0,
        "text": "While the model responses tend to be natural and fluent, the additional verbosity makes traditional QA evaluation metrics such as exact match (EM) and F1 unreliable for accurately quantifying model performance.   ",
        "trunc_text": "While the model responses tend to be natural and fluent, the additional verbosity makes traditional QA evaluation metric",
        "x1": 4.874734401702881,
        "x2": 4.396839141845703,
        "y1": 4.975327014923096,
        "y2": 4.966619491577148
      },
      {
        "r": 0,
        "text": "In this work, we investigate the performance of instruction-following models across three information-seeking QA tasks.",
        "trunc_text": "In this work, we investigate the performance of instruction-following models across three information-seeking QA tasks.",
        "x1": 5.272274971008301,
        "x2": 4.113348484039307,
        "y1": 4.201142311096191,
        "y2": 5.972543716430664
      },
      {
        "r": 0,
        "text": "We use both automatic and human evaluation to evaluate these models along two dimensions: 1) how well they satisfy the user's information need (correctness), and 2) whether they produce a response based on the provided knowledge (faithfulness).",
        "trunc_text": "We use both automatic and human evaluation to evaluate these models along two dimensions: 1) how well they satisfy the u",
        "x1": 4.820577621459961,
        "x2": 4.403365612030029,
        "y1": 5.023510456085205,
        "y2": 4.992918014526367
      },
      {
        "r": 0,
        "text": "Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and faithfulness.",
        "trunc_text": "Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and f",
        "x1": 4.823166847229004,
        "x2": 4.407421112060547,
        "y1": 5.101628303527832,
        "y2": 5.034502029418945
      },
      {
        "r": 0,
        "text": "We then propose simple token-overlap based and model-based metrics that reflect the true performance of these models.",
        "trunc_text": "We then propose simple token-overlap based and model-based metrics that reflect the true performance of these models.",
        "x1": 4.286285400390625,
        "x2": 4.632735252380371,
        "y1": 6.189671039581299,
        "y2": 3.756119728088379
      },
      {
        "r": 0,
        "text": "Our analysis reveals that instruction-following models are competitive, and sometimes even outperform fine-tuned models for correctness.",
        "trunc_text": "Our analysis reveals that instruction-following models are competitive, and sometimes even outperform fine-tuned models ",
        "x1": 5.293301105499268,
        "x2": 4.139528751373291,
        "y1": 4.223238945007324,
        "y2": 5.978012561798096
      },
      {
        "r": 0,
        "text": "However, these models struggle to stick to the provided knowledge and often hallucinate in their responses.",
        "trunc_text": "However, these models struggle to stick to the provided knowledge and often hallucinate in their responses.",
        "x1": 4.346718788146973,
        "x2": 3.5550918579101562,
        "y1": 4.272468566894531,
        "y2": 5.43229341506958
      },
      {
        "r": 0,
        "text": "We hope our work encourages a more holistic evaluation of instruction-following models for QA.",
        "trunc_text": "We hope our work encourages a more holistic evaluation of instruction-following models for QA.",
        "x1": 5.25125789642334,
        "x2": 4.140251159667969,
        "y1": 4.255920886993408,
        "y2": 5.983750343322754
      },
      {
        "r": 0,
        "text": "Our code and data is available at https://github.com/McGill-NLP/instruct-qa",
        "trunc_text": "Our code and data is available at https://github.com/McGill-NLP/instruct-qa",
        "x1": 7.463276386260986,
        "x2": 8.055951118469238,
        "y1": 7.804403781890869,
        "y2": 2.060120105743408
      },
      {
        "r": 0,
        "text": "Our dataset comprises traffic at a real environment for more than 1 year.",
        "trunc_text": "Our dataset comprises traffic at a real environment for more than 1 year.",
        "x1": 5.639107704162598,
        "x2": 6.646067142486572,
        "y1": 7.966953277587891,
        "y2": 1.9090666770935059
      },
      {
        "r": 0,
        "text": "Indicators of Compromise (IOCs), such as IP addresses, file hashes, and domain names associated with known malware or attacks, are cornerstones of cybersecurity, serving to identify malicious activity on a network.",
        "trunc_text": "Indicators of Compromise (IOCs), such as IP addresses, file hashes, and domain names associated with known malware or at",
        "x1": 7.386570453643799,
        "x2": 6.6674723625183105,
        "y1": 5.486077308654785,
        "y2": 6.006637096405029
      },
      {
        "r": 0,
        "text": "In this work, we leverage real data to compare different parameterizations of IOC aging models.  ",
        "trunc_text": "In this work, we leverage real data to compare different parameterizations of IOC aging models.  ",
        "x1": 4.273775577545166,
        "x2": 5.400550365447998,
        "y1": 6.029016017913818,
        "y2": 4.0804314613342285
      },
      {
        "r": 0,
        "text": "Among our trace-driven findings, we determine thresholds for the ratio between miss over monitoring costs such that the system benefits from storing IOCs for a finite time-to-live (TTL) before eviction.",
        "trunc_text": "Among our trace-driven findings, we determine thresholds for the ratio between miss over monitoring costs such that the ",
        "x1": 4.363287448883057,
        "x2": 5.423147678375244,
        "y1": 5.875403881072998,
        "y2": 4.175625324249268
      },
      {
        "r": 0,
        "text": "To the best of our knowledge, this is the first real world evaluation of thresholds related to IOC aging, paving the way towards realistic IOC decaying models.",
        "trunc_text": "To the best of our knowledge, this is the first real world evaluation of thresholds related to IOC aging, paving the way",
        "x1": 4.306303024291992,
        "x2": 5.378525733947754,
        "y1": 5.943753719329834,
        "y2": 4.108715057373047
      },
      {
        "r": 0,
        "text": "As people's daily life becomes increasingly inseparable from various mobile electronic devices, relevant service application platforms and network operators can collect numerous individual information easily.",
        "trunc_text": "As people's daily life becomes increasingly inseparable from various mobile electronic devices, relevant service applica",
        "x1": 6.4866251945495605,
        "x2": 6.852853298187256,
        "y1": 6.289377212524414,
        "y2": 3.5972893238067627
      },
      {
        "r": 0,
        "text": "When releasing these data for scientific research or commercial purposes, users' privacy will be in danger, especially in the publication of spatiotemporal trajectory datasets.",
        "trunc_text": "When releasing these data for scientific research or commercial purposes, users' privacy will be in danger, especially i",
        "x1": 4.866305828094482,
        "x2": 6.480989456176758,
        "y1": 9.511658668518066,
        "y2": 0.8447282910346985
      },
      {
        "r": 0,
        "text": "Therefore, to avoid the leakage of users' privacy, it is necessary to anonymize the data before they are released.",
        "trunc_text": "Therefore, to avoid the leakage of users' privacy, it is necessary to anonymize the data before they are released.",
        "x1": 4.879835605621338,
        "x2": 6.473846912384033,
        "y1": 9.54951286315918,
        "y2": 0.8366175293922424
      },
      {
        "r": 0,
        "text": "However, more than simply removing the unique identifiers of individuals is needed to protect the trajectory privacy, because some attackers may infer the identity of users by the connection with other databases.",
        "trunc_text": "However, more than simply removing the unique identifiers of individuals is needed to protect the trajectory privacy, be",
        "x1": 4.76763391494751,
        "x2": 6.4431562423706055,
        "y1": 9.636679649353027,
        "y2": 0.6749656796455383
      },
      {
        "r": 0,
        "text": "Much work has been devoted to merging multiple trajectories to avoid re-identification, but these solutions always require sacrificing data quality to achieve the anonymity requirement.",
        "trunc_text": "Much work has been devoted to merging multiple trajectories to avoid re-identification, but these solutions always requi",
        "x1": 4.672499179840088,
        "x2": 6.354762554168701,
        "y1": 9.689678192138672,
        "y2": 0.5494775176048279
      },
      {
        "r": 0,
        "text": "In order to provide sufficient privacy protection for users' trajectory datasets, this paper develops a study on trajectory privacy against re-identification attacks, proposing a trajectory K-anonymity model based on Point Density and Partition (KPDP).",
        "trunc_text": "In order to provide sufficient privacy protection for users' trajectory datasets, this paper develops a study on traject",
        "x1": 4.730222225189209,
        "x2": 6.453404426574707,
        "y1": 9.680390357971191,
        "y2": 0.635347306728363
      },
      {
        "r": 0,
        "text": "Our approach improves the existing trajectory generalization anonymization techniques regarding trajectory set partition preprocessing and trajectory clustering algorithms.",
        "trunc_text": "Our approach improves the existing trajectory generalization anonymization techniques regarding trajectory set partition",
        "x1": 4.682911396026611,
        "x2": 6.413965702056885,
        "y1": 9.705681800842285,
        "y2": 0.5731790661811829
      },
      {
        "r": 0,
        "text": "It successfully resists re-identification attacks and reduces the data utility loss of the k-anonymized dataset.",
        "trunc_text": "It successfully resists re-identification attacks and reduces the data utility loss of the k-anonymized dataset.",
        "x1": 4.896249294281006,
        "x2": 6.438401222229004,
        "y1": 9.394062042236328,
        "y2": 1.1701107025146484
      },
      {
        "r": 0,
        "text": "A series of experiments on a real-world dataset show that the proposed model has significant advantages in terms of higher data utility and shorter algorithm execution time than other existing techniques.",
        "trunc_text": "A series of experiments on a real-world dataset show that the proposed model has significant advantages in terms of high",
        "x1": 4.211771488189697,
        "x2": 4.9278717041015625,
        "y1": 7.308885097503662,
        "y2": 2.7765283584594727
      },
      {
        "r": 0,
        "text": "We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT.",
        "trunc_text": "We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT.",
        "x1": 6.484655857086182,
        "x2": 4.78684663772583,
        "y1": 3.8617539405822754,
        "y2": 6.721401214599609
      },
      {
        "r": 0,
        "text": "Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub",
        "trunc_text": "Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub",
        "x1": 6.7803635597229,
        "x2": 6.868948936462402,
        "y1": 6.615331649780273,
        "y2": 3.03568434715271
      },
      {
        "r": 0,
        "text": "Despite the advancements of open-source large language models (LLMs) and their variants, e.g., LLaMA and Vicuna, they remain significantly limited in performing higher-level tasks, such as following human instructions to use external tools (APIs).",
        "trunc_text": "Despite the advancements of open-source large language models (LLMs) and their variants, e.g., LLaMA and Vicuna, they re",
        "x1": 5.568675994873047,
        "x2": 4.330558776855469,
        "y1": 4.0856242179870605,
        "y2": 6.311626434326172
      },
      {
        "r": 0,
        "text": "This is because current instruction tuning largely focuses on basic language tasks instead of the tool-use domain.",
        "trunc_text": "This is because current instruction tuning largely focuses on basic language tasks instead of the tool-use domain.",
        "x1": 5.343724250793457,
        "x2": 4.1312174797058105,
        "y1": 4.084549427032471,
        "y2": 6.088919639587402
      },
      {
        "r": 0,
        "text": "This is in contrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have demonstrated excellent tool-use capabilities but are unfortunately closed source.",
        "trunc_text": "This is in contrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have demonstrated excellent tool-use capabilit",
        "x1": 6.506899356842041,
        "x2": 4.687049865722656,
        "y1": 3.8588364124298096,
        "y2": 6.80884313583374
      },
      {
        "r": 0,
        "text": "To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework of data construction, model training and evaluation.  ",
        "trunc_text": "To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework of data ",
        "x1": 6.215512275695801,
        "x2": 5.017119884490967,
        "y1": 4.7845025062561035,
        "y2": 6.207773685455322
      },
      {
        "r": 0,
        "text": "Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub, then prompt ChatGPTi-tool scenarios.",
        "trunc_text": "Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub, then prompt ChatGPTi-t",
        "x1": 6.734420299530029,
        "x2": 4.848900318145752,
        "y1": 3.9080681800842285,
        "y2": 6.923117637634277
      },
      {
        "r": 0,
        "text": "Finally, we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction.",
        "trunc_text": "Finally, we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction.",
        "x1": 6.593028545379639,
        "x2": 4.839290618896484,
        "y1": 3.893575668334961,
        "y2": 6.774580478668213
      },
      {
        "r": 0,
        "text": "To make the searching process more efficient, we develop a novel depth-first search-based decision tree (DFSDT), enabling LLMs to evaluate multiple reasoning traces and expand the search space.",
        "trunc_text": "To make the searching process more efficient, we develop a novel depth-first search-based decision tree (DFSDT), enablin",
        "x1": 5.795712947845459,
        "x2": 4.76539945602417,
        "y1": 4.8715033531188965,
        "y2": 5.681393146514893
      },
      {
        "r": 0,
        "text": "We show that DFSDT significantly enhances the planning and reasoning capabilities of LLMs.",
        "trunc_text": "We show that DFSDT significantly enhances the planning and reasoning capabilities of LLMs.",
        "x1": 5.985971927642822,
        "x2": 4.847070217132568,
        "y1": 4.699018478393555,
        "y2": 6.172642707824707
      },
      {
        "r": 0,
        "text": "For efficient tool-use assessment, we develop an automatic evaluator: ToolEval.",
        "trunc_text": "For efficient tool-use assessment, we develop an automatic evaluator: ToolEval.",
        "x1": 6.157234191894531,
        "x2": 5.149929046630859,
        "y1": 5.045962333679199,
        "y2": 5.924283981323242
      },
      {
        "r": 0,
        "text": "We fine-tune LLaMA on ToolBench and obtain ToolLLaMA.",
        "trunc_text": "We fine-tune LLaMA on ToolBench and obtain ToolLLaMA.",
        "x1": 6.115285873413086,
        "x2": 4.801593780517578,
        "y1": 4.635172367095947,
        "y2": 6.236815452575684
      },
      {
        "r": 0,
        "text": "Our ToolEval reveals that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT.",
        "trunc_text": "Our ToolEval reveals that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to ",
        "x1": 6.514771461486816,
        "x2": 4.83904504776001,
        "y1": 3.8708670139312744,
        "y2": 6.738132476806641
      },
      {
        "r": 0,
        "text": "To make the pipeline more practical, we devise a neural API retriever to recommend appropriate APIs for each instruction, negating the need for manual API selection.",
        "trunc_text": "To make the pipeline more practical, we devise a neural API retriever to recommend appropriate APIs for each instruction",
        "x1": 6.388858318328857,
        "x2": 4.640273094177246,
        "y1": 3.9226982593536377,
        "y2": 6.556504726409912
      },
      {
        "r": 0,
        "text": "This paper accompanies a new dataset of non-linear real arithmetic problems for the SMT-LIB benchmark collection.",
        "trunc_text": "This paper accompanies a new dataset of non-linear real arithmetic problems for the SMT-LIB benchmark collection.",
        "x1": 4.74448299407959,
        "x2": 4.94353723526001,
        "y1": 6.134084224700928,
        "y2": 4.253146648406982
      },
      {
        "r": 0,
        "text": "The problems come from an automated proof procedure of Gerhold--Kauers, which is well suited for solution by SMT.",
        "trunc_text": "The problems come from an automated proof procedure of Gerhold--Kauers, which is well suited for solution by SMT.",
        "x1": 4.931087017059326,
        "x2": 5.2968926429748535,
        "y1": 6.000077247619629,
        "y2": 4.576867580413818
      },
      {
        "r": 0,
        "text": "The problems of this type have not been tackled by SMT-solvers before.",
        "trunc_text": "The problems of this type have not been tackled by SMT-solvers before.",
        "x1": 4.82250452041626,
        "x2": 5.135326862335205,
        "y1": 5.913495063781738,
        "y2": 4.608815670013428
      },
      {
        "r": 0,
        "text": "We describe the proof technique and give one new such proof to illustrate it.",
        "trunc_text": "We describe the proof technique and give one new such proof to illustrate it.",
        "x1": 5.326178550720215,
        "x2": 6.0927348136901855,
        "y1": 8.296952247619629,
        "y2": 4.562616348266602
      },
      {
        "r": 0,
        "text": "The benchmarks on the new dataset are quite different to the existing ones.",
        "trunc_text": "The benchmarks on the new dataset are quite different to the existing ones.",
        "x1": 4.535871982574463,
        "x2": 4.771016597747803,
        "y1": 6.490054607391357,
        "y2": 3.4613380432128906
      },
      {
        "r": 0,
        "text": "The benchmarking also brings forward some interesting debate on the use/inclusion of rational functions and algebraic numbers in the SMT-LIB.",
        "trunc_text": "The benchmarking also brings forward some interesting debate on the use/inclusion of rational functions and algebraic nu",
        "x1": 4.7626118659973145,
        "x2": 4.925516128540039,
        "y1": 6.158870220184326,
        "y2": 4.267860412597656
      },
      {
        "r": 0,
        "text": "Moreover, we release a challenging dataset named ChaMS, comprising both real-world and synthetic sets with significant parallax, providing a new option for comprehensive evaluation.",
        "trunc_text": "Moreover, we release a challenging dataset named ChaMS, comprising both real-world and synthetic sets with significant p",
        "x1": 4.655491828918457,
        "x2": 5.0556440353393555,
        "y1": 7.045290470123291,
        "y2": 2.4194982051849365
      },
      {
        "r": 0,
        "text": "Multi-spectral image stitching leverages the complementarity between infrared and visible images to generate a robust and reliable wide field-of-view (FOV) scene.",
        "trunc_text": "Multi-spectral image stitching leverages the complementarity between infrared and visible images to generate a robust an",
        "x1": 1.3706339597702026,
        "x2": 3.396348476409912,
        "y1": 9.065353393554688,
        "y2": 0.3016122579574585
      },
      {
        "r": 0,
        "text": "The primary challenge of this task is to explore the relations between multi-spectral images for aligning and integrating multi-view scenes.",
        "trunc_text": "The primary challenge of this task is to explore the relations between multi-spectral images for aligning and integratin",
        "x1": 1.35934317111969,
        "x2": 3.3364369869232178,
        "y1": 9.01511287689209,
        "y2": 0.36742138862609863
      },
      {
        "r": 0,
        "text": "Capitalizing on the strengths of Graph Convolutional Networks (GCNs) in modeling feature relationships, we propose a spatial graph reasoning based multi-spectral image stitching method that effectively distills the deformation and integration of multi-spectral images across different viewpoints.",
        "trunc_text": "Capitalizing on the strengths of Graph Convolutional Networks (GCNs) in modeling feature relationships, we propose a spa",
        "x1": 1.3530652523040771,
        "x2": 3.258317470550537,
        "y1": 8.85711669921875,
        "y2": 0.3053555488586426
      },
      {
        "r": 0,
        "text": "To accomplish this, we embed multi-scale complementary features from the same view position into a set of nodes.",
        "trunc_text": "To accomplish this, we embed multi-scale complementary features from the same view position into a set of nodes.",
        "x1": 1.2737599611282349,
        "x2": 3.0361931324005127,
        "y1": 8.856107711791992,
        "y2": 0.34622833132743835
      },
      {
        "r": 0,
        "text": "The correspondence across different views is learned through powerful dense feature embeddings, where both inter- and intra-correlations are developed to exploit cross-view matching and enhance inner feature disparity.",
        "trunc_text": "The correspondence across different views is learned through powerful dense feature embeddings, where both inter- and in",
        "x1": 1.2093651294708252,
        "x2": 3.0489611625671387,
        "y1": 8.897795677185059,
        "y2": 0.3717247545719147
      },
      {
        "r": 0,
        "text": "By introducing long-range coherence along spatial and channel dimensions, the complementarity of pixel relations and channel interdependencies aids in the reconstruction of aligned multi-view features, generating informative and reliable wide FOV scenes.  ",
        "trunc_text": "By introducing long-range coherence along spatial and channel dimensions, the complementarity of pixel relations and cha",
        "x1": 1.2866573333740234,
        "x2": 3.2569870948791504,
        "y1": 9.024230003356934,
        "y2": 0.3115840554237366
      },
      {
        "r": 0,
        "text": "Extensive experiments demonstrate that our method surpasses the state-of-the-arts.",
        "trunc_text": "Extensive experiments demonstrate that our method surpasses the state-of-the-arts.",
        "x1": 5.264562606811523,
        "x2": 6.4592814445495605,
        "y1": 8.815678596496582,
        "y2": 4.797476291656494
      },
      {
        "r": 0,
        "text": "Here, we collect and publicly release Repair-QA, the first large dataset of TPRs in a conversational question answering (QA) setting.",
        "trunc_text": "Here, we collect and publicly release Repair-QA, the first large dataset of TPRs in a conversational question answering ",
        "x1": 6.054497241973877,
        "x2": 4.3090434074401855,
        "y1": 3.4400041103363037,
        "y2": 6.920413017272949
      },
      {
        "r": 0,
        "text": "The data is comprised of the TPR turns, corresponding dialogue contexts, and candidate repairs of the original turn for execution of TPRs.",
        "trunc_text": "The data is comprised of the TPR turns, corresponding dialogue contexts, and candidate repairs of the original turn for ",
        "x1": 6.040288925170898,
        "x2": 4.31891393661499,
        "y1": 3.434169292449951,
        "y2": 6.895401954650879
      },
      {
        "r": 0,
        "text": "The ability to handle miscommunication is crucial to robust and faithful conversational AI.",
        "trunc_text": "The ability to handle miscommunication is crucial to robust and faithful conversational AI.",
        "x1": 5.866975784301758,
        "x2": 3.912724733352661,
        "y1": 3.3494982719421387,
        "y2": 7.282755374908447
      },
      {
        "r": 0,
        "text": "People usually deal with miscommunication immediately as they detect it, using highly systematic interactional mechanisms called repair.",
        "trunc_text": "People usually deal with miscommunication immediately as they detect it, using highly systematic interactional mechanism",
        "x1": 5.906510353088379,
        "x2": 4.106717586517334,
        "y1": 3.425154685974121,
        "y2": 7.1467766761779785
      },
      {
        "r": 0,
        "text": "One important type of repair is Third Position Repair (TPR) whereby a speaker is initially misunderstood but then corrects the misunderstanding as it becomes apparent after the addressee's erroneous response.  ",
        "trunc_text": "One important type of repair is Third Position Repair (TPR) whereby a speaker is initially misunderstood but then correc",
        "x1": 6.010108947753906,
        "x2": 4.198944091796875,
        "y1": 3.4248976707458496,
        "y2": 7.017714977264404
      },
      {
        "r": 0,
        "text": "The data is comprised of the TPR turns, corresponding dialogue contexts, and candidate repairs of the original turn for execution of execution, we perform both automatic and human evaluations on a fine-tuned T5 model, as well as OpenAI's GPT-3 LLMs.",
        "trunc_text": "The data is comprised of the TPR turns, corresponding dialogue contexts, and candidate repairs of the original turn for ",
        "x1": 6.129600524902344,
        "x2": 4.412262439727783,
        "y1": 3.5381298065185547,
        "y2": 6.889890670776367
      },
      {
        "r": 0,
        "text": "Additionally, we extrinsically evaluate the LLMs' TPR processing capabilities in the downstream conversational QA task.",
        "trunc_text": "Additionally, we extrinsically evaluate the LLMs' TPR processing capabilities in the downstream conversational QA task.",
        "x1": 5.9254655838012695,
        "x2": 4.32362699508667,
        "y1": 3.671527624130249,
        "y2": 6.681330680847168
      },
      {
        "r": 0,
        "text": "The results indicate poor out-of-the-box performance on TPR's by the GPT-3 models, which then significantly improves when exposed to Repair-QA.",
        "trunc_text": "The results indicate poor out-of-the-box performance on TPR's by the GPT-3 models, which then significantly improves whe",
        "x1": 6.206631660461426,
        "x2": 4.454600811004639,
        "y1": 3.5149085521698,
        "y2": 6.899895191192627
      },
      {
        "r": 0,
        "text": "The datasets, competition toolkit, workshop recordings, and source code from the winning teams are publicly available on the challenge website.",
        "trunc_text": "The datasets, competition toolkit, workshop recordings, and source code from the winning teams are publicly available on",
        "x1": 7.477546691894531,
        "x2": 7.753898620605469,
        "y1": 7.5808796882629395,
        "y2": 2.3080263137817383
      },
      {
        "r": 0,
        "text": "Accurate depth estimation under out-of-distribution (OoD) scenarios, such as adverse weather conditions, sensor failure, and noise contamination, is desirable for safety-critical applications.",
        "trunc_text": "Accurate depth estimation under out-of-distribution (OoD) scenarios, such as adverse weather conditions, sensor failure,",
        "x1": 0.8853480815887451,
        "x2": 2.569319725036621,
        "y1": 8.709432601928711,
        "y2": 0.311329185962677
      },
      {
        "r": 0,
        "text": "Existing depth estimation systems, however, suffer inevitably from real-world corruptions and perturbations and are struggled to provide reliable depth predictions under such cases.",
        "trunc_text": "Existing depth estimation systems, however, suffer inevitably from real-world corruptions and perturbations and are stru",
        "x1": 0.9149892330169678,
        "x2": 2.5639989376068115,
        "y1": 8.739474296569824,
        "y2": 0.3292854428291321
      },
      {
        "r": 0,
        "text": "In this paper, we summarize the winning solutions from the RoboDepth Challenge -- an academic competition designed to facilitate and advance robust OoD depth estimation.",
        "trunc_text": "In this paper, we summarize the winning solutions from the RoboDepth Challenge -- an academic competition designed to fa",
        "x1": 0.9017186760902405,
        "x2": 2.604285478591919,
        "y1": 8.714533805847168,
        "y2": 0.3266391456127167
      },
      {
        "r": 0,
        "text": "This challenge was developed based on the newly established KITTI-C and NYUDepth2-C benchmarks.",
        "trunc_text": "This challenge was developed based on the newly established KITTI-C and NYUDepth2-C benchmarks.",
        "x1": 4.377302169799805,
        "x2": 4.2908477783203125,
        "y1": 6.776447296142578,
        "y2": 3.3411672115325928
      },
      {
        "r": 0,
        "text": "We hosted two stand-alone tracks, with an emphasis on robust self-supervised and robust fully-supervised depth estimation, respectively.",
        "trunc_text": "We hosted two stand-alone tracks, with an emphasis on robust self-supervised and robust fully-supervised depth estimatio",
        "x1": 0.8724067807197571,
        "x2": 2.602057695388794,
        "y1": 8.660042762756348,
        "y2": 0.35480114817619324
      },
      {
        "r": 0,
        "text": "Out of more than two hundred participants, nine unique and top-performing solutions have appeared, with novel designs ranging from the following aspects: spatial- and frequency-domain augmentations, masked image modeling, image restoration and super-resolution, adversarial training, diffusion-based noise suppression, vision-language pre-training, learned model ensembling, and hierarchical feature enhancement.",
        "trunc_text": "Out of more than two hundred participants, nine unique and top-performing solutions have appeared, with novel designs ra",
        "x1": 2.1084065437316895,
        "x2": 3.2377331256866455,
        "y1": 6.926039218902588,
        "y2": 2.2781081199645996
      },
      {
        "r": 0,
        "text": "Extensive experimental analyses along with insightful observations are drawn to better understand the rationale behind each design.",
        "trunc_text": "Extensive experimental analyses along with insightful observations are drawn to better understand the rationale behind e",
        "x1": 5.43141508102417,
        "x2": 6.5415825843811035,
        "y1": 8.930623054504395,
        "y2": 4.875669479370117
      },
      {
        "r": 0,
        "text": "We hope this challenge could lay a solid foundation for future research on robust and reliable depth estimation and beyond.",
        "trunc_text": "We hope this challenge could lay a solid foundation for future research on robust and reliable depth estimation and beyo",
        "x1": 0.8966468572616577,
        "x2": 2.6106767654418945,
        "y1": 8.70139217376709,
        "y2": 0.2997542917728424
      },
      {
        "r": 0,
        "text": "Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is widely recognized that realistic sensor simulation will play a critical role in solving remaining corner cases by simulating them.",
        "trunc_text": "Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is widely recognized that realistic sensor simula",
        "x1": 2.7696635723114014,
        "x2": 4.67460298538208,
        "y1": 10.03786849975586,
        "y2": 0.3466428518295288
      },
      {
        "r": 0,
        "text": "To this end, we propose an autonomous driving simulator based upon neural radiance fields (NeRFs).",
        "trunc_text": "To this end, we propose an autonomous driving simulator based upon neural radiance fields (NeRFs).",
        "x1": 2.504378318786621,
        "x2": 4.5633039474487305,
        "y1": 10.113354682922363,
        "y2": 0.37537986040115356
      },
      {
        "r": 0,
        "text": "Compared with existing works, ours has three notable features: (1) Instance-aware.",
        "trunc_text": "Compared with existing works, ours has three notable features: (1) Instance-aware.",
        "x1": 5.679673671722412,
        "x2": 5.939668655395508,
        "y1": 6.116960048675537,
        "y2": 4.28883171081543
      },
      {
        "r": 0,
        "text": "Our simulator models the foreground instances and background environments separately with independent networks so that the static (e.g., size and appearance) and dynamic (e.g., trajectory) properties of instances can be controlled separately.",
        "trunc_text": "Our simulator models the foreground instances and background environments separately with independent networks so that t",
        "x1": 2.7997922897338867,
        "x2": 4.874650001525879,
        "y1": 10.025201797485352,
        "y2": 0.5467744469642639
      },
      {
        "r": 0,
        "text": "(2) Modular.",
        "trunc_text": "(2) Modular.",
        "x1": 4.653809547424316,
        "x2": 6.223860263824463,
        "y1": 8.264328002929688,
        "y2": 4.062057971954346
      },
      {
        "r": 0,
        "text": "Our simulator allows flexible switching between different modern NeRF-related backbones, sampling strategies, input modalities, etc.",
        "trunc_text": "Our simulator allows flexible switching between different modern NeRF-related backbones, sampling strategies, input moda",
        "x1": 3.0503177642822266,
        "x2": 4.917667865753174,
        "y1": 9.910089492797852,
        "y2": 0.5744481086730957
      },
      {
        "r": 0,
        "text": "We expect this modular design to boost academic progress and industrial deployment of NeRF-based autonomous driving simulation.",
        "trunc_text": "We expect this modular design to boost academic progress and industrial deployment of NeRF-based autonomous driving simu",
        "x1": 3.2970516681671143,
        "x2": 4.996943950653076,
        "y1": 9.982427597045898,
        "y2": 0.2545764148235321
      },
      {
        "r": 0,
        "text": "(3) Realistic.",
        "trunc_text": "(3) Realistic.",
        "x1": 4.418297290802002,
        "x2": 6.255582332611084,
        "y1": 8.257962226867676,
        "y2": 3.826885223388672
      },
      {
        "r": 0,
        "text": "Our simulator set new state-of-the-art photo-realism results given the best module selection.",
        "trunc_text": "Our simulator set new state-of-the-art photo-realism results given the best module selection.",
        "x1": 2.0831916332244873,
        "x2": 3.8101673126220703,
        "y1": 8.792335510253906,
        "y2": 0.6994351744651794
      },
      {
        "r": 0,
        "text": "Our simulator will be open-sourced while most of our counterparts are not.",
        "trunc_text": "Our simulator will be open-sourced while most of our counterparts are not.",
        "x1": 7.811255931854248,
        "x2": 8.177064895629883,
        "y1": 6.965577125549316,
        "y2": 2.9232418537139893
      },
      {
        "r": 0,
        "text": "Project page: https://open-air-sun.github.io/mars/.",
        "trunc_text": "Project page: https://open-air-sun.github.io/mars/.",
        "x1": 7.836824417114258,
        "x2": 8.533519744873047,
        "y1": 7.891883850097656,
        "y2": 1.9286679029464722
      },
      {
        "r": 0,
        "text": "Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains.",
        "trunc_text": "Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning,",
        "x1": 4.006484031677246,
        "x2": 1.601009488105774,
        "y1": 1.7904258966445923,
        "y2": 7.0108232498168945
      },
      {
        "r": 0,
        "text": "In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text.",
        "trunc_text": "In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text.",
        "x1": 3.9400975704193115,
        "x2": 1.5146015882492065,
        "y1": 1.783966064453125,
        "y2": 6.942885398864746
      },
      {
        "r": 0,
        "text": "Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation.",
        "trunc_text": "Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and re",
        "x1": 3.901618480682373,
        "x2": 2.8405792713165283,
        "y1": 3.7128939628601074,
        "y2": 5.205497741699219
      },
      {
        "r": 0,
        "text": "However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks.",
        "trunc_text": "However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks.",
        "x1": 4.631363391876221,
        "x2": 3.7945711612701416,
        "y1": 4.295622825622559,
        "y2": 5.5970001220703125
      },
      {
        "r": 0,
        "text": "Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy.",
        "trunc_text": "Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in th",
        "x1": 4.540592193603516,
        "x2": 3.1342477798461914,
        "y1": 3.6383750438690186,
        "y2": 5.995479106903076
      },
      {
        "r": 0,
        "text": "To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and speed.",
        "trunc_text": "To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard",
        "x1": 4.428702354431152,
        "x2": 3.8633005619049072,
        "y1": 4.780194282531738,
        "y2": 5.323967933654785
      },
      {
        "r": 0,
        "text": "Our approach can model language dependencies and relies only on the attention mechanism, thereby making it more parallelizable and less complex.",
        "trunc_text": "Our approach can model language dependencies and relies only on the attention mechanism, thereby making it more parallel",
        "x1": 4.868666172027588,
        "x2": 3.8093788623809814,
        "y1": 4.045863151550293,
        "y2": 5.951804161071777
      },
      {
        "r": 0,
        "text": "We employ pre-trained Transformers for both image understanding and language modeling.",
        "trunc_text": "We employ pre-trained Transformers for both image understanding and language modeling.",
        "x1": 4.140844821929932,
        "x2": 2.8296148777008057,
        "y1": 3.8777174949645996,
        "y2": 4.69697904586792
      },
      {
        "r": 0,
        "text": "Our evaluation on the Arabic KHATT dataset demonstrates that our proposed method outperforms the current state-of-the-art approaches for recognizing offline Arabic handwritten text.",
        "trunc_text": "Our evaluation on the Arabic KHATT dataset demonstrates that our proposed method outperforms the current state-of-the-ar",
        "x1": 3.9148316383361816,
        "x2": 1.5238564014434814,
        "y1": 1.7818899154663086,
        "y2": 6.9540605545043945
      },
      {
        "r": 0,
        "text": "For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.",
        "trunc_text": "For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset ",
        "x1": 7.274806022644043,
        "x2": 7.657756328582764,
        "y1": 7.380984306335449,
        "y2": 2.2381551265716553
      },
      {
        "r": 0,
        "text": "This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling.",
        "trunc_text": "This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling.",
        "x1": 4.615795612335205,
        "x2": 4.672670841217041,
        "y1": 5.18032169342041,
        "y2": 5.018410682678223
      },
      {
        "r": 0,
        "text": "Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy.",
        "trunc_text": "Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrat",
        "x1": 4.6296610832214355,
        "x2": 4.774252414703369,
        "y1": 5.240662574768066,
        "y2": 4.957247257232666
      },
      {
        "r": 0,
        "text": "Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec.",
        "trunc_text": "Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of curre",
        "x1": 4.653045654296875,
        "x2": 4.775859355926514,
        "y1": 5.270939350128174,
        "y2": 4.902237892150879
      },
      {
        "r": 0,
        "text": "A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings.",
        "trunc_text": "A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in prac",
        "x1": 4.658876895904541,
        "x2": 4.721487998962402,
        "y1": 5.338011264801025,
        "y2": 4.837686538696289
      },
      {
        "r": 0,
        "text": "In ground-view object change detection, the recently emerging map-less navigation has great potential as a means of navigating a robot to distantly detected objects and identifying their changing states (appear/disappear/no-change) with high resolution imagery.",
        "trunc_text": "In ground-view object change detection, the recently emerging map-less navigation has great potential as a means of navi",
        "x1": 0.8155820965766907,
        "x2": 2.3618996143341064,
        "y1": 7.995577812194824,
        "y2": 0.9897165894508362
      },
      {
        "r": 0,
        "text": "However, the brute-force naive action strategy of navigating to every distant object requires huge sense/plan/action costs proportional to the number of objects.",
        "trunc_text": "However, the brute-force naive action strategy of navigating to every distant object requires huge sense/plan/action cos",
        "x1": 2.757967948913574,
        "x2": 4.264829635620117,
        "y1": 8.707404136657715,
        "y2": 0.27993860840797424
      },
      {
        "r": 0,
        "text": "In this work, we study this new problem of ``Which distant objects should be prioritized for map-less navigation?\" and in order to speed up the R{\\&}D cycle, propose a highly-simplified approach that is easy to implement and easy to extend.",
        "trunc_text": "In this work, we study this new problem of ``Which distant objects should be prioritized for map-less navigation?\" and i",
        "x1": 2.582965135574341,
        "x2": 4.20011568069458,
        "y1": 8.753616333007812,
        "y2": 0.20446784794330597
      },
      {
        "r": 0,
        "text": "In our approach, a new layer called map-based navigation is added on top of the map-less navigation, which constitutes a hierarchical planner.",
        "trunc_text": "In our approach, a new layer called map-based navigation is added on top of the map-less navigation, which constitutes a",
        "x1": 2.7701268196105957,
        "x2": 4.241225242614746,
        "y1": 8.840259552001953,
        "y2": 0.11900121718645096
      },
      {
        "r": 0,
        "text": "First, a dataset consisting of $N$ view sequences is acquired by a real robot via map-less navigation.",
        "trunc_text": "First, a dataset consisting of $N$ view sequences is acquired by a real robot via map-less navigation.",
        "x1": 2.5680408477783203,
        "x2": 4.24629020690918,
        "y1": 8.977856636047363,
        "y2": 0.10583579540252686
      },
      {
        "r": 0,
        "text": "Then, an environment simulator was built to simulate a simple action planning problem: ``Which view sequence should the robot select next?\".",
        "trunc_text": "Then, an environment simulator was built to simulate a simple action planning problem: ``Which view sequence should the ",
        "x1": 2.987764835357666,
        "x2": 4.453471660614014,
        "y1": 9.09189224243164,
        "y2": -0.018197674304246902
      },
      {
        "r": 0,
        "text": "Then, a solver was built inspired by the analogy to the multi-armed bandit problem: ``Which arm should the player select next?\".",
        "trunc_text": "Then, a solver was built inspired by the analogy to the multi-armed bandit problem: ``Which arm should the player select",
        "x1": 4.1085991859436035,
        "x2": 3.95735764503479,
        "y1": 4.976602554321289,
        "y2": 5.034759998321533
      },
      {
        "r": 0,
        "text": "Finally, the effectiveness of the proposed framework was verified using the semantically non-trivial scenario ``sofa as bookshelf\".",
        "trunc_text": "Finally, the effectiveness of the proposed framework was verified using the semantically non-trivial scenario ``sofa as ",
        "x1": 4.4875288009643555,
        "x2": 5.905920028686523,
        "y1": 8.657641410827637,
        "y2": 4.256003379821777
      },
      {
        "r": 0,
        "text": "Despite the presence of the classification task in many different benchmark datasets for perception in the automotive domain, few efforts have been undertaken to define consistent classification requirements.",
        "trunc_text": "Despite the presence of the classification task in many different benchmark datasets for perception in the automotive do",
        "x1": 3.748650312423706,
        "x2": 5.142312049865723,
        "y1": 9.038110733032227,
        "y2": 0.568409264087677
      },
      {
        "r": 0,
        "text": "This work addresses the topic by proposing a structured method to generate a classification structure.",
        "trunc_text": "This work addresses the topic by proposing a structured method to generate a classification structure.",
        "x1": 3.266854763031006,
        "x2": 3.365529775619507,
        "y1": 4.150914669036865,
        "y2": 5.199502468109131
      },
      {
        "r": 0,
        "text": "First, legal categories are identified based on behavioral requirements for the vehicle.",
        "trunc_text": "First, legal categories are identified based on behavioral requirements for the vehicle.",
        "x1": 3.974982261657715,
        "x2": 5.421158313751221,
        "y1": 9.140287399291992,
        "y2": 0.5676009058952332
      },
      {
        "r": 0,
        "text": "This structure is further substantiated by considering the two aspects of collision safety for objects as well as perceptual categories.",
        "trunc_text": "This structure is further substantiated by considering the two aspects of collision safety for objects as well as percep",
        "x1": 3.7807021141052246,
        "x2": 5.2870869636535645,
        "y1": 9.15263843536377,
        "y2": 0.484730988740921
      },
      {
        "r": 0,
        "text": "A classification hierarchy is obtained by applying the method to an exemplary legal text.",
        "trunc_text": "A classification hierarchy is obtained by applying the method to an exemplary legal text.",
        "x1": 3.2338457107543945,
        "x2": 3.317275285720825,
        "y1": 4.007061958312988,
        "y2": 5.2273335456848145
      },
      {
        "r": 0,
        "text": "A comparison of the results with benchmark dataset categories shows limited agreement.",
        "trunc_text": "A comparison of the results with benchmark dataset categories shows limited agreement.",
        "x1": 4.095044136047363,
        "x2": 4.510052680969238,
        "y1": 6.596745491027832,
        "y2": 3.014472246170044
      },
      {
        "r": 0,
        "text": "This indicates the necessity for explicit consideration of legal requirements regarding perception.",
        "trunc_text": "This indicates the necessity for explicit consideration of legal requirements regarding perception.",
        "x1": 4.0029401779174805,
        "x2": 5.402090072631836,
        "y1": 9.078509330749512,
        "y2": 0.6077384948730469
      },
      {
        "r": 0,
        "text": "The main challenges of 3D pose transfer are: 1) Lack of paired training data with different characters performing the same pose; 2) Disentangling pose and shape information from the target mesh; 3) Difficulty in applying to meshes with different topologies.",
        "trunc_text": "The main challenges of 3D pose transfer are: 1) Lack of paired training data with different characters performing the sa",
        "x1": 2.2102482318878174,
        "x2": 3.5392110347747803,
        "y1": 9.091741561889648,
        "y2": -0.1604030579328537
      },
      {
        "r": 0,
        "text": "We thus propose a novel weakly-supervised keypoint-based framework to overcome these difficulties.",
        "trunc_text": "We thus propose a novel weakly-supervised keypoint-based framework to overcome these difficulties.",
        "x1": 1.6943572759628296,
        "x2": 1.7213401794433594,
        "y1": 5.028960227966309,
        "y2": 3.3204405307769775
      },
      {
        "r": 0,
        "text": "Specifically, we use a topology-agnostic keypoint detector with inverse kinematics to compute transformations between the source and target meshes.",
        "trunc_text": "Specifically, we use a topology-agnostic keypoint detector with inverse kinematics to compute transformations between th",
        "x1": 1.7072559595108032,
        "x2": 3.154768228530884,
        "y1": 9.204249382019043,
        "y2": -0.12294136732816696
      },
      {
        "r": 0,
        "text": "Our method only requires supervision on the keypoints, can be applied to meshes with different topologies and is shape-invariant for the target which allows extraction of pose-only information from the target meshes without transferring shape information.",
        "trunc_text": "Our method only requires supervision on the keypoints, can be applied to meshes with different topologies and is shape-i",
        "x1": 1.8357211351394653,
        "x2": 3.2067439556121826,
        "y1": 9.139373779296875,
        "y2": -0.058820340782403946
      },
      {
        "r": 0,
        "text": "We further design a cycle reconstruction to perform self-supervised pose transfer without the need for ground truth deformed mesh with the same pose and shape as the target and source, respectively.",
        "trunc_text": "We further design a cycle reconstruction to perform self-supervised pose transfer without the need for ground truth defo",
        "x1": 2.051682233810425,
        "x2": 3.3481297492980957,
        "y1": 9.055193901062012,
        "y2": -0.05735568329691887
      },
      {
        "r": 0,
        "text": "We evaluate our approach on benchmark human and animal datasets, where we achieve superior performance compared to the state-of-the-art unsupervised approaches and even comparable performance with the fully supervised approaches.",
        "trunc_text": "We evaluate our approach on benchmark human and animal datasets, where we achieve superior performance compared to the s",
        "x1": 3.648724317550659,
        "x2": 4.109408378601074,
        "y1": 6.572702884674072,
        "y2": 3.11067271232605
      },
      {
        "r": 0,
        "text": "We test on the more challenging Mixamo dataset to verify our approach's ability in handling meshes with different topologies and complex clothes.",
        "trunc_text": "We test on the more challenging Mixamo dataset to verify our approach's ability in handling meshes with different topolo",
        "x1": 3.949768543243408,
        "x2": 5.041853904724121,
        "y1": 7.209052562713623,
        "y2": 2.667800188064575
      },
      {
        "r": 0,
        "text": "We curate and release a dataset we call COVERAGEEVAL by executing tests and code from the HumanEval dataset and collecting code coverage information.",
        "trunc_text": "We curate and release a dataset we call COVERAGEEVAL by executing tests and code from the HumanEval dataset and collecti",
        "x1": 6.78903865814209,
        "x2": 7.403067588806152,
        "y1": 7.3875226974487305,
        "y2": 2.485118865966797
      },
      {
        "r": 0,
        "text": "Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branches, are executed during testing.",
        "trunc_text": "Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branch",
        "x1": 6.955565452575684,
        "x2": 5.683542251586914,
        "y1": 4.734573841094971,
        "y2": 6.534693717956543
      },
      {
        "r": 0,
        "text": "Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the instrumentation.",
        "trunc_text": "Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the ",
        "x1": 6.940898895263672,
        "x2": 5.686298370361328,
        "y1": 4.751536846160889,
        "y2": 6.485487937927246
      },
      {
        "r": 0,
        "text": "Furthermore, computing coverage of any snippet of code requires the whole program context.",
        "trunc_text": "Furthermore, computing coverage of any snippet of code requires the whole program context.",
        "x1": 6.955772399902344,
        "x2": 5.720416069030762,
        "y1": 4.772154808044434,
        "y2": 6.459342956542969
      },
      {
        "r": 0,
        "text": "Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the source code context, and the task of code coverage prediction can be a novel benchmark for judging the ability of models to understand code.",
        "trunc_text": "Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the so",
        "x1": 6.918776512145996,
        "x2": 5.662219047546387,
        "y1": 4.738574504852295,
        "y2": 6.45089864730835
      },
      {
        "r": 0,
        "text": "We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs).",
        "trunc_text": "We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs).",
        "x1": 6.777451038360596,
        "x2": 5.457383632659912,
        "y1": 4.5751447677612305,
        "y2": 6.4909186363220215
      },
      {
        "r": 0,
        "text": "We formalize this task to evaluate the capability of LLMs in understanding code execution by determining which lines of a method are executed by a given test case and inputs.  ",
        "trunc_text": "We formalize this task to evaluate the capability of LLMs in understanding code execution by determining which lines of ",
        "x1": 6.983730792999268,
        "x2": 5.571622371673584,
        "y1": 4.635311126708984,
        "y2": 6.555235862731934
      },
      {
        "r": 0,
        "text": "We report the performance of four state-of-the-art LLMs used for code-related tasks, including OpenAI's GPT-4 and GPT-3.5-Turbo, Google's BARD, and Anthropic's Claude, on the Code Coverage Prediction task.",
        "trunc_text": "We report the performance of four state-of-the-art LLMs used for code-related tasks, including OpenAI's GPT-4 and GPT-3.",
        "x1": 6.784174919128418,
        "x2": 5.458131313323975,
        "y1": 4.597506523132324,
        "y2": 6.536012649536133
      },
      {
        "r": 0,
        "text": "Finally, we argue that code coverage as a metric and pre-training data source are valuable for overall LLM performance on software engineering tasks.",
        "trunc_text": "Finally, we argue that code coverage as a metric and pre-training data source are valuable for overall LLM performance o",
        "x1": 6.894294738769531,
        "x2": 5.5433149337768555,
        "y1": 4.667545795440674,
        "y2": 6.501498699188232
      },
      {
        "r": 0,
        "text": "Task-oriented grasping (TOG) refers to the problem of predicting grasps on an object that enable subsequent manipulation tasks.",
        "trunc_text": "Task-oriented grasping (TOG) refers to the problem of predicting grasps on an object that enable subsequent manipulation",
        "x1": 4.656929016113281,
        "x2": 3.812138795852661,
        "y1": 4.402033805847168,
        "y2": 5.678224563598633
      },
      {
        "r": 0,
        "text": "To model the complex relationships between objects, tasks, and grasps, existing methods incorporate semantic knowledge as priors into TOG pipelines.",
        "trunc_text": "To model the complex relationships between objects, tasks, and grasps, existing methods incorporate semantic knowledge a",
        "x1": 4.673041343688965,
        "x2": 3.8035061359405518,
        "y1": 4.31932258605957,
        "y2": 5.699580192565918
      },
      {
        "r": 0,
        "text": "However, the existing semantic knowledge is typically constructed based on closed-world concept sets, restraining the generalization to novel concepts out of the pre-defined sets.",
        "trunc_text": "However, the existing semantic knowledge is typically constructed based on closed-world concept sets, restraining the ge",
        "x1": 3.4051320552825928,
        "x2": 2.7260630130767822,
        "y1": 4.489965438842773,
        "y2": 4.284580230712891
      },
      {
        "r": 0,
        "text": "To address this issue, we propose GraspGPT, a large language model (LLM) based TOG framework that leverages the open-end semantic knowledge from an LLM to achieve zero-shot generalization to novel concepts.",
        "trunc_text": "To address this issue, we propose GraspGPT, a large language model (LLM) based TOG framework that leverages the open-end",
        "x1": 4.860994815826416,
        "x2": 3.7403602600097656,
        "y1": 4.0590033531188965,
        "y2": 6.0037455558776855
      },
      {
        "r": 0,
        "text": "We conduct experiments on Language Augmented TaskGrasp (LA-TaskGrasp) dataset and demonstrate that GraspGPT outperforms existing TOG methods on different held-out settings when generalizing to novel concepts out of the training set.",
        "trunc_text": "We conduct experiments on Language Augmented TaskGrasp (LA-TaskGrasp) dataset and demonstrate that GraspGPT outperforms ",
        "x1": 4.738277912139893,
        "x2": 3.7222979068756104,
        "y1": 4.171900272369385,
        "y2": 5.779759883880615
      },
      {
        "r": 0,
        "text": "The effectiveness of GraspGPT is further validated in real-robot experiments.",
        "trunc_text": "The effectiveness of GraspGPT is further validated in real-robot experiments.",
        "x1": 3.1273388862609863,
        "x2": 3.853306531906128,
        "y1": 9.202339172363281,
        "y2": 5.607699394226074
      },
      {
        "r": 0,
        "text": "Our code, data, appendix, and video are publicly available at https://sites.google.com/view/graspgpt/.",
        "trunc_text": "Our code, data, appendix, and video are publicly available at https://sites.google.com/view/graspgpt/.",
        "x1": 7.9063401222229,
        "x2": 8.374677658081055,
        "y1": 7.12901496887207,
        "y2": 2.5955073833465576
      },
      {
        "r": 0,
        "text": "With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged.",
        "trunc_text": "With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language com",
        "x1": 5.7417893409729,
        "x2": 4.049834728240967,
        "y1": 4.055943965911865,
        "y2": 7.384931564331055
      },
      {
        "r": 0,
        "text": "However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation.",
        "trunc_text": "However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting r",
        "x1": 6.035542964935303,
        "x2": 4.245817184448242,
        "y1": 4.2158308029174805,
        "y2": 7.560297012329102
      },
      {
        "r": 0,
        "text": "In this paper, we build an environment for agent command and control that is highly realistic and reproducible.",
        "trunc_text": "In this paper, we build an environment for agent command and control that is highly realistic and reproducible.",
        "x1": 6.031535625457764,
        "x2": 4.2579851150512695,
        "y1": 4.1998772621154785,
        "y2": 7.631625175476074
      },
      {
        "r": 0,
        "text": "Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.",
        "trunc_text": "Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional webs",
        "x1": 6.089114665985107,
        "x2": 4.266737937927246,
        "y1": 4.214389801025391,
        "y2": 7.655630111694336
      },
      {
        "r": 0,
        "text": "Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving.",
        "trunc_text": "Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage huma",
        "x1": 4.9394450187683105,
        "x2": 3.950388193130493,
        "y1": 4.670413017272949,
        "y2": 5.444384574890137
      },
      {
        "r": 0,
        "text": "Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions.",
        "trunc_text": "Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of ",
        "x1": 4.373122692108154,
        "x2": 4.428650379180908,
        "y1": 6.08112907409668,
        "y2": 3.945481777191162
      },
      {
        "r": 0,
        "text": "The tasks in our benchmark are diverse, long-horizon, and are designed to emulate tasks that humans routinely perform on the internet.",
        "trunc_text": "The tasks in our benchmark are diverse, long-horizon, and are designed to emulate tasks that humans routinely perform on",
        "x1": 4.391336441040039,
        "x2": 4.3355913162231445,
        "y1": 6.061808109283447,
        "y2": 3.703296422958374
      },
      {
        "r": 0,
        "text": "We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting.",
        "trunc_text": "We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting.",
        "x1": 5.97578239440918,
        "x2": 4.26876974105835,
        "y1": 4.130863189697266,
        "y2": 7.596332550048828
      },
      {
        "r": 0,
        "text": "The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 10.59%.",
        "trunc_text": "The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-en",
        "x1": 6.0041303634643555,
        "x2": 4.435080051422119,
        "y1": 4.231178283691406,
        "y2": 7.547045707702637
      },
      {
        "r": 0,
        "text": "These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.",
        "trunc_text": "These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far fro",
        "x1": 6.069216728210449,
        "x2": 4.368463516235352,
        "y1": 4.200110912322998,
        "y2": 7.491091728210449
      },
      {
        "r": 0,
        "text": "Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/.",
        "trunc_text": "Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.",
        "x1": 7.7672624588012695,
        "x2": 8.384095191955566,
        "y1": 7.1677422523498535,
        "y2": 2.5783145427703857
      },
      {
        "r": 0,
        "text": "Crafting effective deep learning models for medical image analysis is a complex task, particularly in cases where the medical image dataset lacks significant inter-class variation.",
        "trunc_text": "Crafting effective deep learning models for medical image analysis is a complex task, particularly in cases where the me",
        "x1": 1.7689132690429688,
        "x2": 2.7740018367767334,
        "y1": 6.356427192687988,
        "y2": 2.440197467803955
      },
      {
        "r": 0,
        "text": "This challenge is further aggravated when employing such datasets to generate synthetic images using generative adversarial networks (GANs), as the output of GANs heavily relies on the input data.",
        "trunc_text": "This challenge is further aggravated when employing such datasets to generate synthetic images using generative adversar",
        "x1": 2.1141068935394287,
        "x2": 3.3209354877471924,
        "y1": 7.150877475738525,
        "y2": 2.2129805088043213
      },
      {
        "r": 0,
        "text": "In this research, we propose a novel filtering algorithm called Cosine Similarity-based Image Filtering (CosSIF).",
        "trunc_text": "In this research, we propose a novel filtering algorithm called Cosine Similarity-based Image Filtering (CosSIF).",
        "x1": 2.140145778656006,
        "x2": 3.0988523960113525,
        "y1": 7.180359840393066,
        "y2": 2.1313531398773193
      },
      {
        "r": 0,
        "text": "We leverage CosSIF to develop two distinct filtering methods: Filtering Before GAN Training (FBGT) and Filtering After GAN Training (FAGT).",
        "trunc_text": "We leverage CosSIF to develop two distinct filtering methods: Filtering Before GAN Training (FBGT) and Filtering After G",
        "x1": 2.2481796741485596,
        "x2": 3.0386621952056885,
        "y1": 6.957154750823975,
        "y2": 2.3484792709350586
      },
      {
        "r": 0,
        "text": "FBGT involves the removal of real images that exhibit similarities to images of other classes before utilizing them as the training dataset for a GAN.",
        "trunc_text": "FBGT involves the removal of real images that exhibit similarities to images of other classes before utilizing them as t",
        "x1": 2.286937713623047,
        "x2": 2.9832284450531006,
        "y1": 6.901224136352539,
        "y2": 2.39963960647583
      },
      {
        "r": 0,
        "text": "On the other hand, FAGT focuses on eliminating synthetic images with less discriminative features compared to real images used for training the GAN.",
        "trunc_text": "On the other hand, FAGT focuses on eliminating synthetic images with less discriminative features compared to real image",
        "x1": 2.2703511714935303,
        "x2": 3.012904167175293,
        "y1": 6.924257278442383,
        "y2": 2.4074835777282715
      },
      {
        "r": 0,
        "text": "Experimental results reveal that employing either the FAGT or FBGT method with modern transformer and convolutional-based networks leads to substantial performance gains in various evaluation metrics.",
        "trunc_text": "Experimental results reveal that employing either the FAGT or FBGT method with modern transformer and convolutional-base",
        "x1": 2.355384349822998,
        "x2": 2.868123769760132,
        "y1": 6.940255641937256,
        "y2": 2.460881233215332
      },
      {
        "r": 0,
        "text": "FAGT implementation on the ISIC-2016 dataset surpasses the baseline method in terms of sensitivity by 1.59\\% and AUC by 1.88\\%.",
        "trunc_text": "FAGT implementation on the ISIC-2016 dataset surpasses the baseline method in terms of sensitivity by 1.59\\% and AUC by ",
        "x1": 3.99625301361084,
        "x2": 4.8986358642578125,
        "y1": 6.413434028625488,
        "y2": 3.5609915256500244
      },
      {
        "r": 0,
        "text": "Furthermore, for the HAM10000 dataset, applying FABT outperforms the baseline approach in terms of recall by 13.75\\%, and with the sole implementation of FAGT, achieves a maximum accuracy of 94.44\\%.",
        "trunc_text": "Furthermore, for the HAM10000 dataset, applying FABT outperforms the baseline approach in terms of recall by 13.75\\%, an",
        "x1": 4.025331497192383,
        "x2": 4.918429374694824,
        "y1": 6.384583473205566,
        "y2": 3.55357027053833
      },
      {
        "r": 0,
        "text": "Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e.g. summarizing a paper) and conversations with more extensive histories.",
        "trunc_text": "Recently, there has been growing interest in extending the context length of instruction-following models in order to ef",
        "x1": 5.16093635559082,
        "x2": 3.928518772125244,
        "y1": 4.081092357635498,
        "y2": 6.155714988708496
      },
      {
        "r": 0,
        "text": "While proprietary models such as GPT-4 and Claude have demonstrated considerable advancements in handling tens of thousands of tokens of context, open-sourced models are still in the early stages of experimentation.",
        "trunc_text": "While proprietary models such as GPT-4 and Claude have demonstrated considerable advancements in handling tens of thousa",
        "x1": 7.476733207702637,
        "x2": 8.052386283874512,
        "y1": 6.59487247467041,
        "y2": 3.051417350769043
      },
      {
        "r": 0,
        "text": "It also remains unclear whether developing these long context models can offer substantial gains on practical downstream tasks over retrieval-based methods or models simply trained on chunked contexts.",
        "trunc_text": "It also remains unclear whether developing these long context models can offer substantial gains on practical downstream",
        "x1": 5.001116752624512,
        "x2": 3.5746543407440186,
        "y1": 3.6824758052825928,
        "y2": 6.269606590270996
      },
      {
        "r": 0,
        "text": "To address this challenge, we propose to institute standardized evaluation for long context language models.",
        "trunc_text": "To address this challenge, we propose to institute standardized evaluation for long context language models.",
        "x1": 4.95988130569458,
        "x2": 3.6647140979766846,
        "y1": 3.711109161376953,
        "y2": 6.192575454711914
      },
      {
        "r": 0,
        "text": "Concretely, we develop L-Eval which contains 411 long documents and over 2,000 query-response pairs manually annotated and checked by the authors encompassing areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings.",
        "trunc_text": "Concretely, we develop L-Eval which contains 411 long documents and over 2,000 query-response pairs manually annotated a",
        "x1": 4.615169525146484,
        "x2": 3.231689453125,
        "y1": 3.6047494411468506,
        "y2": 6.081069469451904
      },
      {
        "r": 0,
        "text": "L-Eval also adopts diverse evaluation methods and instruction styles, enabling a more reliable assessment of Long Context Language Models (LCLMs).",
        "trunc_text": "L-Eval also adopts diverse evaluation methods and instruction styles, enabling a more reliable assessment of Long Contex",
        "x1": 5.036497116088867,
        "x2": 3.7089505195617676,
        "y1": 3.798948049545288,
        "y2": 6.2261881828308105
      },
      {
        "r": 0,
        "text": "Our findings indicate that while open-source models typically lag behind their commercial counterparts, they still exhibit impressive performance.",
        "trunc_text": "Our findings indicate that while open-source models typically lag behind their commercial counterparts, they still exhib",
        "x1": 7.391451358795166,
        "x2": 8.01790714263916,
        "y1": 6.592257499694824,
        "y2": 3.119866132736206
      },
      {
        "r": 0,
        "text": "LLaMA2 achieves the best results (win 45\\% vs turbo-16k) on open-ended tasks with only 4k context length and ChatGLM2 achieves the best results on closed-ended tasks with 8k input tokens.",
        "trunc_text": "LLaMA2 achieves the best results (win 45\\% vs turbo-16k) on open-ended tasks with only 4k context length and ChatGLM2 ac",
        "x1": 6.265361309051514,
        "x2": 4.634552001953125,
        "y1": 3.895677089691162,
        "y2": 6.511112213134766
      },
      {
        "r": 0,
        "text": "We release our new evaluation suite, code, and all generation results including predictions from all open-sourced LCLMs, GPT4-32k, Cluade-100k at {\\url{https://github.com/OpenLMLab/LEval}}.",
        "trunc_text": "We release our new evaluation suite, code, and all generation results including predictions from all open-sourced LCLMs,",
        "x1": 6.4473724365234375,
        "x2": 7.370248317718506,
        "y1": 7.121500492095947,
        "y2": 2.8913955688476562
      },
      {
        "r": 0,
        "text": "Object localization in general environments is a fundamental part of vision systems.",
        "trunc_text": "Object localization in general environments is a fundamental part of vision systems.",
        "x1": 0.681591272354126,
        "x2": 2.2777717113494873,
        "y1": 7.971906661987305,
        "y2": 0.9331291913986206
      },
      {
        "r": 0,
        "text": "While dominating on the COCO benchmark, recent Transformer-based detection methods are not competitive in diverse domains.",
        "trunc_text": "While dominating on the COCO benchmark, recent Transformer-based detection methods are not competitive in diverse domain",
        "x1": 0.6566833257675171,
        "x2": 2.125319242477417,
        "y1": 7.354838848114014,
        "y2": 1.7515857219696045
      },
      {
        "r": 0,
        "text": "Moreover, these methods still struggle to very accurately estimate the object bounding boxes in complex environments.   ",
        "trunc_text": "Moreover, these methods still struggle to very accurately estimate the object bounding boxes in complex environments.   ",
        "x1": 0.5081011652946472,
        "x2": 2.089505910873413,
        "y1": 7.668743133544922,
        "y2": 1.169206142425537
      },
      {
        "r": 0,
        "text": "We introduce Cascade-DETR for high-quality universal object detection.",
        "trunc_text": "We introduce Cascade-DETR for high-quality universal object detection.",
        "x1": 0.575238049030304,
        "x2": 2.0753769874572754,
        "y1": 7.401420593261719,
        "y2": 1.5438804626464844
      },
      {
        "r": 0,
        "text": "We jointly tackle the generalization to diverse domains and localization accuracy by proposing the Cascade Attention layer, which explicitly integrates object-centric information into the detection decoder by limiting the attention to the previous box prediction.",
        "trunc_text": "We jointly tackle the generalization to diverse domains and localization accuracy by proposing the Cascade Attention lay",
        "x1": 0.5405192375183105,
        "x2": 2.1171438694000244,
        "y1": 7.716161727905273,
        "y2": 1.2262402772903442
      },
      {
        "r": 0,
        "text": "To further enhance accuracy, we also revisit the scoring of queries.",
        "trunc_text": "To further enhance accuracy, we also revisit the scoring of queries.",
        "x1": 4.609105110168457,
        "x2": 5.157453536987305,
        "y1": 5.920438289642334,
        "y2": 4.229777812957764
      },
      {
        "r": 0,
        "text": "Instead of relying on classification scores, we predict the expected IoU of the query, leading to substantially more well-calibrated confidences.",
        "trunc_text": "Instead of relying on classification scores, we predict the expected IoU of the query, leading to substantially more wel",
        "x1": 4.165248394012451,
        "x2": 4.926837921142578,
        "y1": 5.999411582946777,
        "y2": 4.137382984161377
      },
      {
        "r": 0,
        "text": "Lastly, we introduce a universal object detection benchmark, UDB10, that contains 10 datasets from diverse domains.",
        "trunc_text": "Lastly, we introduce a universal object detection benchmark, UDB10, that contains 10 datasets from diverse domains.",
        "x1": 0.6352911591529846,
        "x2": 2.0741491317749023,
        "y1": 7.438249111175537,
        "y2": 1.5210888385772705
      },
      {
        "r": 0,
        "text": "While also advancing the state-of-the-art on COCO, Cascade-DETR substantially improves DETR-based detectors on all datasets in UDB10, even by over 10 mAP in some cases.",
        "trunc_text": "While also advancing the state-of-the-art on COCO, Cascade-DETR substantially improves DETR-based detectors on all datas",
        "x1": 0.5881091952323914,
        "x2": 2.0060806274414062,
        "y1": 7.313154220581055,
        "y2": 1.7002367973327637
      },
      {
        "r": 0,
        "text": "The improvements under stringent quality requirements are even more pronounced.",
        "trunc_text": "The improvements under stringent quality requirements are even more pronounced.",
        "x1": 5.165017604827881,
        "x2": 5.74672269821167,
        "y1": 5.814251899719238,
        "y2": 4.271008014678955
      },
      {
        "r": 0,
        "text": "Our code and models will be released at https://github.com/SysCV/cascade-detr.",
        "trunc_text": "Our code and models will be released at https://github.com/SysCV/cascade-detr.",
        "x1": 7.582495212554932,
        "x2": 8.246041297912598,
        "y1": 7.530977249145508,
        "y2": 2.4399561882019043
      },
      {
        "r": 0,
        "text": "Neural Radiance Field (NeRF) is a promising approach for synthesizing novel views, given a set of images and the corresponding camera poses of a scene.",
        "trunc_text": "Neural Radiance Field (NeRF) is a promising approach for synthesizing novel views, given a set of images and the corresp",
        "x1": 2.0068156719207764,
        "x2": 4.275513172149658,
        "y1": 10.00615406036377,
        "y2": 0.48483577370643616
      },
      {
        "r": 0,
        "text": "However, images photographed from a low-light scene can hardly be used to train a NeRF model to produce high-quality results, due to their low pixel intensities, heavy noise, and color distortion.",
        "trunc_text": "However, images photographed from a low-light scene can hardly be used to train a NeRF model to produce high-quality res",
        "x1": 1.7657057046890259,
        "x2": 4.085414886474609,
        "y1": 9.891188621520996,
        "y2": 0.511532187461853
      },
      {
        "r": 0,
        "text": "Combining existing low-light image enhancement methods with NeRF methods also does not work well due to the view inconsistency caused by the individual 2D enhancement process.",
        "trunc_text": "Combining existing low-light image enhancement methods with NeRF methods also does not work well due to the view inconsi",
        "x1": 1.7109723091125488,
        "x2": 4.046662330627441,
        "y1": 9.847748756408691,
        "y2": 0.4817320704460144
      },
      {
        "r": 0,
        "text": "In this paper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), to enhance the scene representation and synthesize normal-light novel views directly from sRGB low-light images in an unsupervised manner.",
        "trunc_text": "In this paper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), to enhance the scene representation and s",
        "x1": 1.714731216430664,
        "x2": 4.121425151824951,
        "y1": 9.838277816772461,
        "y2": 0.4945979416370392
      },
      {
        "r": 0,
        "text": "The core of our approach is a decomposition of radiance field learning, which allows us to enhance the illumination, reduce noise and correct the distorted colors jointly with the NeRF optimization process.",
        "trunc_text": "The core of our approach is a decomposition of radiance field learning, which allows us to enhance the illumination, red",
        "x1": 1.9468636512756348,
        "x2": 4.298260688781738,
        "y1": 10.045966148376465,
        "y2": 0.47211340069770813
      },
      {
        "r": 0,
        "text": "Our method is able to produce novel view images with proper lighting and vivid colors and details, given a collection of camera-finished low dynamic range (8-bits/channel) images from a low-light scene.",
        "trunc_text": "Our method is able to produce novel view images with proper lighting and vivid colors and details, given a collection of",
        "x1": 1.5564110279083252,
        "x2": 3.838899612426758,
        "y1": 9.452347755432129,
        "y2": 0.47618308663368225
      },
      {
        "r": 0,
        "text": "Experiments demonstrate that our method outperforms existing low-light enhancement methods and NeRF methods.",
        "trunc_text": "Experiments demonstrate that our method outperforms existing low-light enhancement methods and NeRF methods.",
        "x1": 1.7710663080215454,
        "x2": 4.144340515136719,
        "y1": 9.991576194763184,
        "y2": 0.47503402829170227
      },
      {
        "r": 0,
        "text": "Using a scalable data engine that incorporates human feedback and efficient models in the loop, we create a new dataset (AS-1B) with over 1 billion regions annotated with semantic tags, question-answering pairs, and detailed captions.",
        "trunc_text": "Using a scalable data engine that incorporates human feedback and efficient models in the loop, we create a new dataset ",
        "x1": 3.7174973487854004,
        "x2": 2.816406488418579,
        "y1": 3.5875258445739746,
        "y2": 5.570780277252197
      },
      {
        "r": 0,
        "text": "Leveraging this new dataset, we develop the All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding",
        "trunc_text": "Leveraging this new dataset, we develop the All-Seeing model (ASM), a unified framework for panoptic visual recognition ",
        "x1": 1.5969817638397217,
        "x2": 3.5771307945251465,
        "y1": 7.504430294036865,
        "y2": 1.4082502126693726
      },
      {
        "r": 0,
        "text": "Models and the dataset shall be released at https://github.com/OpenGVLab/All-Seeing, and demo can be seen at https://huggingface.co/spaces/OpenGVLab/all-seeing.",
        "trunc_text": "Models and the dataset shall be released at https://github.com/OpenGVLab/All-Seeing, and demo can be seen at https://hug",
        "x1": 7.201259136199951,
        "x2": 7.645781993865967,
        "y1": 8.03812026977539,
        "y2": 1.8040077686309814
      },
      {
        "r": 0,
        "text": "We present the All-Seeing (AS) project: a large-scale data and model for recognizing and understanding everything in the open world.  ",
        "trunc_text": "We present the All-Seeing (AS) project: a large-scale data and model for recognizing and understanding everything in the",
        "x1": 1.6178840398788452,
        "x2": 3.7935562133789062,
        "y1": 7.541813373565674,
        "y2": 1.214964747428894
      },
      {
        "r": 0,
        "text": "It covers a wide range of 3.5 million common and rare concepts in the real world, and has 132.2 billion tokens that describe the concepts and their attributes.",
        "trunc_text": "It covers a wide range of 3.5 million common and rare concepts in the real world, and has 132.2 billion tokens that desc",
        "x1": 6.677685737609863,
        "x2": 6.637959957122803,
        "y1": 6.478496551513672,
        "y2": 3.1296043395996094
      },
      {
        "r": 0,
        "text": "Leveraging this new dataset, we develop the All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding.",
        "trunc_text": "Leveraging this new dataset, we develop the All-Seeing model (ASM), a unified framework for panoptic visual recognition ",
        "x1": 1.6348625421524048,
        "x2": 3.5978002548217773,
        "y1": 7.5101494789123535,
        "y2": 1.3663331270217896
      },
      {
        "r": 0,
        "text": "The model is trained with open-ended language prompts and locations, which allows it to generalioning, and question-answering.",
        "trunc_text": "The model is trained with open-ended language prompts and locations, which allows it to generalioning, and question-answ",
        "x1": 4.993610382080078,
        "x2": 3.7868738174438477,
        "y1": 3.965257167816162,
        "y2": 5.998802661895752
      },
      {
        "r": 0,
        "text": "We hope that this project can serve as a foundation for vision-language artificial general intelligence research.",
        "trunc_text": "We hope that this project can serve as a foundation for vision-language artificial general intelligence research.",
        "x1": 4.40877103805542,
        "x2": 2.9021999835968018,
        "y1": 3.7706401348114014,
        "y2": 4.842536926269531
      },
      {
        "r": 0,
        "text": "Conversational agents are consistently growing in popularity and many people interact with them every day.",
        "trunc_text": "Conversational agents are consistently growing in popularity and many people interact with them every day.",
        "x1": 5.824891567230225,
        "x2": 3.895507574081421,
        "y1": 3.378725528717041,
        "y2": 7.3238115310668945
      },
      {
        "r": 0,
        "text": "While many conversational agents act as personal assistants, they can have many different goals.",
        "trunc_text": "While many conversational agents act as personal assistants, they can have many different goals.",
        "x1": 5.832526206970215,
        "x2": 4.039077281951904,
        "y1": 3.481222152709961,
        "y2": 7.406306266784668
      },
      {
        "r": 0,
        "text": "Some are task-oriented, such as providing customer support for a bank or making a reservation.",
        "trunc_text": "Some are task-oriented, such as providing customer support for a bank or making a reservation.",
        "x1": 5.417325973510742,
        "x2": 4.804693222045898,
        "y1": 4.875279426574707,
        "y2": 5.400958061218262
      },
      {
        "r": 0,
        "text": "Others are designed to be empathetic and to form emotional connections with the user.",
        "trunc_text": "Others are designed to be empathetic and to form emotional connections with the user.",
        "x1": 4.397988796234131,
        "x2": 3.394444227218628,
        "y1": 4.2566447257995605,
        "y2": 5.2069783210754395
      },
      {
        "r": 0,
        "text": "The dataset has been scraped from Codeforces, a major competitive programming website.",
        "trunc_text": "The dataset has been scraped from Codeforces, a major competitive programming website.",
        "x1": 6.983511447906494,
        "x2": 7.487553119659424,
        "y1": 7.674144268035889,
        "y2": 2.2509756088256836
      },
      {
        "r": 0,
        "text": "In the past decade, the amount of research being done in the fields of machine learning and deep learning, predominantly in the area of natural language processing (NLP), has risen dramatically.",
        "trunc_text": "In the past decade, the amount of research being done in the fields of machine learning and deep learning, predominantly",
        "x1": 3.5837178230285645,
        "x2": 3.0017247200012207,
        "y1": 3.823538064956665,
        "y2": 5.521258354187012
      },
      {
        "r": 0,
        "text": "A well-liked method for developing programming abilities like logic building and problem solving is competitive programming.",
        "trunc_text": "A well-liked method for developing programming abilities like logic building and problem solving is competitive programm",
        "x1": 4.967006206512451,
        "x2": 4.004191875457764,
        "y1": 4.590821743011475,
        "y2": 5.428184986114502
      },
      {
        "r": 0,
        "text": "It can be tough for novices and even veteran programmers to traverse the wide collection of questions due to the massive number of accessible questions and the variety of themes, levels of difficulty, and questions offered.",
        "trunc_text": "It can be tough for novices and even veteran programmers to traverse the wide collection of questions due to the massive",
        "x1": 5.212336540222168,
        "x2": 4.065432548522949,
        "y1": 4.367104530334473,
        "y2": 5.821592330932617
      },
      {
        "r": 0,
        "text": "In order to help programmers find questions that are appropriate for their knowledge and interests, there is a need for an automated method.",
        "trunc_text": "In order to help programmers find questions that are appropriate for their knowledge and interests, there is a need for ",
        "x1": 5.110246181488037,
        "x2": 3.9665396213531494,
        "y1": 4.443980693817139,
        "y2": 5.713621139526367
      },
      {
        "r": 0,
        "text": "This can be done using automated tagging of the questions using Text Classification.",
        "trunc_text": "This can be done using automated tagging of the questions using Text Classification.",
        "x1": 3.4084198474884033,
        "x2": 3.401541233062744,
        "y1": 3.985008478164673,
        "y2": 5.452775001525879
      },
      {
        "r": 0,
        "text": "Text classification is one of the important tasks widely researched in the field of Natural Language Processing.",
        "trunc_text": "Text classification is one of the important tasks widely researched in the field of Natural Language Processing.",
        "x1": 3.3592121601104736,
        "x2": 3.1855082511901855,
        "y1": 3.8720600605010986,
        "y2": 5.385012149810791
      },
      {
        "r": 0,
        "text": "In this paper, we present a way to use text classification techniques to determine the domain of a competitive programming problem.",
        "trunc_text": "In this paper, we present a way to use text classification techniques to determine the domain of a competitive programmi",
        "x1": 3.384472370147705,
        "x2": 3.3654727935791016,
        "y1": 3.9836716651916504,
        "y2": 5.338955402374268
      },
      {
        "r": 0,
        "text": "A variety of models, including are implemented LSTM, GRU, and MLP.  ",
        "trunc_text": "A variety of models, including are implemented LSTM, GRU, and MLP.  ",
        "x1": 5.311323165893555,
        "x2": 4.24233865737915,
        "y1": 4.572058200836182,
        "y2": 5.832920551300049
      },
      {
        "r": 0,
        "text": "A total of 2400 problems were scraped and preprocessed, which we used as a dataset for our training and testing of models.",
        "trunc_text": "A total of 2400 problems were scraped and preprocessed, which we used as a dataset for our training and testing of model",
        "x1": 4.959244728088379,
        "x2": 5.599918842315674,
        "y1": 6.954931259155273,
        "y2": 2.8946754932403564
      },
      {
        "r": 0,
        "text": "The maximum accuracy reached using our model is 78.0% by MLP(Multi Layer Perceptron).",
        "trunc_text": "The maximum accuracy reached using our model is 78.0% by MLP(Multi Layer Perceptron).",
        "x1": 4.271212577819824,
        "x2": 5.259531021118164,
        "y1": 6.173983097076416,
        "y2": 4.086915016174316
      },
      {
        "r": 0,
        "text": "Open-source EDA shows promising potential in unleashing EDA innovation and lowering the cost of chip design.",
        "trunc_text": "Open-source EDA shows promising potential in unleashing EDA innovation and lowering the cost of chip design.",
        "x1": 7.216351509094238,
        "x2": 7.851592540740967,
        "y1": 6.66132926940918,
        "y2": 3.0772924423217773
      },
      {
        "r": 0,
        "text": "This paper presents an open-source EDA project, iEDA, aiming for building a basic infrastructure for EDA technology evolution and closing the industrial-academic gap in the EDA area.",
        "trunc_text": "This paper presents an open-source EDA project, iEDA, aiming for building a basic infrastructure for EDA technology evol",
        "x1": 7.204765319824219,
        "x2": 7.779574871063232,
        "y1": 6.801457405090332,
        "y2": 3.0432095527648926
      },
      {
        "r": 0,
        "text": "iEDA now covers the whole flow of physical design (including Floorplan, Placement, CTS, Routing, Timing Optimization etc.), and part of the analysis tools (Static Timing Analysis and Power Analysis).",
        "trunc_text": "iEDA now covers the whole flow of physical design (including Floorplan, Placement, CTS, Routing, Timing Optimization etc",
        "x1": 7.202347755432129,
        "x2": 7.777417182922363,
        "y1": 6.821651935577393,
        "y2": 3.019700765609741
      },
      {
        "r": 0,
        "text": "To demonstrate the effectiveness of iEDA, we implement and tape out three chips of different scales (from 700k to 1.5M gates) on different process nodes (110nm and 28nm) with iEDA.",
        "trunc_text": "To demonstrate the effectiveness of iEDA, we implement and tape out three chips of different scales (from 700k to 1.5M g",
        "x1": 7.220171928405762,
        "x2": 7.777527332305908,
        "y1": 6.676405429840088,
        "y2": 3.1186342239379883
      },
      {
        "r": 0,
        "text": "This paper addresses such issues with several contributions: (1) we introduce models for interaction signature estimation (ISP) encompassing contact detection, segmentation, and 3d contact signature prediction; (2) we show how such components can be leveraged to ensure contact consistency during 3d reconstruction; (3) we construct several large datasets for learning and evaluating 3d contact prediction and reconstruction methods; specifically, we introduce CHI3D, a lab-based accurate 3d motion capture dataset with 631 sequences containing $2,525$ contact events, $728,664$ ground truth 3d poses, as well as FlickrCI3D, a dataset of $11,216$ images, with $14,081$ processed pairs of people, and $81,233$ facet-level surface correspondences.",
        "trunc_text": "This paper addresses such issues with several contributions: (1) we introduce models for interaction signature estimatio",
        "x1": 2.468907356262207,
        "x2": 3.833423137664795,
        "y1": 9.255928993225098,
        "y2": -0.4109286665916443
      },
      {
        "r": 0,
        "text": "Understanding 3d human interactions is fundamental for fine-grained scene analysis and behavioural modeling.",
        "trunc_text": "Understanding 3d human interactions is fundamental for fine-grained scene analysis and behavioural modeling.",
        "x1": 2.5817298889160156,
        "x2": 3.996912956237793,
        "y1": 9.309757232666016,
        "y2": -0.37535783648490906
      },
      {
        "r": 0,
        "text": "However, most of the existing models predict incorrect, lifeless 3d estimates, that miss the subtle human contact aspects--the essence of the event--and are of little use for detailed behavioral understanding.  ",
        "trunc_text": "However, most of the existing models predict incorrect, lifeless 3d estimates, that miss the subtle human contact aspect",
        "x1": 3.0981268882751465,
        "x2": 4.210626125335693,
        "y1": 9.014283180236816,
        "y2": -0.4093630611896515
      },
      {
        "r": 0,
        "text": "Finally, (4) we propose methodology for recovering the ground-truth pose and shape of interacting people in a controlled setup and (5) annotate all 3d interaction motions in CHI3D with textual descriptions.",
        "trunc_text": "Finally, (4) we propose methodology for recovering the ground-truth pose and shape of interacting people in a controlled",
        "x1": 2.4684665203094482,
        "x2": 3.8479838371276855,
        "y1": 9.366683959960938,
        "y2": -0.4115227162837982
      },
      {
        "r": 0,
        "text": "Motion data in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) is made available for research purposes at \\url{https://ci3d.imar.ro}, together with an evaluation server and a public benchmark.",
        "trunc_text": "Motion data in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) is made available for research purposes",
        "x1": 2.512855052947998,
        "x2": 3.9274508953094482,
        "y1": 9.486806869506836,
        "y2": -0.48031091690063477
      },
      {
        "r": 0,
        "text": "Our dataset, consisting of input and output images, was generated by implementing boundary conditions and extracting stress-strain field maps.",
        "trunc_text": "Our dataset, consisting of input and output images, was generated by implementing boundary conditions and extracting str",
        "x1": 1.8770004510879517,
        "x2": 3.590630292892456,
        "y1": 6.727339267730713,
        "y2": 2.1434476375579834
      },
      {
        "r": 0,
        "text": "This study investigated the potential of end-to-end deep learning tools as a more effective substitute for FEM in predicting stress-strain fields within 2D cross sections of arterial wall.",
        "trunc_text": "This study investigated the potential of end-to-end deep learning tools as a more effective substitute for FEM in predic",
        "x1": 1.8419245481491089,
        "x2": 3.4785032272338867,
        "y1": 6.700179576873779,
        "y2": 2.4237759113311768
      },
      {
        "r": 0,
        "text": "We first proposed a U-Net based fully convolutional neural network (CNN) to predict the von Mises stress and strain distribution based on the spatial arrangement of calcification within arterial wall cross-sections.",
        "trunc_text": "We first proposed a U-Net based fully convolutional neural network (CNN) to predict the von Mises stress and strain dist",
        "x1": 1.7804235219955444,
        "x2": 3.3901009559631348,
        "y1": 6.655735015869141,
        "y2": 2.313546657562256
      },
      {
        "r": 0,
        "text": "Further, we developed a conditional generative adversarial network (cGAN) to enhance, particularly from the perceptual perspective, the prediction accuracy of stress and strain field maps for arterial walls with various calcification quantities and spatial configurations.",
        "trunc_text": "Further, we developed a conditional generative adversarial network (cGAN) to enhance, particularly from the perceptual p",
        "x1": 1.8679999113082886,
        "x2": 3.4386353492736816,
        "y1": 6.721570014953613,
        "y2": 2.2683029174804688
      },
      {
        "r": 0,
        "text": "On top of U-Net and cGAN, we also proposed their ensemble approaches, respectively, to further improve the prediction accuracy of field maps.  ",
        "trunc_text": "On top of U-Net and cGAN, we also proposed their ensemble approaches, respectively, to further improve the prediction ac",
        "x1": 2.0395936965942383,
        "x2": 3.477262496948242,
        "y1": 6.816847801208496,
        "y2": 2.301992177963257
      },
      {
        "r": 0,
        "text": "The trained U-Net models can accurately predict von Mises stress and strain fields, with structural similarity index scores (SSIM) of 0.854 and 0.830 and mean squared errors of 0.017 and 0.018 for stress and strain, respectively, on a reserved test set.",
        "trunc_text": "The trained U-Net models can accurately predict von Mises stress and strain fields, with structural similarity index sco",
        "x1": 1.963815450668335,
        "x2": 3.5262022018432617,
        "y1": 6.595313549041748,
        "y2": 2.4891748428344727
      },
      {
        "r": 0,
        "text": "Meanwhile, the cGAN models in a combination of ensemble and transfer learning techniques demonstrate high accuracy in predicting von Mises stress and strain fields, as evidenced by SSIM scores of 0.890 for stress and 0.803 for strain.",
        "trunc_text": "Meanwhile, the cGAN models in a combination of ensemble and transfer learning techniques demonstrate high accuracy in pr",
        "x1": 1.9961118698120117,
        "x2": 3.464963436126709,
        "y1": 6.633511066436768,
        "y2": 2.361198663711548
      },
      {
        "r": 0,
        "text": "Additionally, mean squared errors of 0.008 for stress and 0.017 for strain further support the model's performance on a designated test set.",
        "trunc_text": "Additionally, mean squared errors of 0.008 for stress and 0.017 for strain further support the model's performance on a ",
        "x1": 1.9680702686309814,
        "x2": 3.6604161262512207,
        "y1": 6.579996109008789,
        "y2": 2.5392279624938965
      },
      {
        "r": 0,
        "text": "Overall, this study developed a surrogate model for finite element analysis, which can accurately and efficiently predict stress-strain fields of arterial walls regardless of complex geometries and boundary conditions.",
        "trunc_text": "Overall, this study developed a surrogate model for finite element analysis, which can accurately and efficiently predic",
        "x1": 1.8899118900299072,
        "x2": 3.495577335357666,
        "y1": 6.717787742614746,
        "y2": 2.376255512237549
      },
      {
        "r": 0,
        "text": "The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final diagnoses, and condition codes.",
        "trunc_text": "The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final ",
        "x1": 1.5087871551513672,
        "x2": 3.5459113121032715,
        "y1": 6.746929168701172,
        "y2": 1.9174559116363525
      },
      {
        "r": 0,
        "text": "This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex, domain-specific tasks.",
        "trunc_text": "This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with",
        "x1": 5.121315002441406,
        "x2": 3.7415521144866943,
        "y1": 3.9140450954437256,
        "y2": 6.177952289581299
      },
      {
        "r": 0,
        "text": "Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports.",
        "trunc_text": "Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports.",
        "x1": 3.237413167953491,
        "x2": 4.20404052734375,
        "y1": 7.4464263916015625,
        "y2": 2.0805928707122803
      },
      {
        "r": 0,
        "text": "The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and provide structured outputs.  ",
        "trunc_text": "The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and pr",
        "x1": 5.765576362609863,
        "x2": 4.580170154571533,
        "y1": 4.12190055847168,
        "y2": 6.377042770385742
      },
      {
        "r": 0,
        "text": "They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance.",
        "trunc_text": "They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance.",
        "x1": 5.4802093505859375,
        "x2": 4.442525386810303,
        "y1": 4.837790489196777,
        "y2": 5.94519567489624
      },
      {
        "r": 0,
        "text": "The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, even with extremely reduced precision.",
        "trunc_text": "The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, ev",
        "x1": 5.455674648284912,
        "x2": 4.434959888458252,
        "y1": 4.954898357391357,
        "y2": 5.87277889251709
      },
      {
        "r": 0,
        "text": "The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-label tasks.",
        "trunc_text": "The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-lab",
        "x1": 5.62733268737793,
        "x2": 4.545112609863281,
        "y1": 4.859665393829346,
        "y2": 5.9757256507873535
      },
      {
        "r": 0,
        "text": "Overall, this work presents an effective approach for utilizing LLMs to perform domain-specific tasks using accessible hardware, with potential applications in the medical domain, where complex data extraction and classification are required.",
        "trunc_text": "Overall, this work presents an effective approach for utilizing LLMs to perform domain-specific tasks using accessible h",
        "x1": 6.184106349945068,
        "x2": 4.905972480773926,
        "y1": 4.640696048736572,
        "y2": 6.344579219818115
      },
      {
        "r": 0,
        "text": "Patch-DM produces high-quality image synthesis results on our newly collected dataset of nature images (1024$\\times$512), as well as on standard benchmarks of smaller sizes (256$\\times$256), including LSUN-Bedroom, LSUN-Church, and FFHQ.",
        "trunc_text": "Patch-DM produces high-quality image synthesis results on our newly collected dataset of nature images (1024$\\times$512)",
        "x1": 2.0642426013946533,
        "x2": 3.445542573928833,
        "y1": 7.471395969390869,
        "y2": 1.6261793375015259
      },
      {
        "r": 0,
        "text": "We propose an effective denoising diffusion model for generating high-resolution images (e.g., 1024$\\times$512), trained on small-size image patches (e.g., 64$\\times$64).",
        "trunc_text": "We propose an effective denoising diffusion model for generating high-resolution images (e.g., 1024$\\times$512), trained",
        "x1": 2.024351119995117,
        "x2": 3.2029101848602295,
        "y1": 7.047584056854248,
        "y2": 1.9038608074188232
      },
      {
        "r": 0,
        "text": "We name our algorithm Patch-DM, in which a new feature collage strategy is designed to avoid the boundary artifact when synthesizing large-size images.",
        "trunc_text": "We name our algorithm Patch-DM, in which a new feature collage strategy is designed to avoid the boundary artifact when ",
        "x1": 1.951054334640503,
        "x2": 3.3840725421905518,
        "y1": 7.442701816558838,
        "y2": 1.5949063301086426
      },
      {
        "r": 0,
        "text": "Feature collage systematically crops and combines partial features of the neighboring patches to predict the features of a shifted image patch, allowing the seamless generation of the entire image due to the overlap in the patch feature space.  ",
        "trunc_text": "Feature collage systematically crops and combines partial features of the neighboring patches to predict the features of",
        "x1": 1.8647468090057373,
        "x2": 3.371673822402954,
        "y1": 7.578822612762451,
        "y2": 1.4898563623428345
      },
      {
        "r": 0,
        "text": "We compare our method with previous patch-based generation methods and achieve state-of-the-art FID scores on all four datasets.",
        "trunc_text": "We compare our method with previous patch-based generation methods and achieve state-of-the-art FID scores on all four d",
        "x1": 4.500668525695801,
        "x2": 5.101231098175049,
        "y1": 6.791749954223633,
        "y2": 3.132875919342041
      },
      {
        "r": 0,
        "text": "Further, Patch-DM also reduces memory complexity compared to the classic diffusion models.",
        "trunc_text": "Further, Patch-DM also reduces memory complexity compared to the classic diffusion models.",
        "x1": 2.4099483489990234,
        "x2": 3.2909185886383057,
        "y1": 6.485187530517578,
        "y2": 2.3029379844665527
      },
      {
        "r": 0,
        "text": "Next basket recommendation (NBR) is the task of predicting the next set of items based on a sequence of already purchased baskets.",
        "trunc_text": "Next basket recommendation (NBR) is the task of predicting the next set of items based on a sequence of already purchase",
        "x1": 4.578085422515869,
        "x2": 4.907036781311035,
        "y1": 5.319186687469482,
        "y2": 5.03311014175415
      },
      {
        "r": 0,
        "text": "It is a recommendation task that has been widely studied, especially in the context of grocery shopping.",
        "trunc_text": "It is a recommendation task that has been widely studied, especially in the context of grocery shopping.",
        "x1": 4.632986545562744,
        "x2": 4.841136932373047,
        "y1": 5.18793249130249,
        "y2": 5.114411354064941
      },
      {
        "r": 0,
        "text": "In next basket recommendation (NBR), it is useful to distinguish between repeat items, i.e., items that a user has consumed before, and explore items, i.e., items that a user has not consumed before.",
        "trunc_text": "In next basket recommendation (NBR), it is useful to distinguish between repeat items, i.e., items that a user has consu",
        "x1": 4.57764196395874,
        "x2": 4.910371780395508,
        "y1": 5.300140857696533,
        "y2": 5.035349369049072
      },
      {
        "r": 0,
        "text": "Most NBR work either ignores this distinction or focuses on repeat items.",
        "trunc_text": "Most NBR work either ignores this distinction or focuses on repeat items.",
        "x1": 4.52805233001709,
        "x2": 4.906766891479492,
        "y1": 5.3250412940979,
        "y2": 4.979547500610352
      },
      {
        "r": 0,
        "text": "We formulate the next novel basket recommendation (NNBR) task, i.e., the task of recommending a basket that only consists of novel items, which is valuable for both real-world application and NBR evaluation.",
        "trunc_text": "We formulate the next novel basket recommendation (NNBR) task, i.e., the task of recommending a basket that only consist",
        "x1": 4.658576965332031,
        "x2": 4.873264312744141,
        "y1": 5.229098320007324,
        "y2": 5.049462795257568
      },
      {
        "r": 0,
        "text": "We evaluate how existing NBR methods perform on the NNBR task and find that, so far, limited progress has been made w.r.t.",
        "trunc_text": "We evaluate how existing NBR methods perform on the NNBR task and find that, so far, limited progress has been made w.r.",
        "x1": 4.617361068725586,
        "x2": 4.956070423126221,
        "y1": 5.4263715744018555,
        "y2": 4.920016765594482
      },
      {
        "r": 0,
        "text": "the NNBR task.",
        "trunc_text": "the NNBR task.",
        "x1": 4.639692783355713,
        "x2": 4.914064884185791,
        "y1": 5.421466827392578,
        "y2": 4.982585430145264
      },
      {
        "r": 0,
        "text": "To address the NNBR task, we propose a simple bi-directional transformer basket recommendation model (BTBR), which is focused on directly modeling item-to-item correlations within and across baskets instead of learning complex basket representations.",
        "trunc_text": "To address the NNBR task, we propose a simple bi-directional transformer basket recommendation model (BTBR), which is fo",
        "x1": 4.561376094818115,
        "x2": 4.810172080993652,
        "y1": 5.194671154022217,
        "y2": 5.072469711303711
      },
      {
        "r": 0,
        "text": "To properly train BTBR, we propose and investigate several masking strategies and training objectives: (i) item-level random masking, (ii) item-level select masking, (iii) basket-level all masking, (iv) basket-level explore masking, and (v) joint masking.",
        "trunc_text": "To properly train BTBR, we propose and investigate several masking strategies and training objectives: (i) item-level ra",
        "x1": 4.462628364562988,
        "x2": 4.884480953216553,
        "y1": 5.401418209075928,
        "y2": 4.9481940269470215
      },
      {
        "r": 0,
        "text": "In addition, an item-basket swapping strategy is proposed to enrich the item interactions within the same baskets.",
        "trunc_text": "In addition, an item-basket swapping strategy is proposed to enrich the item interactions within the same baskets.",
        "x1": 4.540685176849365,
        "x2": 4.8780059814453125,
        "y1": 5.249424934387207,
        "y2": 5.0659308433532715
      },
      {
        "r": 0,
        "text": "We conduct extensive experiments on three open datasets with various characteristics.",
        "trunc_text": "We conduct extensive experiments on three open datasets with various characteristics.",
        "x1": 4.503422260284424,
        "x2": 5.292106628417969,
        "y1": 7.115005016326904,
        "y2": 2.9657506942749023
      },
      {
        "r": 0,
        "text": "The results demonstrate the effectiveness of BTBR and our masking and swapping strategies for the NNBR task.",
        "trunc_text": "The results demonstrate the effectiveness of BTBR and our masking and swapping strategies for the NNBR task.",
        "x1": 4.561756134033203,
        "x2": 4.962985515594482,
        "y1": 5.395003318786621,
        "y2": 4.960143566131592
      },
      {
        "r": 0,
        "text": "BTBR with a properly selected masking and swapping strategy can substantially improve NNBR performance.",
        "trunc_text": "BTBR with a properly selected masking and swapping strategy can substantially improve NNBR performance.",
        "x1": 4.6024394035339355,
        "x2": 4.974889278411865,
        "y1": 5.385024547576904,
        "y2": 4.962601661682129
      },
      {
        "r": 0,
        "text": "Enumeration kernelization was first proposed by Creignou et al.",
        "trunc_text": "Enumeration kernelization was first proposed by Creignou et al.",
        "x1": 2.6703460216522217,
        "x2": 4.654634475708008,
        "y1": 5.431098937988281,
        "y2": 4.3389363288879395
      },
      {
        "r": 0,
        "text": "[TOCS 2017] and was later refined by Golovach et al.",
        "trunc_text": "[TOCS 2017] and was later refined by Golovach et al.",
        "x1": 6.792585849761963,
        "x2": 7.418827056884766,
        "y1": 7.098084449768066,
        "y2": 2.951836585998535
      },
      {
        "r": 0,
        "text": "[JCSS 2022] into two different variants: fully-polynomial enumeration kernelization and polynomial-delay enumeration kernelization.",
        "trunc_text": "[JCSS 2022] into two different variants: fully-polynomial enumeration kernelization and polynomial-delay enumeration ker",
        "x1": 2.6426849365234375,
        "x2": 4.593128681182861,
        "y1": 5.339710712432861,
        "y2": 4.324124813079834
      },
      {
        "r": 0,
        "text": "In this paper, we consider the d-CUT problem from the perspective of (polynomial-delay) enumeration kenrelization.",
        "trunc_text": "In this paper, we consider the d-CUT problem from the perspective of (polynomial-delay) enumeration kenrelization.",
        "x1": 2.655353307723999,
        "x2": 4.5035719871521,
        "y1": 5.351956367492676,
        "y2": 4.378993034362793
      },
      {
        "r": 0,
        "text": "Given an undirected graph G = (V, E), a cut F = E(A, B) is a d-cut of G if every u in A has at most d neighbors in B and every v in B has at most d neighbors in A. Checking the existence of a d-cut in a graph is a well-known NP-hard problem and is well-studied in parameterized complexity",
        "trunc_text": "Given an undirected graph G = (V, E), a cut F = E(A, B) is a d-cut of G if every u in A has at most d neighbors in B and",
        "x1": 2.6716599464416504,
        "x2": 4.467393398284912,
        "y1": 5.340649604797363,
        "y2": 4.381078720092773
      },
      {
        "r": 0,
        "text": "[Algorithmica 2021, IWOCA 2021].",
        "trunc_text": "[Algorithmica 2021, IWOCA 2021].",
        "x1": 7.173065185546875,
        "x2": 7.944371700286865,
        "y1": 7.244837760925293,
        "y2": 2.6818130016326904
      },
      {
        "r": 0,
        "text": "This problem also generalizes a well-studied problem MATCHING CUT (set d = 1) that has been a central problem in the literature of polynomial-delay enumeration kernelization.",
        "trunc_text": "This problem also generalizes a well-studied problem MATCHING CUT (set d = 1) that has been a central problem in the lit",
        "x1": 2.6480371952056885,
        "x2": 4.5368146896362305,
        "y1": 5.338846683502197,
        "y2": 4.382761001586914
      },
      {
        "r": 0,
        "text": "In this paper, we study three different enumeration variants of this problem, ENUM d-CUT, ENUM MIN-d-CUT and ENUM MAX-d-CUT that intends to enumerate all the d-cuts, all the minimal d-cuts and all the maximal d-cuts respectively.",
        "trunc_text": "In this paper, we study three different enumeration variants of this problem, ENUM d-CUT, ENUM MIN-d-CUT and ENUM MAX-d-",
        "x1": 2.63576340675354,
        "x2": 4.528499603271484,
        "y1": 5.321855068206787,
        "y2": 4.389246463775635
      },
      {
        "r": 0,
        "text": "We consider various structural parameters of the input and provide polynomial-delay enumeration kernels for ENUM d-CUT and ENUM MAX-d-CUT and fully-polynomial enumeration kernels of polynomial size for ENUM MIN-d-CUT.",
        "trunc_text": "We consider various structural parameters of the input and provide polynomial-delay enumeration kernels for ENUM d-CUT a",
        "x1": 2.6445579528808594,
        "x2": 4.554903507232666,
        "y1": 5.307662487030029,
        "y2": 4.355074405670166
      },
      {
        "r": 0,
        "text": "Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content generation at scale.",
        "trunc_text": "Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content g",
        "x1": 5.460289001464844,
        "x2": 4.023369789123535,
        "y1": 3.917393922805786,
        "y2": 6.346593379974365
      },
      {
        "r": 0,
        "text": "Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator.",
        "trunc_text": "Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector ",
        "x1": 6.215019226074219,
        "x2": 4.077718734741211,
        "y1": 3.398519515991211,
        "y2": 6.815103054046631
      },
      {
        "r": 0,
        "text": "We evaluate the zero-shot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perform experiments on publicly available datasets.",
        "trunc_text": "We evaluate the zero-shot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perfo",
        "x1": 6.303891181945801,
        "x2": 4.202852725982666,
        "y1": 3.3479154109954834,
        "y2": 6.9586873054504395
      },
      {
        "r": 0,
        "text": "We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text.",
        "trunc_text": "We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text.",
        "x1": 6.219208240509033,
        "x2": 4.197840690612793,
        "y1": 3.399426221847534,
        "y2": 6.9533867835998535
      },
      {
        "r": 0,
        "text": "Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply focusing on solving a specific aspect of the problem and deriving the rest from that solution.",
        "trunc_text": "Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply",
        "x1": 6.503854751586914,
        "x2": 4.65681266784668,
        "y1": 3.7396140098571777,
        "y2": 6.876033782958984
      },
      {
        "r": 0,
        "text": "All code and data is available at \\url{https://github.com/AmritaBh/ChatGPT-as-Detector}.",
        "trunc_text": "All code and data is available at \\url{https://github.com/AmritaBh/ChatGPT-as-Detector}.",
        "x1": 6.651196479797363,
        "x2": 4.80514669418335,
        "y1": 3.7080397605895996,
        "y2": 7.016979217529297
      },
      {
        "r": 0,
        "text": "As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar angle into account in a NeRF-based framework for rendering a scene from a novel viewpoint using satellite images for training.",
        "trunc_text": "As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar angle into account in a NeRF-based framework f",
        "x1": 1.9107489585876465,
        "x2": 4.243854522705078,
        "y1": 9.999022483825684,
        "y2": 0.533042848110199
      },
      {
        "r": 0,
        "text": "Our work extends those contributions and shows how one can make the renderings season-specific.",
        "trunc_text": "Our work extends those contributions and shows how one can make the renderings season-specific.",
        "x1": 2.045849323272705,
        "x2": 4.294315338134766,
        "y1": 9.807958602905273,
        "y2": 0.7308817505836487
      },
      {
        "r": 0,
        "text": "Our main challenge was creating a Neural Radiance Field (NeRF) that could render seasonal features independently of viewing angle and solar angle while still being able to render shadows.",
        "trunc_text": "Our main challenge was creating a Neural Radiance Field (NeRF) that could render seasonal features independently of view",
        "x1": 1.9582746028900146,
        "x2": 4.329861164093018,
        "y1": 10.049710273742676,
        "y2": 0.6281788349151611
      },
      {
        "r": 0,
        "text": "We teach our network to render seasonal features by introducing one more input variable -- time of the year.",
        "trunc_text": "We teach our network to render seasonal features by introducing one more input variable -- time of the year.",
        "x1": 1.9978909492492676,
        "x2": 4.395118236541748,
        "y1": 9.984452247619629,
        "y2": 0.7412959337234497
      },
      {
        "r": 0,
        "text": "However, the small training datasets typical of satellite imagery can introduce ambiguities in cases where shadows are present in the same location for every image of a particular season.",
        "trunc_text": "However, the small training datasets typical of satellite imagery can introduce ambiguities in cases where shadows are p",
        "x1": 1.7903035879135132,
        "x2": 4.3762993812561035,
        "y1": 9.932701110839844,
        "y2": 0.8287363648414612
      },
      {
        "r": 0,
        "text": "We add additional terms to the loss function to discourage the network from using seasonal features for accounting for shadows.",
        "trunc_text": "We add additional terms to the loss function to discourage the network from using seasonal features for accounting for s",
        "x1": 1.9475891590118408,
        "x2": 4.42868709564209,
        "y1": 9.996177673339844,
        "y2": 0.7224465012550354
      },
      {
        "r": 0,
        "text": "We show the performance of our network on eight Areas of Interest containing images captured by the Maxar WorldView-3 satellite.",
        "trunc_text": "We show the performance of our network on eight Areas of Interest containing images captured by the Maxar WorldView-3 sa",
        "x1": 1.8015810251235962,
        "x2": 4.339029788970947,
        "y1": 8.317358016967773,
        "y2": 1.0978925228118896
      },
      {
        "r": 0,
        "text": "This evaluation includes tests measuring the ability of our framework to accurately render novel views, generate height maps, predict shadows, and specify seasonal features independently from shadows.",
        "trunc_text": "This evaluation includes tests measuring the ability of our framework to accurately render novel views, generate height ",
        "x1": 1.9035485982894897,
        "x2": 4.188685894012451,
        "y1": 9.883055686950684,
        "y2": 0.7589190602302551
      },
      {
        "r": 0,
        "text": "Our ablation studies justify the choices made for network design parameters.",
        "trunc_text": "Our ablation studies justify the choices made for network design parameters.",
        "x1": 5.809634685516357,
        "x2": 6.647955894470215,
        "y1": 8.704338073730469,
        "y2": 5.299111366271973
      },
      {
        "r": 0,
        "text": "To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models.",
        "trunc_text": "To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D mo",
        "x1": 2.3142948150634766,
        "x2": 3.985206127166748,
        "y1": 8.409002304077148,
        "y2": 0.9768391251564026
      },
      {
        "r": 0,
        "text": "Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry.",
        "trunc_text": "Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vi",
        "x1": 2.0806593894958496,
        "x2": 3.8389341831207275,
        "y1": 8.598541259765625,
        "y2": 0.7972644567489624
      },
      {
        "r": 0,
        "text": "However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries.",
        "trunc_text": "However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing ch",
        "x1": 5.4378132820129395,
        "x2": 5.981015205383301,
        "y1": 6.888974189758301,
        "y2": 2.691788911819458
      },
      {
        "r": 0,
        "text": "Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage.  ",
        "trunc_text": "Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage.  ",
        "x1": 2.3562941551208496,
        "x2": 4.080432415008545,
        "y1": 8.503255844116211,
        "y2": 1.005165457725525
      },
      {
        "r": 0,
        "text": "Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques.",
        "trunc_text": "Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques.",
        "x1": 1.2525132894515991,
        "x2": 3.4083757400512695,
        "y1": 9.138559341430664,
        "y2": 0.16462074220180511
      },
      {
        "r": 0,
        "text": "It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances.",
        "trunc_text": "It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advance",
        "x1": 1.513502597808838,
        "x2": 3.6103768348693848,
        "y1": 9.078346252441406,
        "y2": 0.17098639905452728
      },
      {
        "r": 0,
        "text": "Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment.",
        "trunc_text": "Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud ",
        "x1": 7.40958833694458,
        "x2": 7.838616847991943,
        "y1": 8.071292877197266,
        "y2": 1.7573328018188477
      },
      {
        "r": 0,
        "text": "In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images.",
        "trunc_text": "In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, usin",
        "x1": 2.312875986099243,
        "x2": 4.025188446044922,
        "y1": 8.429686546325684,
        "y2": 1.0265001058578491
      },
      {
        "r": 0,
        "text": "These models are available for viewing, interaction, and download on the Tirtha website.",
        "trunc_text": "These models are available for viewing, interaction, and download on the Tirtha website.",
        "x1": 7.543263912200928,
        "x2": 7.917539596557617,
        "y1": 8.067469596862793,
        "y2": 1.759731411933899
      },
      {
        "r": 0,
        "text": "Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage co",
        "trunc_text": "Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, herita",
        "x1": 2.3599088191986084,
        "x2": 3.9846627712249756,
        "y1": 8.402013778686523,
        "y2": 1.0426528453826904
      },
      {
        "r": 0,
        "text": "We also present the musdb-XL-train dataset, consisting of 300k segments created by applying a commercial limiter plug-in for training real-world friendly de-limiter networks.",
        "trunc_text": "We also present the musdb-XL-train dataset, consisting of 300k segments created by applying a commercial limiter plug-in",
        "x1": 5.351949691772461,
        "x2": 6.882906436920166,
        "y1": 6.67358922958374,
        "y2": 2.1122195720672607
      },
      {
        "r": 0,
        "text": "The loudness war, an ongoing phenomenon in the music industry characterized by the increasing final loudness of music while reducing its dynamic range, has been a controversial topic for decades.",
        "trunc_text": "The loudness war, an ongoing phenomenon in the music industry characterized by the increasing final loudness of music wh",
        "x1": 2.80464243888855,
        "x2": 0.8687313199043274,
        "y1": 2.259619951248169,
        "y2": 5.996651649475098
      },
      {
        "r": 0,
        "text": "Music mastering engineers have used limiters to heavily compress and make music louder, which can induce ear fatigue and hearing loss in listeners.",
        "trunc_text": "Music mastering engineers have used limiters to heavily compress and make music louder, which can induce ear fatigue and",
        "x1": 2.7683281898498535,
        "x2": 0.8745073676109314,
        "y1": 2.2844295501708984,
        "y2": 6.027680397033691
      },
      {
        "r": 0,
        "text": "In this paper, we introduce music de-limiter networks that estimate uncompressed music from heavily compressed signals.",
        "trunc_text": "In this paper, we introduce music de-limiter networks that estimate uncompressed music from heavily compressed signals.",
        "x1": 2.73867130279541,
        "x2": 0.8053132891654968,
        "y1": 2.25600266456604,
        "y2": 6.158040523529053
      },
      {
        "r": 0,
        "text": "Inspired by the principle of a limiter, which performs sample-wise gain reduction of a given signal, we propose the framework of sample-wise gain inversion (SGI).  ",
        "trunc_text": "Inspired by the principle of a limiter, which performs sample-wise gain reduction of a given signal, we propose the fram",
        "x1": 3.1931650638580322,
        "x2": 3.4279465675354004,
        "y1": 5.623575687408447,
        "y2": 3.9322218894958496
      },
      {
        "r": 0,
        "text": "Our proposed de-limiter network achieves excellent performance with a scale-invariant source-to-distortion ratio (SI-SDR) of 23.8 dB in reconstructing musdb-HQ from musdb- XL data, a limiter-applied version of musdb-HQ.",
        "trunc_text": "Our proposed de-limiter network achieves excellent performance with a scale-invariant source-to-distortion ratio (SI-SDR",
        "x1": 4.633068084716797,
        "x2": 4.645169258117676,
        "y1": 5.499505996704102,
        "y2": 4.846174716949463
      },
      {
        "r": 0,
        "text": "The training data, codes, and model weights are available in our repository (https://github.com/jeonchangbin49/De-limiter).",
        "trunc_text": "The training data, codes, and model weights are available in our repository (https://github.com/jeonchangbin49/De-limite",
        "x1": 7.320932388305664,
        "x2": 7.85951042175293,
        "y1": 7.671971797943115,
        "y2": 2.0619986057281494
      },
      {
        "r": 0,
        "text": "Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage.",
        "trunc_text": "Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage.",
        "x1": 5.466156005859375,
        "x2": 4.270605087280273,
        "y1": 4.103844165802002,
        "y2": 6.294082164764404
      },
      {
        "r": 0,
        "text": "Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen.",
        "trunc_text": "Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration",
        "x1": 6.123207092285156,
        "x2": 5.511200428009033,
        "y1": 5.513360023498535,
        "y2": 5.445747375488281
      },
      {
        "r": 0,
        "text": "Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide.",
        "trunc_text": "Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determ",
        "x1": 6.060938358306885,
        "x2": 5.445878505706787,
        "y1": 5.573029518127441,
        "y2": 5.389922142028809
      },
      {
        "r": 0,
        "text": "As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable.",
        "trunc_text": "As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable.",
        "x1": 5.206268787384033,
        "x2": 4.0420074462890625,
        "y1": 4.873542785644531,
        "y2": 5.177059173583984
      },
      {
        "r": 0,
        "text": "Our work provides an alternative to demonstrations: tool documentation.",
        "trunc_text": "Our work provides an alternative to demonstrations: tool documentation.",
        "x1": 6.14990234375,
        "x2": 5.358727931976318,
        "y1": 5.468351364135742,
        "y2": 5.5213494300842285
      },
      {
        "r": 0,
        "text": "We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations.",
        "trunc_text": "We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations.",
        "x1": 6.050512790679932,
        "x2": 5.3597636222839355,
        "y1": 5.418704986572266,
        "y2": 5.513309955596924
      },
      {
        "r": 0,
        "text": "We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities.",
        "trunc_text": "We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities.",
        "x1": 4.282924175262451,
        "x2": 2.8201072216033936,
        "y1": 3.4512391090393066,
        "y2": 4.923144340515137
      },
      {
        "r": 0,
        "text": "First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts.",
        "trunc_text": "First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool u",
        "x1": 2.884366273880005,
        "x2": 3.657379627227783,
        "y1": 6.527433395385742,
        "y2": 3.4199395179748535
      },
      {
        "r": 0,
        "text": "Second, on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation.",
        "trunc_text": "Second, on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool document",
        "x1": 6.075493335723877,
        "x2": 5.291910648345947,
        "y1": 5.472763538360596,
        "y2": 5.514644622802734
      },
      {
        "r": 0,
        "text": "Third, we highlight the benefits of tool documentations by tackling image generation and video tracking using just-released unseen state-of-the-art models as tools.",
        "trunc_text": "Third, we highlight the benefits of tool documentations by tackling image generation and video tracking using just-relea",
        "x1": 6.064202308654785,
        "x2": 5.27518892288208,
        "y1": 5.450159072875977,
        "y2": 5.553568363189697
      },
      {
        "r": 0,
        "text": "Finally, we highlight the possibility of using tool documentation to automatically enable new applications: by using nothing more than the documentation of GroundingDino, Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released Grounded-SAM and Track Anything models.",
        "trunc_text": "Finally, we highlight the possibility of using tool documentation to automatically enable new applications: by using not",
        "x1": 6.226339340209961,
        "x2": 5.1566243171691895,
        "y1": 5.0813679695129395,
        "y2": 5.939924716949463
      },
      {
        "r": 0,
        "text": "In this paper, we construct an ancient Chinese character image dataset that contains both radical-level and character-level annotations to satisfy the requirements of the above-mentioned methods, namely, ACCID, where radical-level annotations include radical categories, radical locations, and structural relations.",
        "trunc_text": "In this paper, we construct an ancient Chinese character image dataset that contains both radical-level and character-le",
        "x1": 2.9485273361206055,
        "x2": 2.5318636894226074,
        "y1": 4.448590278625488,
        "y2": 4.502982139587402
      },
      {
        "r": 0,
        "text": "Optical character recognition (OCR) methods have been applied to diverse tasks, e.g., street view text recognition and document analysis.",
        "trunc_text": "Optical character recognition (OCR) methods have been applied to diverse tasks, e.g., street view text recognition and d",
        "x1": 4.005597114562988,
        "x2": 1.5606085062026978,
        "y1": 1.7811247110366821,
        "y2": 6.9289164543151855
      },
      {
        "r": 0,
        "text": "Recently, zero-shot OCR has piqued the interest of the research community because it considers a practical OCR scenario with unbalanced data distribution.",
        "trunc_text": "Recently, zero-shot OCR has piqued the interest of the research community because it considers a practical OCR scenario ",
        "x1": 4.027981281280518,
        "x2": 1.6126173734664917,
        "y1": 1.8072009086608887,
        "y2": 6.821844577789307
      },
      {
        "r": 0,
        "text": "However, there is a lack of benchmarks for evaluating such zero-shot methods that apply a divide-and-conquer recognition strategy by decomposing characters into radicals.",
        "trunc_text": "However, there is a lack of benchmarks for evaluating such zero-shot methods that apply a divide-and-conquer recognition",
        "x1": 2.8691437244415283,
        "x2": 2.522282361984253,
        "y1": 4.65749979019165,
        "y2": 4.367074966430664
      },
      {
        "r": 0,
        "text": "Meanwhile, radical recognition, as another important OCR task, also lacks radical-level annotation for model training.  ",
        "trunc_text": "Meanwhile, radical recognition, as another important OCR task, also lacks radical-level annotation for model training.  ",
        "x1": 2.8966000080108643,
        "x2": 2.5035433769226074,
        "y1": 4.541426658630371,
        "y2": 4.43437385559082
      },
      {
        "r": 0,
        "text": "To increase the adaptability of ACCID, we propose a splicing-based synthetic character algorithm to augment the training samples and apply an image denoising method to improve the image quality.",
        "trunc_text": "To increase the adaptability of ACCID, we propose a splicing-based synthetic character algorithm to augment the training",
        "x1": 2.091168165206909,
        "x2": 3.0973727703094482,
        "y1": 7.133984565734863,
        "y2": 2.1107146739959717
      },
      {
        "r": 0,
        "text": "By introducing character decomposition and recombination, we propose a baseline method for zero-shot OCR.",
        "trunc_text": "By introducing character decomposition and recombination, we propose a baseline method for zero-shot OCR.",
        "x1": 3.995727062225342,
        "x2": 1.5906234979629517,
        "y1": 1.7955645322799683,
        "y2": 6.805044174194336
      },
      {
        "r": 0,
        "text": "The experimental results demonstrate the validity of ACCID and the baseline model quantitatively and qualitatively.",
        "trunc_text": "The experimental results demonstrate the validity of ACCID and the baseline model quantitatively and qualitatively.",
        "x1": 5.489109039306641,
        "x2": 6.535008430480957,
        "y1": 8.904671669006348,
        "y2": 4.87279748916626
      },
      {
        "r": 0,
        "text": "Advanced driving assistance systems are available on many late-model vehicles, and automated driving systems are testing on public roads.",
        "trunc_text": "Advanced driving assistance systems are available on many late-model vehicles, and automated driving systems are testing",
        "x1": 3.6832661628723145,
        "x2": 5.320436477661133,
        "y1": 10.037324905395508,
        "y2": -0.010908972471952438
      },
      {
        "r": 0,
        "text": "Regulators and developers continue to assess the safety of these vehicles by comparing automated vehicle crash rates to baseline, human-driven crash rates.",
        "trunc_text": "Regulators and developers continue to assess the safety of these vehicles by comparing automated vehicle crash rates to ",
        "x1": 3.779306411743164,
        "x2": 5.39952278137207,
        "y1": 9.907852172851562,
        "y2": 0.01092130970209837
      },
      {
        "r": 0,
        "text": "While there are several widely-cited automated vehicle and conventional vehicle crash databases, these databases have different underlying assumptions and inclusion criteria.",
        "trunc_text": "While there are several widely-cited automated vehicle and conventional vehicle crash databases, these databases have di",
        "x1": 3.862623691558838,
        "x2": 5.417896270751953,
        "y1": 9.863141059875488,
        "y2": 0.0825379490852356
      },
      {
        "r": 0,
        "text": "Crash rates among databases may be directly comparable only with significant filtering and normalization, if at all.",
        "trunc_text": "Crash rates among databases may be directly comparable only with significant filtering and normalization, if at all.",
        "x1": 3.909381628036499,
        "x2": 5.076187610626221,
        "y1": 9.680638313293457,
        "y2": 4.18922233581543
      },
      {
        "r": 0,
        "text": "This paper reviews current automated vehicle and baseline human-driven crash databases and evaluates their comparability.",
        "trunc_text": "This paper reviews current automated vehicle and baseline human-driven crash databases and evaluates their comparability",
        "x1": 3.7160181999206543,
        "x2": 5.319530487060547,
        "y1": 9.902938842773438,
        "y2": -0.09668324887752533
      },
      {
        "r": 0,
        "text": "Recommendations are presented to improve their comparability, both in terms of normalization and contextualization, as well as additional data fields that can be incorporated into existing databases.",
        "trunc_text": "Recommendations are presented to improve their comparability, both in terms of normalization and contextualization, as w",
        "x1": 4.830844879150391,
        "x2": 5.101491928100586,
        "y1": 5.678679466247559,
        "y2": 4.554963111877441
      },
      {
        "r": 0,
        "text": "These findings may assist researchers, regulators, and automated vehicle developers attempting to evaluate the safety of driving automation systems.",
        "trunc_text": "These findings may assist researchers, regulators, and automated vehicle developers attempting to evaluate the safety of",
        "x1": 3.7776577472686768,
        "x2": 5.367380619049072,
        "y1": 9.919872283935547,
        "y2": 0.009196522645652294
      }
    ]
  },
  "hconcat": [
    {
      "encoding": {
        "color": {
          "condition": {
            "field": "id",
            "legend": null,
            "param": "param_28",
            "type": "ordinal"
          },
          "value": "lightgray"
        },
        "tooltip": [
          {
            "field": "text",
            "type": "nominal"
          }
        ],
        "x": {
          "axis": null,
          "field": "x1",
          "scale": {
            "zero": false
          },
          "type": "quantitative"
        },
        "y": {
          "axis": null,
          "field": "y1",
          "scale": {
            "zero": false
          },
          "type": "quantitative"
        }
      },
      "height": 350,
      "mark": {
        "opacity": 0.6,
        "size": 20,
        "type": "circle"
      },
      "name": "view_35",
      "title": "embedding space X1",
      "width": 350
    },
    {
      "encoding": {
        "color": {
          "condition": {
            "field": "id",
            "legend": null,
            "param": "param_28",
            "type": "ordinal"
          },
          "value": "lightgray"
        },
        "tooltip": [
          {
            "field": "text",
            "type": "nominal"
          }
        ],
        "x": {
          "axis": null,
          "field": "x2",
          "scale": {
            "zero": false
          },
          "type": "quantitative"
        },
        "y": {
          "axis": null,
          "field": "y2",
          "scale": {
            "zero": false
          },
          "type": "quantitative"
        }
      },
      "height": 350,
      "mark": {
        "opacity": 0.6,
        "size": 20,
        "type": "circle"
      },
      "name": "view_36",
      "title": "embedding space X2",
      "width": 350
    },
    {
      "encoding": {
        "text": {
          "field": "trunc_text",
          "type": "nominal"
        },
        "x": {
          "axis": null,
          "field": "r",
          "type": "quantitative"
        },
        "y": {
          "axis": null,
          "field": "row_number",
          "type": "ordinal"
        }
      },
      "mark": {
        "type": "text"
      },
      "title": "text",
      "transform": [
        {
          "window": [
            {
              "as": "row_number",
              "field": "",
              "op": "row_number"
            }
          ]
        },
        {
          "filter": {
            "param": "param_28"
          }
        },
        {
          "window": [
            {
              "as": "rank",
              "field": "row_number",
              "op": "rank"
            }
          ]
        },
        {
          "filter": "(datum.rank < 18)"
        }
      ]
    }
  ],
  "params": [
    {
      "name": "param_28",
      "select": {
        "type": "interval"
      },
      "views": [
        "view_35",
        "view_36"
      ]
    }
  ]
}